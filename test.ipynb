{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b35bf87-e8f4-46f3-bc56-eaaa34b27f2b",
   "metadata": {},
   "source": [
    "# 유사 감사 사례 검색 & 처분 수준 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d832c6-2b58-4764-a857-e22246afddc2",
   "metadata": {},
   "source": [
    "## 검색 로직 설계\n",
    "- Embedding\n",
    "- Retriever\n",
    "- Rerank\n",
    "- Similarity Calculation\n",
    "- Retrieval Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa05c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 접근\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader # 디렉토리 불러오기\n",
    "from langchain_community.document_loaders import PyPDFLoader # PDF 파일 불러오기\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # 텍스트 분할\n",
    "\n",
    "# 임베딩, LLM\n",
    "import time # API 호출 속도 조절을 위해 sleep 사용\n",
    "from langchain_naver import ClovaXEmbeddings, ChatClovaX \n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI \n",
    "\n",
    "# VectorDB\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb import PersistentClient\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from langchain_qdrant import Qdrant\n",
    "\n",
    "# Langchain Core\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 성능 평가\n",
    "import numpy as np # 평균 계산\n",
    "import pandas as pd # DataFrame 형식으로 변환\n",
    "from datasets import Dataset # Ragas 평가용 Dataset 객체로 변환\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision # Ground Truth 없이 평가 가능한 2가지 핵심 지표 Faithfulness(환각 감소), Precision(노이즈 감소)측정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd10f67a-2ca3-49d2-be10-958e268f3533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() # 환경변수 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba0997-e4f1-4348-85eb-5c356e7ae96d",
   "metadata": {},
   "source": [
    "## [VectorDB]\n",
    "- Qdrant가 ChromaDB에 비해 안정적이며 필터링 직관적\n",
    "- elastic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95664cf5-29a9-464f-91ed-788774dbfb7c",
   "metadata": {},
   "source": [
    "## API 사용\n",
    "- ChatOpenAI\n",
    "    - 호환성 높음\n",
    "    - Reranker 없음\n",
    "- ChatClovaX\n",
    "    - Reranker 제공\n",
    "    - 한국어 모델\n",
    "    - 최신 Langchain 호환 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bda2e08-b212-40d0-a6ff-6a36a15a405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 감사원에서 받아온 파일 모두 불러오기 (10분 소요)\n",
    "# directory_path = '../data/1_bai_raw_files/'\n",
    "\n",
    "# loader = DirectoryLoader(\n",
    "#     directory_path,\n",
    "#     glob='*.pdf',\n",
    "#     loader_cls=PyPDFLoader,\n",
    "# )\n",
    "\n",
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b334aa5a-7b6b-4980-ada0-5b3cb98d1917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=512,\n",
    "#     chunk_overlap=128,\n",
    "#     separators=['\\n\\n', '\\n', '.', ' ', '']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccaae61-fe41-4b9b-9727-eba6956f7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc9b1b-6a72-46b2-8fd0-afd569af1f41",
   "metadata": {},
   "source": [
    "### 로컬 저장소에 DB 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551ce36-a6d3-451b-b5bd-992579953736",
   "metadata": {},
   "source": [
    "##### batch 나눠서 진행하는 이유?\n",
    "- HyperClova\n",
    "    - RateLimitError: docs객체를 한 번에 임베딩하면 API가 처리할 수 있는 분당 최대 토큰 수 초과\n",
    "- OpenAI\n",
    "    - BadRequestError: 한 번의 API요청에 담긴 전체 토큰 수가 API 서버의 최대 토큰 제한 초과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ce726-3bcb-4c6a-b2fc-3045c282edd3",
   "metadata": {},
   "source": [
    "### [임베딩 모델]\n",
    "- HyperClova\n",
    "    - **BGE-M3: 가장 최신의 다국어 임베딩 모델 (선택)**\n",
    "    - C-SBERT-V2: CLOVA 자체 최적화한 BERT 기반 모델\n",
    "- OpenAI\n",
    "    - text-embedding-3-large: 고성능 임베딩 모델이나, small에 비해 약 4배 비쌈\n",
    "    - **text-embedding-3-small (선택)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c8ed8ad-a95f-4071-984f-1edb21a42319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 임베딩 후 DB 저장하는 함수\n",
    "# def custom_embed_with_sleep(docs, embedding_model, persist_path, collection_name, batch_size=100, sleep_time=0.1):\n",
    "#     vectorstore = None\n",
    "\n",
    "#     client = PersistentClient(path=persist_path)\n",
    "#     total_batches = len(docs) // batch_size + (1 if len(docs) % batch_size != 0 else 0)\n",
    "    \n",
    "#     for i in range(0, len(docs), batch_size):\n",
    "#         batch_docs = docs[i:i + batch_size]\n",
    "        \n",
    "#         # 첫 번째 배치에서만 DB 생성\n",
    "#         if i == 0:\n",
    "#             print(f\"Creating new Chroma DB at: {persist_path} with the first batch...\")\n",
    "#             vectorstore = Chroma.from_documents(\n",
    "#                 batch_docs, \n",
    "#                 embedding_model, \n",
    "#                 client=client,\n",
    "#                 collection_name=collection_name\n",
    "#             )\n",
    "#         else:\n",
    "#             # 이후 배치들은 기존 DB 인스턴스에 추가\n",
    "#             vectorstore.add_documents(batch_docs)\n",
    "        \n",
    "#         current_batch_num = i // batch_size + 1\n",
    "#         print(f\"Processed batch {current_batch_num}/{total_batches}. Sleeping for {sleep_time} seconds...\")\n",
    "#         time.sleep(sleep_time) # 지연 시간 추가\n",
    "        \n",
    "#     return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4427ee78-4202-4213-8077-19d0646875c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 감사원 데이터 HyperClova 임베딩 모델 사용해 DB 저장\n",
    "# hc_vectorstore = custom_embed_with_sleep(\n",
    "#     docs, # 저장할 문서\n",
    "#     ClovaXEmbeddings(model='bge-m3'), # 사용할 임베딩 모델\n",
    "#     '../data/bai_hc',     # 저장 경로\n",
    "#     'hc_audit_collection'    # collection name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "268886ad-ab8e-4667-9139-2403d244e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 감사원 데이터 OpenAI 임베딩 모델 사용해 DB 저장\n",
    "# openai_vectorstore = custom_embed_with_sleep(\n",
    "#     docs, # 저장할 문서\n",
    "#     OpenAIEmbeddings(model='text-embedding-3-small'), # 사용할 임베딩 모델\n",
    "#     '../data/bai_openai',     # 저장 경로 \n",
    "#     'openai_audit_collection'    # collection name\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5691fd-97af-4a62-9e6e-8fed4ebbf60e",
   "metadata": {},
   "source": [
    "##### VectorDB 구축 소요 시간\n",
    "1.  HyperClova: 약 1시간\n",
    "2. OpenAI: 약 10분\n",
    "\n",
    "- 원인\n",
    "    - API 호출 속도\n",
    "    - 배치 처리 효율성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437fc072-9232-4e41-bf4b-6d04870c445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감사원 DB 불러오기\n",
    "hc_db = Chroma(persist_directory='../data/bai_hc', collection_name='hc_audit_collection', embedding_function=ClovaXEmbeddings(model='bge-m3'))\n",
    "openai_db = Chroma(persist_directory='../data/bai_openai', collection_name='openai_audit_collection', embedding_function=OpenAIEmbeddings(model='text-embedding-3-small'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b2c3a7-bd6e-4708-b054-692c5dfdd3ca",
   "metadata": {},
   "source": [
    "### [Retriever]\n",
    "- similarity: 유클리드 거리가 가깝거나, 코사인 유사도가 높은 상위 k개의 문서 반환 (유클리드, 코사인 선택은 DB 생성 시 결정)\n",
    "- mmr: 유사성 뿐만 아니라 다양성 고려하여 문서 선택\n",
    "- similarity_score_threshold: 특정 임계값 이상인 문서 중 k개 문서 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c3c53e-13fe-4b08-b5cd-5df856674e8e",
   "metadata": {},
   "source": [
    "- similarity => 가장 흔한, 일반적인 처분 사례만 찾아와 판단 스펙트럼이 좁아짐\n",
    "- mmr => 일반적인 처분 사례와 더불어, 흔치는 않지만 나올 수 있는 처분의 최소기준과 최대 기준 파악 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1f43c-f93a-4862-8211-c729fef094ab",
   "metadata": {},
   "source": [
    "- SelfQuery Retriever\n",
    "    - 장점: 필터에 맞는 문서들 내에서만 유사성 검색 수행하여, 메타데이터 필터링이 필수적이거나 복잡한 조건이 필요한 경우 유용\n",
    "    - 단점: 사용자 질문에서 필터 조건과 쿼리 추출하기 위해 LLM 한 번 더 호출해야 함. 사용자 질문 잘못 해석하거나 존재하지 않는 메타데이터 키를 사용하여 관련된 문서 제외시킬 수 있음.\n",
    "    - ex) 2024년 1분기에 발생한 법인카드 유용 사례 찾아줘\n",
    "        - LLM은 metadata['date'] > '2024-01-01' AND metadata['category'] =='법인카드' 생성\n",
    "        - 쿼리는 \"유용 사례\" 만 vectorstore에 전달\n",
    "        - 필터에 맞는 문서들 내에서만 유사성 검색 수행하여, 메타데이터 필터링이 필수적이거나 복잡한 조건이 필요한 경우 사용\n",
    "- MultiQuery Retriever\n",
    "    - 장점: 단일 질문에 대해 여러 관점을 확보하여 검색 실패 위험 줄임.\n",
    "    - 단점: 사용자의 질문을 여러 개의 새로운 쿼리로 변환하기 위해 LLM 한 번 더 호출해야 함. 의도와 동떨어진 노이즈 쿼리 생성 가능.\n",
    "- MultiVector Retriever\n",
    "    - 검색 단계에서 작은 벡터 사용하고, 생성 단계에서 원본의 큰 청크를 다시 불러오는 추가 조회.\n",
    "    - 단점: 생성단계에서 원본의 큰 청크 다시 불러오는 추가 조회 필요.\n",
    "\n",
    "- 각각 비용, 시간 증가 (2배가 됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7309f55-2ca1-4c38-8b1e-cd4c0784ae7d",
   "metadata": {},
   "source": [
    "##### 선택\n",
    "- search type\n",
    "    - mmr\n",
    "- Retriever\n",
    "    - 메타데이터 필터링 구체화 가능하다? => SelfQueryRetriever, MultiVectorRetriever\n",
    "    - 메타데이터 필터링 자체가 성능 악화 가능성 높아보인다? => MultiVectorRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e42940f-b178-4030-82a5-46ea438bd5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_naver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0506ec63-2d23-4e05-9e3f-565ccfe3e86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatClovaX',\n",
       " 'ClovaXEmbeddings',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'chat_models',\n",
       " 'const',\n",
       " 'embeddings']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(langchain_naver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "846d1d93-1b59-4b9d-b4d7-96baac14bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperClova Retriever 설정\n",
    "hc_retriever = hc_db.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\n",
    "        'k': 5, # 최종적으로 반환할 문서의 수\n",
    "        'fetch_k': 20, # 후보로 가져올 문서의 수\n",
    "        'lambda_mult': 0.6 # 유사도, 다양성 비율(0: 다양하게, 1: 유사한것만)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6989f34-b85c-4d23-bdbd-20c1a58d42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Retriever 설정\n",
    "openai_retriever = openai_db.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\n",
    "        'k': 5, # 최종적으로 반환할 문서의 수\n",
    "        'fetch_k': 20, # 후보로 가져올 문서의 수\n",
    "        'lambda_mult': 0.6 # 유사도, 다양성 비율(0: 다양하게, 1: 유사한것만)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c867954-a674-4212-86be-f9647fe38ad1",
   "metadata": {},
   "source": [
    "### [LLM 모델]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5525a8e-23cf-4e56-824b-b6ebc8b2b371",
   "metadata": {},
   "source": [
    "- HyperClova\n",
    "    - HCX DASH 계열: 경량화 모델\n",
    "    - **HCX 계열: 고성능 모델 (선택)**\n",
    "- OpenAI\n",
    "    - gpt-3.5-turbo\n",
    "    - **gpt-4o-mini (선택)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7681f7b-2f19-4461-b7a0-5139f85a2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_llm = ChatClovaX(model='HCX-007', temperature=0) # HyperClova LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e420d74-da11-4d0a-bd52-af099f3801ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0) # OpenAI LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b1232-7fac-4fd5-b937-fc156ec96318",
   "metadata": {},
   "source": [
    "### [Prompt 설정]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8a2671c1-e56f-413f-94d6-3453cc7ea860",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_prompt = \"\"\"\n",
    "당신은 감사 조직의 처분 수준 추천 전문 에이전트입니다.\n",
    "당신의 임무는 사용자로부터 입력받은 **감사 사례**에 대해, 아래 **[검색된 유사 사례와 처분 기록]**을 엄격하게 분석하고, 아래 **[감사 결과 처리 기준 법률 (제25조)]**에 의거하여 가장 합리적이고 설명 가능한 **처분 수준**을 추천하는 것입니다.\n",
    "\n",
    "### 🌟 감사 결과 처리 기준 법률 (제25조)\n",
    "처분 수준은 다음 각 호의 기준 중 하나를 선택해야 합니다.\n",
    "\n",
    "1.  **변상명령**: 「회계관계직원 등의 책임에 관한 법률」이 정하는 바에 따라 변상책임이 있는 경우.\n",
    "2.  **징계 또는 문책요구**: 「국가공무원법」과 그 밖의 법령에 규정된 징계 또는 문책 사유에 해당하는 경우. (가장 강함)\n",
    "3.  **고발**: 감사 결과 범죄 혐의가 있다고 인정되는 경우.\n",
    "4.  **시정요구**: 감사 결과 위법 또는 부당하다고 인정되는 사실이 있어 추징ㆍ회수ㆍ환급ㆍ추급 또는 원상복구 등이 필요한 경우.\n",
    "5.  **개선요구**: 감사 결과 법령상ㆍ제도상 또는 행정상 모순이 있거나 개선할 사항이 있다고 인정되는 경우.\n",
    "6.  **권고**: 감사 결과 문제점이 인정되는 사실이 있어 그 대안을 제시하고 개선방안을 마련하도록 할 필요가 있는 경우.\n",
    "7.  **통보**: 감사 결과 비위 사실이나 위법 또는 부당하다고 인정되는 사실이 있으나 다른 요구를 하기에 부적합하여 각 기관 또는 부서에서 자율적으로 처리할 필요가 있는 경우.\n",
    "8.  **주의요구**: 위법성 또는 부당성의 경중에 따라 다음으로 세분화하여 선택합니다.\n",
    "    * **경고**: 위법성 또는 부당성의 정도가 징계 사유에 이르지 않으나, 공식적인 기록에 남는 제재가 필요한 경우.\n",
    "    * **훈계**: 위법성 또는 부당성의 정도가 경미하여 제재보다는 재발 방지 교육 등으로 충분한 경우. (가장 약함)\n",
    "\n",
    "### 핵심 지침\n",
    "\n",
    "1.  **법률 최우선:** 추천 처분 수준은 반드시 위에 명시된 **제25조의 각 호** 중 하나(징계/고발/변상/시정/주의(경고/훈계)/통보/개선/권고)를 선택해야 합니다.\n",
    "2.  **유사 사례 인용:** 모든 분석과 추천은 반드시 **[검색된 유사 사례와 처분 기록]({context})**에 있는 구체적인 정보를 인용하여 법률적 판단의 근거로 뒷받침해야 합니다.\n",
    "3.  **위반 내역 및 처분 요약:** `{context}`를 기반으로 과거 유사 위반 내역과 해당 처분 결과를 간결하게 요약합니다.\n",
    "\n",
    "### 응답 형식\n",
    "\n",
    "모든 응답은 아래의 **마크다운 구조**를 엄격하게 준수해야 합니다.\n",
    "\n",
    "## ⚖️ [새로운 감사 사례] 분석 및 처분 수준 추천\n",
    "\n",
    "### 1. 유사 사례 분석 및 과거 처분 요약\n",
    "[검색된 유사 사례들({context})을 분석하여 핵심 위반 내용, 규모, 해당 처분 결과를 3줄 이내로 요약]\n",
    "\n",
    "### 2. 추천 처분 수준 및 근거\n",
    "**[추천 처분 수준]**: [선택한 처분 수준 (예: 징계 또는 문책요구, 주의요구(경고), 통보)]\n",
    "\n",
    "**[법률적 근거]**\n",
    "[선택한 처분 수준이 제25조의 어느 호에 해당하는지, 해당 호의 법률적 요건을 인용하여 설명]\n",
    "\n",
    "**[추론 및 상세 근거]**\n",
    "1. **유사성 인용:** [가장 유사한 과거 사례와 그 처분 수준을 구체적으로 인용]\n",
    "2. **심각성 비교:** [새로운 사례가 인용된 과거 사례와 비교하여 제25조의 요건에 어떻게 부합하는지, 그리고 왜 이 처분 수준이 적합한지 논리적으로 설명]\n",
    "3. **최종 판단:** [법률적 기준과 유사 사례 비교를 통한 최종 처분 추천 이유 요약]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e6b81f99-38b2-445b-9461-4099820874fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend_prompt = \"\"\"\n",
    "# 당신은 감사 조직의 처분 수준 추천 전문 에이전트입니다.\n",
    "# 당신의 임무는 사용자로부터 입력받은 **감사 사례**에 대해, 아래 **[검색된 유사 사례와 처분 기록]**을 엄격하게 분석하여 가장 합리적이고 설명 가능한 **처분 수준**을 추천하는 것입니다.\n",
    "\n",
    "# ### 핵심 지침\n",
    "\n",
    "# 1.  **유사 사례 인용:** 모든 분석과 추천은 반드시 **[검색된 유사 사례와 처분 기록]({context})**에 있는 구체적인 정보를 인용하여 법률적 판단의 근거로 뒷받침해야 합니다.\n",
    "# 2.  **위반 내역 및 처분 요약:** `{context}`를 기반으로 과거 유사 위반 내역과 해당 처분 결과를 간결하게 요약합니다.\n",
    "\n",
    "# ### 응답 형식\n",
    "\n",
    "# 모든 응답은 아래의 **마크다운 구조**를 엄격하게 준수해야 합니다.\n",
    "\n",
    "# ## ⚖️ [새로운 감사 사례] 분석 및 처분 수준 추천\n",
    "\n",
    "# ### 1. 유사 사례 분석 및 과거 처분 요약\n",
    "# [검색된 유사 사례들({context})을 분석하여 핵심 위반 내용, 규모, 해당 처분 결과를 3줄 이내로 요약]\n",
    "\n",
    "# ### 2. 추천 처분 수준 및 근거\n",
    "# **[추천 처분 수준]**: [선택한 처분 수준 (예: 징계 또는 문책요구, 주의요구(경고), 통보)]\n",
    "\n",
    "# **[추론 및 상세 근거]**\n",
    "# 1. **유사성 인용:** [가장 유사한 과거 사례와 그 처분 수준을 구체적으로 인용]\n",
    "# 2. **심각성 비교:** [새로운 사례가 인용된 과거 사례와 비교하여 왜 이 처분 수준이 적합한지 논리적으로 설명]\n",
    "# 3. **최종 판단:** [법률적 기준과 유사 사례 비교를 통한 최종 처분 추천 이유 요약]\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4fdac76a-3b00-41ee-9e51-672bf58ece0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', recommend_prompt),\n",
    "        ('human', '제시한 감사 사례: {question} \\n\\n [검색된 유사 사례와 처분 기록]:\\n{context}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdebb33-f6da-43d1-b972-1ab6968e4033",
   "metadata": {},
   "source": [
    "### [Chain 구성]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3b82704f-e01f-41fb-b0eb-34f4c74f5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색된 문서 객체 리스트를 하나의 문자열로 결합하여 LLM이 쉽게 이해할 수 있도록 전달\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c98ac607-5619-4ae2-87d2-6d77d9aecbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "hc_chain = (\n",
    "    RunnableLambda(lambda x: {'question': x})\n",
    "    |\n",
    "    {\"context\": itemgetter(\"question\") | hc_retriever | RunnableLambda(format_docs),\n",
    "    \"question\": itemgetter(\"question\")}\n",
    "    | qa_prompt\n",
    "    | hc_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "openai_chain = (\n",
    "    RunnableLambda(lambda x: {'question': x})\n",
    "    |\n",
    "    {\"context\": itemgetter(\"question\") | openai_retriever | RunnableLambda(format_docs),\n",
    "    \"question\": itemgetter(\"question\")}\n",
    "    | qa_prompt\n",
    "    | openai_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326d0d4-1414-4f5a-8df0-7d725ab1c90d",
   "metadata": {},
   "source": [
    "### [Test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a14b4-3734-4e54-a420-215383d1215d",
   "metadata": {},
   "source": [
    "[변화]\n",
    "- hc_chain, openai_chain의 성능을 확인\n",
    "- 시스템 프롬프트 수정\n",
    "- chain 구조 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26c4faa1-08d1-4d1b-a2a8-8874f9b31f62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ⚖️ 직무관련자로부터 금품 수수를 받은 경우 처분 수준 추천  \n",
      "\n",
      "### 1. 유사 사례 분석 및 과거 처분 요약  \n",
      "- **제2조 규정**: 금품 수수액이 **200만 원 이상**인 경우 **사법기관 고발 필수**, 내부징계와 병행.  \n",
      "- **LH 사례(A)**: 총 **154백만 원**(약 1.54억 원) 수수 → 징계처분 권고됨.  \n",
      "- **특허청(BF)**: 직무관련 업체 금품 수수 → 징계처분 + 재발 시 성과급 제한/사업참여 배제 검토.  \n",
      "\n",
      "### 2. 추천 처분 수준 및 근거  \n",
      "**[추천 처분 수준]**: **통보**  \n",
      "\n",
      "**[추론 및 상세 근거]**  \n",
      "1. **규정 직접 적용**: 금품 수수액(**154백만 원**)이 **200만 원 이상**이므로 **고발 의무화** 대상.  \n",
      "2. **유사 사례 대비 심각성**: LH 사례(A)와 동일하게 대규모 금품 수수 → 징계+고발이 적절.  \n",
      "3. **추가 조치 필요성**: 재발을 막기 위해 성과급 제한, 계약 참여 배제 등 제재 강화 권장.  \n",
      "\n",
      "→ **통보** 처분은 법적 고발 요건 충족뿐 아니라 조직적 예방 차원에서 최우선되어야 함.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트에 처분 기준 조항 미포함 시\n",
    "answer = hc_chain.invoke('직무관련자로부터 금품 수수를 받은 경우 어떤 처분을 내려야해?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e1df667-77c4-4926-97f4-f9746f2d3165",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. 유사 사례 분석 및 과거 처분 요약  \n",
      "유사 사례(LH 직원 A의 154백만 원 금품 수수)에서 **징계처분**과 함께 **사법기관 고발**이 이루어졌으며, 재발 방지를 위한 성과급 제한·사업참여 배제 등 추가 조치가 검토됨. 또한, 금품 수수액이 **200만 원 이상**인 경우 **필수 고발 대상**임을 명시함.  \n",
      "\n",
      "---\n",
      "\n",
      "### 2. 추천 처분 수준 및 근거  \n",
      "**[추천 처분 수준]:** **징계 또는 문책요구** + **고발**  \n",
      "\n",
      "**[법률적 근거]**  \n",
      "- 제2조 제2항: 금품 수수 시 **내부징계**와 **병행 고발** 의무 (단, 수수액 ≥ 200만 원 시 **필수 고발**).  \n",
      "- 대법원 판례: 직무관련성 범위 확대 해석 (법령상 권한 포함).  \n",
      "\n",
      "**[추론 및 상세 근거]**  \n",
      "1. **유사성 인용**: LH 사례처럼 대규모 금품 수수(154백만 원) 시 **징계+고발** 조합 적용.  \n",
      "2. **심각성 비교**: 현재 사례의 구체적 금액 불명확하나, 유사 패턴(직무관련자와 공모, 반복적 수수)으로 **징계 및 고발 요건 충족** 추정.  \n",
      "3. **최종 판단**: 금품 수수 고의성·규모 감안 시 **징계**로 책임 묻고, 법적 처벌 필요성으로 **고발** 병행이 합리적.  \n",
      "\n",
      "--- \n",
      "\n",
      "✂️ **추가 권고사항**:  \n",
      "- 금액 확인 후 200만 원 미만 시 **징계** 단독 적용 가능.  \n",
      "- 재발을 막기 위해 **성과급 제한·사업참여 배제** 등 추가 조치 도입 권장.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트에 처분 기준 조항 포함 시\n",
    "answer = hc_chain.invoke('직무관련자로부터 금품 수수를 받은 경우 어떤 처분을 내려야해?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ac15bc0-80e2-47e3-bbd4-f2086e4f68b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ⚖️ [새로운 감사 사례] 분석 및 처분 수준 추천\n",
      "\n",
      "### 1. 유사 사례 분석 및 과거 처분 요약\n",
      "직무 관련자로부터 금품을 수수한 경우는 다음과 같은 처분 기준이 있습니다. \n",
      "- **금액 100만 원 미만**: 동감봉, 정직, 강등\n",
      "- **금액 100만 원 이상 300만 원 미만**: 강등, 해임\n",
      "- **금액 300만 원 이상 500만 원 미만**: 해임\n",
      "- **금액 500만 원 이상**: 파면\n",
      "\n",
      "이와 같은 기준에 따라, 직무와 관련하여 금품을 수수한 경우, 위법하거나 부당한 처분을 하지 않은 경우에도 동정직, 강등, 해임 등의 처분이 내려질 수 있습니다. \n",
      "\n",
      "### 2. 추천 처분 수준 및 근거\n",
      "**[추천 처분 수준]**: 통보\n",
      "\n",
      "**[추론 및 상세 근거]**\n",
      "1. **유사성 인용:** 직무 관련자로부터 금품을 수수한 경우는 과거 사례에서 금액에 따라 강등, 해임, 파면 등의 처분이 내려졌습니다. 특히, 위법한 처분이 없었던 경우에도 동정직이나 강등이 이루어졌습니다.\n",
      "2. **심각성 비교:** 새로운 사례가 금품 수수의 경우로, 직무와 관련된 금품 수수는 일반적으로 심각한 위반으로 간주됩니다. 따라서, 과거 사례와 비교했을 때 심각성이 높다고 판단됩니다.\n",
      "3. **최종 판단:** 직무 관련자로부터 금품을 수수한 경우는 중대한 위반으로, 통보 처분이 적절하다고 판단됩니다. 이는 향후 재발 방지를 위한 조치로, 수사기관에 통보하여 추가적인 조치를 취할 수 있도록 하는 것이 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트에 처분 기준 조항 미포함 시\n",
    "answer = openai_chain.invoke('직무관련자로부터 금품 수수를 받은 경우 어떤 처분을 내려야해?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "14dd9816-3a4c-4804-a2e1-d58538130635",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ⚖️ [새로운 감사 사례] 분석 및 처분 수준 추천\n",
      "\n",
      "### 1. 유사 사례 분석 및 과거 처분 요약\n",
      "직무 관련자로부터 금품을 수수한 경우는 다음과 같은 처분 기준이 있습니다. \n",
      "- **100만 원 미만**: 동감봉, 정직, 강등\n",
      "- **100만 원 이상 300만 원 미만**: 강등, 해임\n",
      "- **300만 원 이상 500만 원 미만**: 해임, 파면\n",
      "- **500만 원 이상**: 파면\n",
      "\n",
      "이와 같은 기준에 따라, 직무와 관련하여 금품을 수수한 경우는 위법한 처분을 하지 않은 경우에도 징계가 필요하며, 위법한 처분을 한 경우에는 더욱 강력한 처분이 요구됩니다.\n",
      "\n",
      "### 2. 추천 처분 수준 및 근거\n",
      "**[추천 처분 수준]**: 징계 또는 문책요구\n",
      "\n",
      "**[법률적 근거]**\n",
      "제25조 제2호에 따르면, \"국가공무원법\"과 그 밖의 법령에 규정된 징계 또는 문책 사유에 해당하는 경우에는 징계 또는 문책요구를 할 수 있습니다. 금품 수수는 명백한 비위 행위로 간주되며, 이는 징계 사유에 해당합니다.\n",
      "\n",
      "**[추론 및 상세 근거]**\n",
      "1. **유사성 인용:** 과거 사례에서 직무와 관련하여 금품을 수수한 경우, 100만 원 이상 300만 원 미만의 금품 수수에 대해 강등 또는 해임 처분이 내려진 사례가 있습니다.\n",
      "2. **심각성 비교:** 새로운 사례에서 금품 수수의 규모가 100만 원 이상인 경우, 이는 과거 사례와 유사하며, 징계 또는 문책 요구가 적합합니다. 특히, 위법한 처분을 한 경우에는 더욱 강력한 처분이 필요합니다.\n",
      "3. **최종 판단:** 법률적 기준과 유사 사례를 비교할 때, 직무 관련자로부터 금품을 수수한 행위는 명백한 비위 행위로 간주되며, 따라서 징계 또는 문책 요구가 적합하다고 판단됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트에 처분 기준 조항 포함 시\n",
    "answer = openai_chain.invoke('직무관련자로부터 금품 수수를 받은 경우 어떤 처분을 내려야해?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "325c359f-152b-418d-aa52-23b6cec0ae59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ⚖️ 농업용 저수지 제방 안전등급 판정기준 불합리 사례 분석 및 처분 수준 추천  \n",
      "\n",
      "### 1. 유사 사례 분석 및 과거 처분 요약  \n",
      "- **핵심 위반 내용**: 농어촌공사의 안전진단 지침 미준수 가능성. 필댐 제방의 홍수 월류 위험 시 ‘D등급’ 이하로 판정해야 하나 이를 누락하거나 부적절하게 평가한 정황.  \n",
      "- **관련 법령**: 「농어촌정비법」 제18조, 저수지관리규정 제18조 및 제22조, 안전진단 지침 등에 의거한 안전등급 판정 절차 위반.  \n",
      "- **과거 처분 추정**: 유사 사례에서 안전등급 오류를 시정하고 관계자 주의 환기 차원에서 **‘경고’** 또는 **‘통보’** 조치가 이루어졌을 것으로 예상됨.  \n",
      "\n",
      "### 2. 추천 처분 수준 및 근거  \n",
      "**[추천 처분 수준]**: **경고**  \n",
      "\n",
      "**[추론 및 상세 근거]**  \n",
      "1. **유사성 인용**: 안전등급 판정 오류는 저수지 붕괴 위험과 직결되며, 과거 유사 사례에서도 법령 미준수에 대해 **‘경고’** 이상의 처분이 적용되었을 가능성이 높음.  \n",
      "2. **심각성 비교**: 현 사례는 잠재적 인명·재산 피해 유발 요인이 있으므로 단순 주의보다 높은 수위인 **‘경고’** 가 적절. 다만, 명백한 고의성이나 반복적 위반 증거가 없어 **‘통보’** 는 과도함.  \n",
      "3. **최종 판단**: 안전등급 재점검 및 관계 기관 협의를 통한 시스템 개선을 유도하기 위해 **‘경고’** 를 권고하되, 향후 동일 오류 재발 시 강화된 조치를 검토해야 함.  \n",
      "\n",
      "---  \n",
      "✅ **처분 수준 선정 이유**: 공공 안전과 직결된 사안이지만, 초기 단계의 절차적 오류로 판단되어 **‘경고’** 로 시정 촉구.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트에 처분 기준 조항 미포함 시\n",
    "answer = hc_chain.invoke('농업용 저수지 제방 안전등급 판정기준 불합리의 경우 어떤 처분을 내려야할까?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "abd5dfd8-3015-4dbd-8029-1397c44f17b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. 유사 사례 분석 및 과거 처분 요약  \n",
      "유사 사례에서 농어촌공사는 저수지 안전등급 판정 시 홍수 월류 가능성 확인 시 ‘D등급 이하’로 판정해야 함에도 불구하고, 이를 누락하거나 부적절하게 평가했을 가능성이 있습니다. 이로 인해 시설 안전 관리가 소홀해질 우려가 있으며, 관계 법령(「농어촌정비법」, 「저수지댐법」) 상 안전등급 판정 기준을 위반한 것으로 보입니다.  \n",
      "\n",
      "### 2. 추천 처분 수준 및 근거  \n",
      "**[추천 처분 수준]**: **시정요구**  \n",
      "\n",
      "**[법률적 근거]**  \n",
      "「농어촌정비법」 제18조 및 시행령 제26조에 따라 저수지 안전등급 판정은 객관적 기준에 따라 이루어져야 하며, 홍수 월류 가능성 검토 누락 등은 위법·부당한 사실로 간주됩니다.  \n",
      "\n",
      "**[추론 및 상세 근거]**  \n",
      "1. **유사성 인용**: 과거 사례에서 안전등급 판정 오류 시 즉시 시정 조치와 함께 재발 방지 대책이 요구되었습니다.  \n",
      "2. **심각성 비교**: 현 사례는 시설물 안전에 직결되며, 단순 절차적 오류가 아닌 생명·재산 피해로 이어질 수 있으므로 시정요구가 필수적입니다.  \n",
      "3. **최종 판단**: 안전등급 판정 과정의 오류로 인한 잠재적 위험을 제거하기 위해 즉각적인 시정 조치가 필요합니다.  \n",
      "\n",
      "---  \n",
      "**결론**: 안전등급 판정 기준 불합리로 인한 위험 요소를 해소하기 위해 **시정요구**가 적절합니다.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트에 처분 기준 조항 포함 시\n",
    "answer = hc_chain.invoke('농업용 저수지 제방 안전등급 판정기준 불합리의 경우 어떤 처분을 내려야할까?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5b5723a-e282-4e1e-8389-977d4ef07c96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ⚖️ [농업용 저수지 제방 안전등급 판정기준 불합리] 분석 및 처분 수준 추천\n",
      "\n",
      "### 1. 유사 사례 분석 및 과거 처분 요약\n",
      "농업용 저수지의 안전등급 판정기준이 불합리하다는 사례는 농어촌공사가 정밀안전진단을 수행하는 과정에서 발생한 문제로, 저수지의 개별 시설에 대한 상태 및 안전성 평가가 제대로 이루어지지 않았음을 나타냅니다. 이와 관련된 법령 및 지침에 따르면, 저수지의 안전등급은 A부터 E까지 판별되며, 이러한 기준이 제대로 적용되지 않을 경우 안전사고의 위험이 증가할 수 있습니다. 과거 유사 사례에서는 안전관리 기준 미비로 인해 경고 처분이 내려진 바 있습니다.\n",
      "\n",
      "### 2. 추천 처분 수준 및 근거\n",
      "**[추천 처분 수준]**: 경고\n",
      "\n",
      "**[추론 및 상세 근거]**\n",
      "1. **유사성 인용:** 과거 유사 사례에서 농업용 저수지의 안전관리 기준 미비로 인해 경고 처분이 내려진 사례가 있습니다. 이 경우, 안전등급 판정 기준이 불합리하다는 점에서 유사성을 지닙니다.\n",
      "2. **심각성 비교:** 새로운 사례는 안전등급 판정 기준의 불합리함으로 인해 저수지의 안전성이 위협받을 수 있는 가능성을 내포하고 있으나, 직접적인 사고나 피해가 발생한 것은 아닙니다. 따라서 경고 처분이 적절하다고 판단됩니다.\n",
      "3. **최종 판단:** 안전관리 기준의 불합리함은 심각한 문제이지만, 현재로서는 경미한 위반으로 볼 수 있으며, 재발 방지를 위한 교육과 경고가 필요하다고 판단하여 '경고' 처분을 추천합니다.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트에 처분 기준 조항 미포함 시\n",
    "answer = openai_chain.invoke('농업용 저수지 제방 안전등급 판정기준 불합리의 경우 어떤 처분을 내려야할까?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "600b3170-187d-444f-9157-6ac51a9df5b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ⚖️ [새로운 감사 사례] 분석 및 처분 수준 추천\n",
      "\n",
      "### 1. 유사 사례 분석 및 과거 처분 요약\n",
      "농업용 저수지 제방 안전등급 판정기준의 불합리성에 대한 사례는 농어촌공사가 정밀안전진단을 수행하는 과정에서 발생한 문제로, 관련 법령에 따라 저수지의 안전성을 평가하는 기준이 불합리하다는 지적이 있었습니다. 이와 관련하여, 과거 유사 사례에서는 저수지의 안전성 평가 기준이 미흡하여 안전사고의 위험이 증가한 경우, 시정요구 및 개선요구가 내려진 바 있습니다.\n",
      "\n",
      "### 2. 추천 처분 수준 및 근거\n",
      "**[추천 처분 수준]**: 개선요구\n",
      "\n",
      "**[법률적 근거]**\n",
      "제25조 제5호에 해당하며, \"감사 결과 법령상ㆍ제도상 또는 행정상 모순이 있거나 개선할 사항이 있다고 인정되는 경우\"에 해당합니다.\n",
      "\n",
      "**[추론 및 상세 근거]**\n",
      "1. **유사성 인용:** 과거 사례에서 저수지 안전성 평가 기준의 불합리성으로 인해 시정요구 및 개선요구가 내려진 사례가 있으며, 이는 안전사고 예방을 위한 필수적인 조치로 간주되었습니다.\n",
      "2. **심각성 비교:** 현재 사례는 저수지의 안전등급 판정기준이 불합리하다는 점에서 법령 및 제도상의 모순이 존재하며, 이는 향후 안전사고로 이어질 수 있는 중대한 문제입니다. 따라서, 단순한 경고나 주의요구보다는 개선요구가 적합합니다.\n",
      "3. **최종 판단:** 법률적 기준과 유사 사례를 비교할 때, 현재 사례는 명백한 개선이 필요하며, 이를 통해 안전성을 확보할 수 있는 기회를 제공해야 하므로 개선요구가 가장 적합한 처분 수준으로 판단됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트에 처분 기준 조항 포함 시\n",
    "answer = openai_chain.invoke('농업용 저수지 제방 안전등급 판정기준 불합리의 경우 어떤 처분을 내려야할까?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be9be13-d688-40ad-9c06-ebba44207e36",
   "metadata": {},
   "source": [
    "### [평가지표] (Evaluator, RAGAS 사용 가능)\n",
    "1. **검색 성능 평가**: 사용자의 질문에 가장 관련성 높은 문서 청크를 얼마나 잘 찾아내는가?  \n",
    "    - Recall: 정답 문서가 검색된 상위 K개의 결과 안에 포함되었는지 측정\n",
    "    - Precision: 검색된 상위 K개의 결과 중 정답에 관련된 문서의 비율 측정\n",
    "    - MAP: 검색된 결과의 순서를 고려하여 정밀도 평균화한 값 (높을수록 좋음)\n",
    "    - MRR: 첫번째 정답 문서의 순위 역수를 평균화한 값 (1에 가까울 수록 좋음)\n",
    "<br><br>\n",
    "2. **생성 성능 평가**: LLM이 검색된 컨텍스트를 사용하여 얼마나 정확한 답변을 생성하는가?\n",
    "    - Faithfulnsess: 답변이 컨텍스트만을 사용하여 작성되었는지, 환각 여부 확인 (높을수록 좋음)\n",
    "    - Answer Relevancy: 답변이 사용자 질문에 얼마나 관련성이 높은지 (높을수록 좋음)\n",
    "    - Context Utilization: 답변을 생성하는 데 검색된 컴텍스트가 얼마나 유용하게 쓰였는지 (높을수록 좋음)\n",
    "<br><br>\n",
    "3. **종합 성능 평가**: 최종 사용자가 느끼는 만족도와 시스템의 최종 목표 달성 여부 평가\n",
    "    - LLM이 생성한 답변과 사람이 작성한 정답 간의 의미적 유사도 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd08e5-7362-4c3c-bbc1-0f7e567dd51d",
   "metadata": {},
   "source": [
    "##### Ragas의 한계\n",
    "- LLM이 판단\n",
    "    - 완전무결한 평가지표라고 할 수 없음\n",
    "    - Judge model의 성능에 따라 점수 천차만별\n",
    "    - LLM의 특성으로, 평가할 때마다 점수가 조금씩 달라지므로 측정의 신뢰도 저해 => 단순한 점수 차이가 실제 개선인지, 노이즈인지 판단 어려움\n",
    "    - 객관성 부족\n",
    "- **측정이 비용효율적 (CI/CD 파이프라인에 통합하여 품질 유지할 수 있게 해줌) => 이제 가장 큰 사용 이유**\n",
    "- 사용 방법\n",
    "    - 상대적 성능 비교로 활용\n",
    "    - 그나마 신뢰도 높은 Judge Model 사용 (비용 고려 필요)\n",
    "    - temperature를 0으로 설정(그럼에도 변동성 존재)\n",
    "    - 대규모 데이터셋에 대해서 평가 (충분히 큰 수의 질문-답변 쌍에 대해 평가를 수행하여 개별 샘플의 측정 오류 희석)\n",
    "    - 인간 검증 동반"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b39f1-682d-4017-a3cc-315f1319bd9d",
   "metadata": {},
   "source": [
    "##### Judge Model 영향\n",
    "- gpt-4o-mini: 가끔 출력형태 불안정으로 오류 발생, 가장 저렴\n",
    "- gpt-3.5-turbo: 안정적인 출력, gpt-4o-mini에 비해 3배 정도 비쌈\n",
    "- gpt-4: 안정적인 출력, gpt-4o-mini에 비해 30배 정도 비쌈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "89966f07-f8b1-46ab-857b-0a3026c6eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 리스트\n",
    "evaluation_questions = [\n",
    "    '연구책임자가 직무관련자와 공모하여 수의계약을 체결한 대가로 1억 5천만원에 달하는금품을 수수한 경우 한국토지주택공사 사장에게 어떤 처분을 내려야해?', \n",
    "    '행정안전부에서 농업용 저수지 붕괴 등의 사고가 발생했음에도 사고 원인 조사를 실시하지 않은 경우 행정안전부 장관에게 어떤 처분을 내려야할까?',\n",
    "    '보강공법 선정기준을 적용하지 않아 인버트 시공을 제외하여 피해가 발생하였으며,  터널 궤도 융기 발생에 대한 보수, 보강 대책이 부적정한 경우 국가철도공단 이사장에게 어떤 처분을 내려야해?',\n",
    "    '이천시에서 입찰의 공정성을 저해하고, 특정 업체에 유리한 기술검토를 진행한 경우 이천시장에게 어떤 처분을 내려야해?',\n",
    "    '지방자치단체에서 권한이 주어진 목적과 실체적 관련이 없는 다른 목적으로 인허가 협의 절차를 중단하여 인허가권을 남용한 경우 지방자체단체장에게 어떤 처분을 내려야할까?',\n",
    "    '법령상 의무를 충분히 이행할 수 있는 상황이었음에도 경기도교육청 관내 각급 학교 중 35%에 해당하는 학교가 법령상 선임기한까지 기계설비유지관리자를 선임하지 않고 있던 경우, 지휘감독 업무를 소홀히 한경우 경기도 교육감에게 어떤 처분을 내려야해?',\n",
    "    '보조금수령자가 지금목적과 다른 용도에 사용했음에도, 보조금을 환수하지 않은경우, 해당 지역 군수에게 어떤 처분을 내려야할까?',\n",
    "    '철저한 조사 없이 퇴직자가 실질적 이사로 근무하는 업체와 단일 견적의 수의계약을 진행하고, 체결 후 결격 사유를 알게되었음에도 사후조치를 철저히 하지 않은 경우 도로공사 사장에게 어떤 추분을 내려야 할까?',\n",
    "    '연구직공무원 및 공무직 채용과정에서 응시자격을 사후 변경하고, 정량평가를 잘못하는 등 서류전형 업무를 소홀히 한 경우, 해당 기관의 청장에게 어떤 처분을 내려야 할까?',\n",
    "    '병무청에서 민원서비스 종합평가에서 최우수기관으로 선정될 목적으로 소속 직원들이 조직적으로 만족도 향상 정도 점수를 부풀려 우수기관 선정 및 대통령 표창과 포상금을 받은 경우 행정안전부장관에게 어떤 처분을 내려야해?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "07a53fea-f936-40b6-af68-78dfe0263d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperClova Ragas 평가용\n",
    "hc_chain_with_context = (\n",
    "    # 1. 입력 문자열을 {'question': str}로 변환\n",
    "    RunnableLambda(lambda x: {'question': x})\n",
    "    |\n",
    "    # 2. 검색 및 원본 컨텍스트 저장\n",
    "    RunnablePassthrough.assign(\n",
    "        docs=itemgetter(\"question\") | hc_retriever \n",
    "    )\n",
    "    |\n",
    "    # 3. LLM이 사용할 포맷된 컨텍스트 생성\n",
    "    RunnablePassthrough.assign(\n",
    "        context=itemgetter(\"docs\") | RunnableLambda(format_docs) \n",
    "    )\n",
    "    |\n",
    "    # 4. Prompt 입력 준비: 튜플이 아닌 딕셔너리 형태로 qa_prompt에 전달\n",
    "    RunnablePassthrough.assign(\n",
    "        llm_input={\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"context\": itemgetter(\"context\")\n",
    "        }\n",
    "    )\n",
    "    |\n",
    "    # 5. 답변 생성: llm_input 딕셔너리를 받아 Prompt에 전달\n",
    "    RunnablePassthrough.assign(\n",
    "        answer=itemgetter(\"llm_input\") | qa_prompt | hc_llm | StrOutputParser()\n",
    "    )\n",
    "    |\n",
    "    # 6. 최종 출력 정리: Ragas용 딕셔너리 반환\n",
    "    {\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"answer\": itemgetter(\"answer\"),\n",
    "        \"raw_contexts_for_ragas\": itemgetter(\"docs\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9271ee72-5cc7-4ac2-b880-b7ea771d4a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0abc7d6c698435886d4d06d561f70c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203f59529e044ba5a6da2c17d330273f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31f5a2aeb4549e1b75f4b20cfdf3532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535a259c4829442c9cb03e27f69656a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[3]: ValidationError(1 validation error for StatementGeneratorOutput\n",
      "  Invalid JSON: EOF while parsing a string at line 76 column 71 [type=json_invalid, input_value='{\\n    \"statements\": [\\n... 붕괴 사고의 원', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/json_invalid)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acece2553f074e8ca8667f298e96eae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d1fda5034c4bef80917bed5150d545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599b21e6a8274d5fb315a42290addc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721e49e235da4a83a1999de473747051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58676552bae34460b4eea270c28e9c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1518bb145e2a4a7b857f53a3ffff0ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faithfulness_li = []\n",
    "answer_relevancy_li = []\n",
    "context_precision_li = []\n",
    "\n",
    "for i in range(10):\n",
    "    results = []\n",
    "    \n",
    "    # 수정된 체인으로 호출\n",
    "    for q in evaluation_questions:\n",
    "        # result_dict는 딕셔너리\n",
    "        result_dict = hc_chain_with_context.invoke(q) \n",
    "    \n",
    "        results.append({\n",
    "            'question': result_dict['question'],\n",
    "            'answer': result_dict['answer'],\n",
    "            'contexts': [doc.page_content for doc in result_dict['raw_contexts_for_ragas']],\n",
    "            'reference': result_dict['answer']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Ragas 평가용 Dataset 객체 생성\n",
    "    ragas_dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    judge_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    \n",
    "    faithfulness.llm = judge_model\n",
    "    answer_relevancy.llm = judge_model\n",
    "    context_precision.llm = judge_model\n",
    "    \n",
    "    metrics = [\n",
    "        faithfulness,       \n",
    "        answer_relevancy, \n",
    "        context_precision\n",
    "    ]\n",
    "    \n",
    "    result = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    faithfulness_li.append(np.mean(result['faithfulness']))\n",
    "    answer_relevancy_li.append(np.mean(result['answer_relevancy']))\n",
    "    context_precision_li.append(np.mean(result['context_precision']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "005f6210-aa32-428a-aa45-15f6fbb05652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness: 0.4731\n",
      "answer_relevancy: 0.5183\n",
      "context_precision: 0.8706\n"
     ]
    }
   ],
   "source": [
    "print(f'faithfulness: {np.mean(result['faithfulness']):.4f}\\nanswer_relevancy: {np.mean(result['answer_relevancy']):.4f}\\ncontext_precision: {np.mean(result['context_precision']):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8529d280-413d-4d31-a9b2-4d8bae68965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Ragas 평가용\n",
    "openai_chain_with_context = (\n",
    "    # 1. 입력 문자열을 {'question': str}로 변환\n",
    "    RunnableLambda(lambda x: {'question': x})\n",
    "    |\n",
    "    # 2. 검색 및 원본 컨텍스트 저장\n",
    "    RunnablePassthrough.assign(\n",
    "        docs=itemgetter(\"question\") | openai_retriever \n",
    "    )\n",
    "    |\n",
    "    # 3. LLM이 사용할 포맷된 컨텍스트 생성\n",
    "    RunnablePassthrough.assign(\n",
    "        context=itemgetter(\"docs\") | RunnableLambda(format_docs) \n",
    "    )\n",
    "    |\n",
    "    # 4. Prompt 입력 준비: 튜플이 아닌 딕셔너리 형태로 qa_prompt에 전달\n",
    "    RunnablePassthrough.assign(\n",
    "        llm_input={\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"context\": itemgetter(\"context\")\n",
    "        }\n",
    "    )\n",
    "    |\n",
    "    # 5. 답변 생성: llm_input 딕셔너리를 받아 Prompt에 전달\n",
    "    RunnablePassthrough.assign(\n",
    "        answer=itemgetter(\"llm_input\") | qa_prompt | openai_llm | StrOutputParser()\n",
    "    )\n",
    "    |\n",
    "    # 6. 최종 출력 정리: Ragas용 딕셔너리 반환\n",
    "    {\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"answer\": itemgetter(\"answer\"),\n",
    "        \"raw_contexts_for_ragas\": itemgetter(\"docs\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "072f20a2-2c64-4e7c-adda-504172a707fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f545242bf646a3a9fff3592efb2b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ca3578d247427ab82fa355d8ec8b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85f29eb2a034e3cab016b6b7c3f9443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164f5253d53b4d2f9668ce159288eca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b9249736c8410f91157b82094277ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a685df7ff5a84678bf6d4981de3470aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bb4b4e54314478841e1325176ad9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0b52dba6054f7b9a568f3f7fb2f576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6882326c208b4c34b16c6eaee33edac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9ff1872b824b11ae081932884446d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faithfulness_li = []\n",
    "answer_relevancy_li = []\n",
    "context_precision_li = []\n",
    "\n",
    "for i in range(10):\n",
    "    results = []\n",
    "    \n",
    "    ## 체인으로 호출\n",
    "    for q in evaluation_questions:\n",
    "        # result_dict는 딕셔너리\n",
    "        result_dict = openai_chain_with_context.invoke(q) \n",
    "    \n",
    "        results.append({\n",
    "            'question': result_dict['question'],\n",
    "            'answer': result_dict['answer'],\n",
    "            'contexts': [doc.page_content for doc in result_dict['raw_contexts_for_ragas']],\n",
    "            'reference': result_dict['answer']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Ragas 평가용 Dataset 객체 생성\n",
    "    ragas_dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    judge_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    \n",
    "    faithfulness.llm = judge_model\n",
    "    answer_relevancy.llm = judge_model\n",
    "    context_precision.llm = judge_model\n",
    "    \n",
    "    metrics = [\n",
    "        faithfulness,       \n",
    "        answer_relevancy,\n",
    "        context_precision\n",
    "    ]\n",
    "    \n",
    "    result = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    faithfulness_li.append(np.mean(result['faithfulness']))\n",
    "    answer_relevancy_li.append(np.mean(result['answer_relevancy']))\n",
    "    context_precision_li.append(np.mean(result['context_precision']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d162f63c-0584-4996-a761-0dac0de6cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness: 0.6005\n",
      "answer_relevancy: 0.5225\n",
      "context_precision: 0.8883\n"
     ]
    }
   ],
   "source": [
    "print(f'faithfulness: {np.mean(result['faithfulness']):.4f}\\nanswer_relevancy: {np.mean(result['answer_relevancy']):.4f}\\ncontext_precision: {np.mean(result['context_precision']):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f04540-38b8-43c1-8b9b-fbbbd4472b29",
   "metadata": {},
   "source": [
    "### Score 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91ebf7-9032-4353-bd72-094967264650",
   "metadata": {},
   "source": [
    "- **Faithfulness(충실도): 환각 방지, 검색된 문서들 중 하나라도 답변을 뒷받침하는 문서가 있는가?**\n",
    "    - 답변을 독립적인 문장 단위의 개별적인 명제로 나눔\n",
    "    - 각 명제가 context(검색한 전체 문서)의 내용에 의해 논리적으로 지지 되는지 판단 (context 중 하나의 문서라도 지지된다면 1, 아니면 0)\n",
    "    - 지지된 명제의 수를 전체 명제의 수로 나눔\n",
    "<br><br>\n",
    "- **Answer Relevancy(답변 적합성): LLM의 답변이 사용자의 원래 질문에 얼마나 집중했는가?**\n",
    "    - 답변을 보고, 해당 답변을 유도했을 법한 가상의 질문 여러 개 생성 (일반적으로 3개)\n",
    "    - 원래 질문과 가상 질문들 간의 의미적 유사도를 임베딩 모델을 통해 측정\n",
    "    - 측정된 유사도 점수들의 평균 계산\n",
    "<br><br>\n",
    "- **Context Precision(맥락 정확도): 검색된 Context 안에 질문에 답하는 데 불필요한 노이즈 정보가 얼마나 섞여 있는가?**\n",
    "    - 검색된 맥락을 개별 청크 또는 문장 단위로 나눔\n",
    "    - 각 청크가 질문에 관련되어 답변에 필요한 정보인지 판단\n",
    "    - 관련된 청크의 수를 전체 청크의 수로 나눔"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8de78b86-59ad-44e2-9239-4ce4be858fb8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 프롬프트에 처벌 수준 미지정\n",
    "\n",
    "#### **Judge Model: gpt-4o-mini**\n",
    "\n",
    "- HyperClova\n",
    "    - faithfulness: 0.5000\n",
    "    - answer_relevancy: 0.7853\n",
    "\n",
    "- OpenAI\n",
    "    - faithfulness: 0.2718\n",
    "    - answer_relevancy: 0.7988\n",
    "\n",
    "#### **Judge Model: gpt-3.5-turbo**\n",
    "\n",
    "- HyperClova\n",
    "    - faithfulness: 0.5389\n",
    "    - answer_relevancy: 0.7713\n",
    "\n",
    "- OpenAI\n",
    "    - faithfulness: 0.3667\n",
    "    - answer_relevancy: 0.7924\n",
    "\n",
    "### 프롬프트에 처벌 수준 지정\n",
    "\n",
    "#### **Judge Model: gpt-4o-mini**\n",
    "\n",
    "- HyperClova\n",
    "    - faithfulness: 0.7305\n",
    "    - answer_relevancy: 0.4156\n",
    "\n",
    "- OpenAI\n",
    "    - faithfulness: 0.1429\n",
    "    - answer_relevancy: 0.7982\n",
    "\n",
    "#### **Judge Model: gpt-3.5-turbo**\n",
    "\n",
    "- HyperClova\n",
    "    - faithfulness: 0.4231\n",
    "    - answer_relevancy: 0.7697\n",
    "\n",
    "- OpenAI\n",
    "    - faithfulness: 0.4318\n",
    "    - answer_relevancy: 0.3984\n",
    "\n",
    "#### **Judge Model: gpt-4**\n",
    "\n",
    "- HyperClova\n",
    "    - faithfulness: 0.256\n",
    "    - answer_relevancy: 0.792\n",
    "\n",
    "- OpenAI\n",
    "    - faithfulness: 0.1944\n",
    "    - answer_relevancy: 0.7830\n",
    "\n",
    "### 실행할 때마다 점수 달라지는 현상\n",
    "\n",
    "- faithfulness: 0.5000\n",
    "- answer_relevancy: 0.7853\n",
    "\n",
    "- faithfulness: 0.6500\n",
    "- answer_relevancy: 0.7791\n",
    "\n",
    "- faithfulness: 0.4548\n",
    "- answer_relevancy: 0.7936\n",
    "\n",
    "- faithfulness: 0.2718\n",
    "- answer_relevancy: 0.7988"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffeaf3-d568-46f6-abd5-57117bad306d",
   "metadata": {},
   "source": [
    "### 프롬프트에 처벌 수준 미지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0289d732-1e68-4286-8acf-61be802820ca",
   "metadata": {},
   "source": [
    "#### HyperClova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "205bdf98-2bbe-40c3-a4b5-698ec5d9e3eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness 평균값: 0.6254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.590138888888889),\n",
       " np.float64(nan),\n",
       " np.float64(0.5548726763432645),\n",
       " np.float64(0.6078138528138528),\n",
       " np.float64(0.635892857142857),\n",
       " np.float64(nan),\n",
       " np.float64(0.7327106227106228),\n",
       " np.float64(nan),\n",
       " np.float64(0.7304334030262822),\n",
       " np.float64(0.5261363636363636)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faithfulness\n",
    "print(f'Faithfulness 평균값: {np.nanmean(faithfulness_li):.4f}')\n",
    "faithfulness_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2f2d8ab3-d405-4db6-b200-704757a03af7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy 평균값: 0.5166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.5330434038730487),\n",
       " np.float64(0.4618568835599878),\n",
       " np.float64(0.4608603676519438),\n",
       " np.float64(0.5421817083842566),\n",
       " np.float64(0.5276706340531379),\n",
       " np.float64(0.603845880135794),\n",
       " np.float64(0.44876508751496036),\n",
       " np.float64(0.458510171179194),\n",
       " np.float64(0.5311873583421178),\n",
       " np.float64(0.5981672246316949)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer_relevancy\n",
    "print(f'answer_relevancy 평균값: {np.nanmean(answer_relevancy_li):.4f}')\n",
    "answer_relevancy_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "09e88435-393b-4820-8cc8-44d241fe7fba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_precision 평균값: 0.9145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.9366666666453611),\n",
       " np.float64(0.945555555534162),\n",
       " np.float64(0.950555555534037),\n",
       " np.float64(0.870555555534037),\n",
       " np.float64(0.870555555534037),\n",
       " np.float64(0.9455555555306618),\n",
       " np.float64(0.8599999999782917),\n",
       " np.float64(0.945555555526162),\n",
       " np.float64(0.94499999997925),\n",
       " np.float64(0.8745833333108717)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context_precision\n",
    "print(f'context_precision 평균값: {np.nanmean(context_precision_li):.4f}')\n",
    "context_precision_li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721bc58c-0981-4b9c-bc75-e4271e730a2b",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c9b3fbf9-24c0-4920-8c21-f5c1df3225b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness 평균값: 0.4664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.5363528138528139),\n",
       " np.float64(0.51500777000777),\n",
       " np.float64(0.4348552754435107),\n",
       " np.float64(0.5486929736929737),\n",
       " np.float64(0.4662698412698412),\n",
       " np.float64(0.3625180375180375),\n",
       " np.float64(0.4082539682539682),\n",
       " np.float64(nan),\n",
       " np.float64(0.34868686868686866),\n",
       " np.float64(0.5770923520923522)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faithfulness\n",
    "print(f'Faithfulness 평균값: {np.nanmean(faithfulness_li):.4f}')\n",
    "faithfulness_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c6be74f5-a7fe-4ce3-b385-5ae08bcba531",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy 평균값: 0.6316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.6852624078402454),\n",
       " np.float64(0.5442103817210435),\n",
       " np.float64(0.5326987241053301),\n",
       " np.float64(0.6678528863804186),\n",
       " np.float64(0.6883498511102121),\n",
       " np.float64(0.6293018427529509),\n",
       " np.float64(0.6926334052692568),\n",
       " np.float64(0.6774501306390072),\n",
       " np.float64(0.6839593617762662),\n",
       " np.float64(0.5138646383437312)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer_relevancy\n",
    "print(f'answer_relevancy 평균값: {np.nanmean(answer_relevancy_li):.4f}')\n",
    "answer_relevancy_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "109892c1-d99e-436d-ba7e-f63b614931e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_precision 평균값: 0.8864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.8488888888678426),\n",
       " np.float64(0.9079166666457187),\n",
       " np.float64(0.8808333333130627),\n",
       " np.float64(0.9162499999801492),\n",
       " np.float64(0.9079166666457187),\n",
       " np.float64(0.8808333333130625),\n",
       " np.float64(0.8791666666467431),\n",
       " np.float64(0.8793055555359434),\n",
       " np.float64(0.9072222221941064),\n",
       " np.float64(0.8555555555352872)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context_precision\n",
    "print(f'context_precision 평균값: {np.nanmean(context_precision_li):.4f}')\n",
    "context_precision_li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50df166-0332-4888-b9c5-0dca89e77d4f",
   "metadata": {},
   "source": [
    "### 프롬프트에 처벌 수준 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7673b53-bec3-4590-901c-014479546cfa",
   "metadata": {},
   "source": [
    "#### HyperClova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "067021af-b6a3-4ddf-a472-50ed375d8281",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness 평균값: 0.5523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.5197604588394062),\n",
       " np.float64(0.5422167525457),\n",
       " np.float64(0.6522644009757013),\n",
       " np.float64(nan),\n",
       " np.float64(0.4714400584795322),\n",
       " np.float64(0.6118181818181817),\n",
       " np.float64(0.489084249084249),\n",
       " np.float64(0.4112501857410324),\n",
       " np.float64(0.7267365967365966),\n",
       " np.float64(0.5464048165793017)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faithfulness\n",
    "print(f'Faithfulness 평균값: {np.nanmean(faithfulness_li):.4f}')\n",
    "faithfulness_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0b3c960e-835b-4ac1-9a5d-29eb17a9caef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy 평균값: 0.6644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.4634267881070782),\n",
       " np.float64(0.6726889507343249),\n",
       " np.float64(0.7623098108787593),\n",
       " np.float64(0.6919665673918317),\n",
       " np.float64(0.6929547130823072),\n",
       " np.float64(0.6906811079453007),\n",
       " np.float64(0.6069352449403846),\n",
       " np.float64(0.6123995241314661),\n",
       " np.float64(0.7705778071757078),\n",
       " np.float64(0.6795993874044236)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer_relevancy\n",
    "print(f'answer_relevancy 평균값: {np.nanmean(answer_relevancy_li):.4f}')\n",
    "answer_relevancy_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0a0af6b4-ce88-4de3-96cf-54df8fefe280",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_precision 평균값: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.9566666666467499),\n",
       " np.float64(0.9866666666452361),\n",
       " np.float64(0.9916666666456111),\n",
       " np.float64(0.9887499999797813),\n",
       " np.float64(0.9916666666456111),\n",
       " np.float64(0.9887499999797813),\n",
       " np.float64(0.9866666666452361),\n",
       " np.float64(0.9866666666452361),\n",
       " np.float64(0.9693055555346517),\n",
       " np.float64(0.9866666666452361)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context_precision\n",
    "print(f'context_precision 평균값: {np.nanmean(context_precision_li):.4f}')\n",
    "context_precision_li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214faac-9aef-48af-8d66-0d31c210adca",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e10279a7-7686-41f9-9bb4-a4e9d20b0c6d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness 평균값: 0.3880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.3221428571428572),\n",
       " np.float64(0.3801098901098901),\n",
       " np.float64(0.32212842712842715),\n",
       " np.float64(0.4081938821412505),\n",
       " np.float64(0.3590277777777778),\n",
       " np.float64(0.33679065086959825),\n",
       " np.float64(0.39888680033416873),\n",
       " np.float64(0.49954790823211875),\n",
       " np.float64(0.42806637806637815),\n",
       " np.float64(0.4246794871794871)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faithfulness\n",
    "print(f'Faithfulness 평균값: {np.nanmean(faithfulness_li):.4f}')\n",
    "faithfulness_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b81cd82a-133c-43b9-aece-371a70b676c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy 평균값: 0.5250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.5956565403333716),\n",
       " np.float64(0.6045480582320883),\n",
       " np.float64(0.45455850586344687),\n",
       " np.float64(0.6657603705696366),\n",
       " np.float64(0.45505924312744633),\n",
       " np.float64(0.6012864929001872),\n",
       " np.float64(0.45072349084581625),\n",
       " np.float64(0.3039234434195962),\n",
       " np.float64(0.6655801708208977),\n",
       " np.float64(0.45334297828169595)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer_relevancy\n",
    "print(f'answer_relevancy 평균값: {np.nanmean(answer_relevancy_li):.4f}')\n",
    "answer_relevancy_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "322c7bf4-05cc-4903-818f-b40ea1ececaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_precision 평균값: 0.8841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.8705555555342871),\n",
       " np.float64(0.8974999999779651),\n",
       " np.float64(0.9138888888682593),\n",
       " np.float64(0.8655555555348009),\n",
       " np.float64(0.9155555555351341),\n",
       " np.float64(0.9045833333122465),\n",
       " np.float64(0.835416666646948),\n",
       " np.float64(0.9037499999809062),\n",
       " np.float64(0.8612499999792742),\n",
       " np.float64(0.8724999999792153)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context_precision\n",
    "print(f'context_precision 평균값: {np.nanmean(context_precision_li):.4f}')\n",
    "context_precision_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917554d-e806-4cf0-9db0-e57ef2379ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
