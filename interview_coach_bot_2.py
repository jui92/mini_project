# -*- coding: utf-8 -*-
# ==========================================================
# ÌöåÏÇ¨ ÌäπÌôî Í∞ÄÏÉÅ Î©¥Ï†ë ÏΩîÏπò (ÌÖçÏä§Ìä∏ Ï†ÑÏö© / RAG + Î†àÏù¥Îçî + CSV)
# - ÏßÅÎ¨¥ ÏÑ†ÌÉù & Ï±ÑÏö©Í≥µÍ≥† ÏûêÎèô ÏàòÏßë(Í∂åÏû•: URL ÏûÖÎ†•, ÏóÜÏúºÎ©¥ Í≤ÄÏÉâ ÏãúÎèÑ)
# - ÌöåÏÇ¨ Îâ¥Ïä§/ÏµúÍ∑º Ïù¥Ïäà Î∞òÏòÅ (Google News RSS)
# - ÏßàÎ¨∏ Îã§ÏñëÏÑ± Í∞ïÌôî: ÌõÑÎ≥¥ NÍ∞ú ÏÉùÏÑ± + Î∞òÏ§ëÎ≥µ ÏÑ†ÌÉù + Î¨¥ÏûëÏúÑ Ìè¨Ïª§Ïä§
# - Ï±ÑÏö©Í≥µÍ≥† Í∏∞Ï§Ä ÏöîÏïΩ(ÌöåÏÇ¨/Í∞ÑÎã® ÏÜåÍ∞ú/Î™®ÏßëÎ∂ÑÏïº/Ï£ºÏöî ÏóÖÎ¨¥/ÏûêÍ≤© ÏöîÍ±¥)
# - Streamlit Cloud Ìò∏Ìôò, Plotly/FAISS ÏÑ†ÌÉùÏ†Å, ÏãúÌÅ¨Î¶ø ÏïàÏ†Ñ Î°úÎçî
# ==========================================================

import os, io, re, json, textwrap, urllib.parse, difflib, random, time
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timezone

import numpy as np
import pandas as pd
import streamlit as st

# ---------- Optional deps ----------
try:
    import pypdf
except Exception:
    pypdf = None

try:
    import plotly.graph_objects as go
    PLOTLY_OK = True
except Exception:
    PLOTLY_OK = False

try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` Ìå®ÌÇ§ÏßÄÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. requirements.txtÏóê openaiÎ•º Ï∂îÍ∞ÄÌñàÎäîÏßÄ ÌôïÏù∏ÌïòÏÑ∏Ïöî.")
    st.stop()

import requests
from bs4 import BeautifulSoup
try:
    import wikipedia
    try:
        wikipedia.set_lang("ko")
    except Exception:
        pass
except Exception:
    wikipedia = None

# ---------- Page config ----------
st.set_page_config(page_title="ÌöåÏÇ¨ ÌäπÌôî Í∞ÄÏÉÅ Î©¥Ï†ë ÏΩîÏπò", page_icon="üéØ", layout="wide")

# ---------- Secrets loader ----------
def _secrets_file_exists() -> bool:
    candidates = [
        os.path.join(os.path.expanduser("~"), ".streamlit", "secrets.toml"),
        os.path.join(os.getcwd(), ".streamlit", "secrets.toml"),
    ]
    return any(os.path.exists(p) for p in candidates)

def load_api_key_from_env_or_secrets() -> Optional[str]:
    key = os.getenv("OPENAI_API_KEY")
    if key: return key
    try:
        if _secrets_file_exists() or hasattr(st, "secrets"):
            return st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        pass
    return None

# ---------- Text utils ----------
def _clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", t or "").strip()

def _snippetize(text: str, maxlen: int = 240) -> str:
    t = _clean_text(text)
    return t if len(t) <= maxlen else t[: maxlen - 1] + "‚Ä¶"

def chunk_text(text: str, size: int = 900, overlap: int = 150) -> List[str]:
    text = re.sub(r"\s+", " ", text).strip()
    if not text: return []
    chunks, start = [], 0
    while start < len(text):
        end = min(len(text), start + size)
        chunks.append(text[start:end])
        if end == len(text): break
        start = max(0, end - overlap)
    return chunks

def read_file_to_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt", ".md")):
        for enc in ("utf-8", "cp949", "euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        if pypdf is None:
            st.warning("pypdfÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. requirements.txtÏóê pypdf Ï∂îÍ∞Ä.")
            return ""
        try:
            reader = pypdf.PdfReader(io.BytesIO(data))
            return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
        except Exception as e:
            st.warning(f"PDF ÌååÏã± Ïã§Ìå®({uploaded.name}): {e}")
            return ""
    return ""

# ---------- Company/domain utils ----------
VAL_KEYWORDS = [
    "ÌïµÏã¨Í∞ÄÏπò","Í∞ÄÏπò","ÎØ∏ÏÖò","ÎπÑÏ†Ñ","Î¨∏Ìôî","ÏõêÏπô","Ï≤†Ìïô","Í≥†Í∞ù","Îç∞Ïù¥ÌÑ∞","ÌòÅÏã†",
    "values","mission","vision","culture","principles","philosophy","customer","data","innovation"
]

def _domain(u: str|None) -> str|None:
    if not u: return None
    try:
        if not u.startswith("http"): u = "https://" + u
        return urllib.parse.urlparse(u).netloc.lower().replace("www.","")
    except Exception:
        return None

def _name_similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()

# ---------- Wikipedia summary ----------
def fetch_wikipedia_summary(company_name: str, homepage: str|None=None) -> dict|None:
    if wikipedia is None:
        return None
    try:
        candidates = wikipedia.search(company_name, results=10)
        if not candidates:
            return None
        target_dom = _domain(homepage)
        best = None; best_score = -1.0
        for title in candidates:
            try:
                page = wikipedia.page(title, auto_suggest=False, redirect=True)
            except Exception:
                continue
            first = _clean_text((page.summary or "").split("\n")[0])
            score = _name_similarity(company_name, page.title)
            if any(k in first for k in ["ÌöåÏÇ¨","Í∏∞ÏóÖ","Company","Corporation","Inc","Co., Ltd"]):
                score += 0.15
            page_dom = None
            try:
                page_dom = _domain(page.url)
            except Exception:
                pass
            if target_dom and page_dom and target_dom in page_dom:
                score += 0.25
            if score > best_score:
                best_score = score; best = (page, first)
        if not best: return None
        page, first = best
        return {
            "company_name": page.title,
            "wiki_summary": first
        }
    except Exception:
        return None

# ---------- Simple site scrape for values/recent ----------
def fetch_site_snippets(base_url: str | None, company_name: str | None = None) -> dict:
    if not base_url:
        return {"values": [], "recent": [], "site_name": None, "about": None}
    url0 = base_url.strip()
    if not url0.startswith("http"): url0 = "https://" + url0
    cand_paths = ["", "/", "/about", "/company", "/about-us", "/mission", "/values", "/culture"]
    values_found, recent_found = [], []
    site_name, about_para = None, None

    for path in cand_paths:
        url = url0.rstrip("/") + path
        try:
            r = requests.get(url, timeout=6)
            if r.status_code != 200 or "text/html" not in r.headers.get("content-type", ""):
                continue
            soup = BeautifulSoup(r.text, "html.parser")

            # site name / about
            if site_name is None:
                og = soup.find("meta", {"property":"og:site_name"}) or soup.find("meta", {"name":"application-name"})
                if og and og.get("content"): site_name = _clean_text(og["content"])
                elif soup.title and soup.title.string: site_name = _clean_text(soup.title.string.split("|")[0])
            if about_para is None:
                # hero/lead Îã®ÎùΩ Ï∂îÏ†ï
                hero = soup.find(["p","div"], class_=re.compile(r"(lead|hero|intro)", re.I)) if soup else None
                if hero:
                    about_para = _snippetize(hero.get_text(" "))

            for tag in soup.find_all(["h1","h2","h3","p","li"]):
                txt = _clean_text(tag.get_text(separator=" "))
                if 10 <= len(txt) <= 240:
                    if any(k.lower() in txt.lower() for k in VAL_KEYWORDS):
                        values_found.append(txt)
                    if any(k in txt for k in ["ÌîÑÎ°úÏ†ùÌä∏","Í∞úÎ∞ú","Ï∂úÏãú","ÏÑ±Í≥º","project","launched","release","delivered","improved"]):
                        recent_found.append(txt)
        except Exception:
            continue

    # name check
    if company_name and site_name and _name_similarity(company_name, site_name) < 0.35:
        values_found, recent_found = [], []

    # dedup & trim
    def dedup(lst):
        seen=set(); out=[]
        for x in lst:
            if x not in seen: seen.add(x); out.append(x)
        return out
    values_found = dedup(values_found)[:5]
    recent_found = dedup(recent_found)[:5]
    trimmed=[]
    for v in values_found:
        v2 = v.split(":",1)[-1]
        if len(v2)>60 and "," in v2:
            trimmed += [p.strip() for p in v2.split(",") if 2<=len(p.strip())<=24][:6]
        else:
            trimmed.append(v2[:60])
    return {"values": trimmed[:6], "recent": recent_found, "site_name": site_name, "about": about_para}

# ---------- Google News RSS (ÏµúÍ∑º Îâ¥Ïä§) ----------
def fetch_news(company_name: str, max_items: int = 6, lang: str = "ko") -> List[dict]:
    # Google News RSS (Í≥µÏãù API ÏïÑÎãò) ‚Äî Îã®Ïàú ÌååÏã±
    q = urllib.parse.quote(company_name)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    items = []
    try:
        r = requests.get(url, timeout=8)
        if r.status_code != 200:
            return []
        soup = BeautifulSoup(r.text, "xml")
        for it in soup.find_all("item")[:max_items]:
            title = _clean_text(it.title.get_text()) if it.title else ""
            link = it.link.get_text() if it.link else ""
            pub = it.pubDate.get_text() if it.pubDate else ""
            items.append({"title": title, "link": link, "pubDate": pub})
    except Exception:
        return []
    return items

# ---------- Job posting discovery & parsing ----------
SEARCH_ENGINES = [
    # DuckDuckGo HTML endpoint (Ïã¨Ìîå, Ï∞®Îã® Í∞ÄÎä•ÏÑ± ÎÇÆÏùå)
    "https://duckduckgo.com/html/?q={query}"
]
JOB_SITES = [
    "wanted.co.kr", "saramin.co.kr", "jobkorea.co.kr", "rocketpunch.com",
    "indeed.com", "linkedin.com", "recruit.navercorp.com", "kakao.recruit", "naver"
]

def discover_job_posting_urls(company_name: str, role: str, limit: int = 5) -> List[str]:
    # Í∞ÑÎã® Í≤ÄÏÉâ: "company role site:wanted.co.kr OR site:saramin.co.kr ..."
    site_part = " OR ".join([f'site:{d}' for d in JOB_SITES])
    q = f'{company_name} {role} ({site_part})'
    urls = []
    for engine in SEARCH_ENGINES:
        url = engine.format(query=urllib.parse.quote(q))
        try:
            r = requests.get(url, timeout=8, headers={"User-Agent":"Mozilla/5.0"})
            if r.status_code != 200: 
                continue
            soup = BeautifulSoup(r.text, "html.parser")
            for a in soup.find_all("a", href=True):
                href = a["href"]
                # DDGÎäî Î¶¨Îã§Ïù¥Î†âÌä∏ ÎßÅÌÅ¨Ïùº Ïàò ÏûàÏùå
                if href.startswith("/l/?kh=-1&uddg="):
                    href = urllib.parse.unquote(href.split("/l/?kh=-1&uddg=")[-1])
                dom = _domain(href)
                if not dom: 
                    continue
                if any(d in dom for d in JOB_SITES):
                    if href not in urls:
                        urls.append(href)
                if len(urls) >= limit:
                    break
        except Exception:
            continue
    return urls[:limit]

def _extract_json_ld_job(soup: BeautifulSoup) -> Optional[dict]:
    # schema.org JobPosting JSON-LD Ï∂îÏ∂ú
    for s in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(s.string or "")
            if isinstance(data, list):
                for obj in data:
                    typ = obj.get("@type") if isinstance(obj, dict) else None
                    if (isinstance(typ, list) and "JobPosting" in typ) or typ == "JobPosting":
                        return obj
            elif isinstance(data, dict):
                typ = data.get("@type")
                if (isinstance(typ, list) and "JobPosting" in typ) or typ == "JobPosting":
                    return data
        except Exception:
            continue
    return None

def parse_job_posting(url: str) -> dict:
    # Í≥µÍ≥† ÌéòÏù¥ÏßÄÏóêÏÑú Î™®ÏßëÎ∂ÑÏïº/Ï£ºÏöîÏóÖÎ¨¥/ÏûêÍ≤©ÏöîÍ±¥ Ï∂îÏ∂ú (1) JSON-LD Ïö∞ÏÑ† (2) Ìó§Îî©/ÌÇ§ÏõåÎìú Ìú¥Î¶¨Ïä§Ìã±
    out = {"title": None, "responsibilities": [], "qualifications": [], "company_intro": None}
    try:
        r = requests.get(url, timeout=10, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
            return out
        soup = BeautifulSoup(r.text, "html.parser")

        # (1) JSON-LD JobPosting
        jp = _extract_json_ld_job(soup)
        if jp:
            out["title"] = jp.get("title")
            # desc ‚Üí Î¨∏Ïû•/Î∂àÎ¶ø Î∂ÑÌï¥
            desc = _clean_text(jp.get("description", ""))
            if desc:
                bullets = re.split(r"[‚Ä¢\-\n‚Ä¢¬∑‚ñ™Ô∏è‚ñ∂Ô∏é]+", desc)
                bullets = [b.strip(" -‚Ä¢¬∑‚ñ™Ô∏è‚ñ∂Ô∏é") for b in bullets if len(b.strip()) > 3]
                # Í∞ÑÎã® Í∑úÏπôÏúºÎ°ú responsibilities/qualifications Î∂ÑÌï†
                for b in bullets:
                    if any(k in b for k in ["ÏûêÍ≤©", "ÏöîÍ±¥", "requirements", "qualification", "ÌïÑÏàò", "Ïö∞ÎåÄ"]):
                        out["qualifications"].append(b)
                    else:
                        out["responsibilities"].append(b)

        # (2) Ìú¥Î¶¨Ïä§Ìã±: Ìó§ÎçîÏóê Í∏∞Î∞òÌïú ÏÑπÏÖò Ï∂îÏ∂ú
        sections = {}
        for h in soup.find_all(re.compile("^h[1-4]$")):
            head = _clean_text(h.get_text())
            if not head: 
                continue
            nxt = []
            sib = h.find_next_sibling()
            stop_at = {"h1","h2","h3","h4"}
            while sib and sib.name not in stop_at:
                if sib.name in {"p","li","ul","ol","div"}:
                    txt = _clean_text(sib.get_text(" "))
                    if len(txt) > 5: nxt.append(txt)
                sib = sib.find_next_sibling()
            if nxt:
                sections[head] = " ".join(nxt)

        # ÌÇ§ÏõåÎìú Îß§Ïπ≠
        resp_keys = ["Ï£ºÏöî ÏóÖÎ¨¥","Îã¥Îãπ ÏóÖÎ¨¥","ÏóÖÎ¨¥","Responsibilities","What you will do","Role"]
        qual_keys = ["ÏûêÍ≤© ÏöîÍ±¥","ÏßÄÏõê ÏûêÍ≤©","Ïö∞ÎåÄ","Requirements","Qualifications","Must have","Preferred"]
        def pick(keys):
            for k in sections:
                if any(kk.lower() in k.lower() for kk in keys):
                    return sections[k]
            return None

        if not out["responsibilities"]:
            resp = pick(resp_keys)
            if resp:
                out["responsibilities"] = [x for x in re.split(r"[‚Ä¢\-\n‚Ä¢¬∑‚ñ™Ô∏è‚ñ∂Ô∏é]+", resp) if len(x.strip())>3][:12]

        if not out["qualifications"]:
            qual = pick(qual_keys)
            if qual:
                out["qualifications"] = [x for x in re.split(r"[‚Ä¢\-\n‚Ä¢¬∑‚ñ™Ô∏è‚ñ∂Ô∏é]+", qual) if len(x.strip())>3][:12]

        # ÌöåÏÇ¨ ÏÜåÍ∞ú Ï∂îÏ†ï
        meta_desc = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
        if meta_desc and meta_desc.get("content"):
            out["company_intro"] = _snippetize(meta_desc["content"], 220)
    except Exception:
        pass
    # ÏµúÏ¢Ö Ï†ïÎ¶¨
    out["responsibilities"] = [ _snippetize(x, 140) for x in out["responsibilities"] ][:12]
    out["qualifications"]  = [ _snippetize(x, 140) for x in out["qualifications"]   [:12]]
    return out

# ---------- OpenAI client ----------
with st.sidebar:
    st.title("üéØ Í∞ÄÏÉÅ Î©¥Ï†ë ÏΩîÏπò")

    API_KEY = load_api_key_from_env_or_secrets()
    if not API_KEY:
        st.info("ÌôòÍ≤ΩÎ≥ÄÏàò/SecretsÏóêÏÑú ÌÇ§Î•º Î™ª Ï∞æÏïòÏäµÎãàÎã§. ÏïÑÎûòÏóê ÏûÖÎ†•ÌïòÎ©¥ Ï¶âÏãú ÏÇ¨Ïö© Í∞ÄÎä•Ìï¥Ïöî.")
        API_KEY = st.text_input("OPENAI_API_KEY", type="password")

    MODEL = st.selectbox("Ï±ó Î™®Îç∏", ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"], index=0)
    EMBED_MODEL = st.selectbox("ÏûÑÎ≤†Îî© Î™®Îç∏", ["text-embedding-3-small", "text-embedding-3-large"], index=0)

    # ÎîîÎ≤ÑÍ∑∏
    _openai_ver = None; _httpx_ver = None
    try:
        import openai as _openai_pkg; _openai_ver = getattr(_openai_pkg, "__version__", None)
    except Exception: pass
    try:
        import httpx as _httpx_pkg; _httpx_ver = getattr(_httpx_pkg, "__version__", None)
    except Exception: pass
    with st.expander("ÎîîÎ≤ÑÍ∑∏: ÏãúÌÅ¨Î¶ø/Î≤ÑÏ†Ñ ÏÉÅÌÉú"):
        st.write({
            "env_has_key": bool(os.getenv("OPENAI_API_KEY")),
            "api_key_provided": bool(API_KEY),
            "openai_version": _openai_ver,
            "httpx_version": _httpx_ver,
        })

if not API_KEY:
    st.error("OpenAI API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. (Cloud: App ‚Üí Settings ‚Üí Secrets)")
    st.stop()

try:
    client = OpenAI(api_key=API_KEY, timeout=30.0)
except TypeError:
    st.error("OpenAI Ï¥àÍ∏∞Ìôî TypeError. requirements.txtÏóêÏÑú openai==1.44.0, httpx==0.27.2Î°ú Í≥†Ï†ï ÌõÑ Clear cache ‚Üí Reboot Ìï¥Ï£ºÏÑ∏Ïöî.")
    st.stop()
except Exception as e:
    st.error(f"OpenAI ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî Ïò§Î•ò: {e}")
    st.stop()

# ---------- Sidebar: ÌöåÏÇ¨/ÏßÅÎ¨¥ + ÏûêÎèô ÌîÑÎ°úÌïÑ + Í≥µÍ≥†/Îâ¥Ïä§ + RAG ----------
with st.sidebar:
    st.markdown("---")
    st.markdown("#### ÌöåÏÇ¨/ÏßÅÎ¨¥ ÏÑ§Ï†ï")

    # ÏßÅÎ¨¥ ÏßÅÏ†ë ÏÑ†ÌÉù/ÏûÖÎ†• (Î¨∏Ï†ú 1 Ìï¥Í≤∞)
    role_title = st.text_input("ÏßÄÏõê ÏßÅÎ¨¥Î™Ö (Ïòà: Îç∞Ïù¥ÌÑ∞ Ïï†ÎÑêÎ¶¨Ïä§Ìä∏, ML ÏóîÏßÄÎãàÏñ¥)")

    st.markdown("#### üîé ÏûêÎèô ÌîÑÎ°úÌïÑ ÏÉùÏÑ± (ÌöåÏÇ¨/ÌôàÌéòÏù¥ÏßÄ/Ï±ÑÏö©Í≥µÍ≥†)")
    auto_name = st.text_input("ÌöåÏÇ¨ Ïù¥Î¶Ñ (Ïòà: ÎÑ§Ïù¥Î≤Ñ, Kakao, Samsung SDS)")
    auto_home = st.text_input("ÌôàÌéòÏù¥ÏßÄ URL (ÏÑ†ÌÉù)")
    job_url = st.text_input("Ï±ÑÏö© Í≥µÍ≥† URL (ÏÑ†ÌÉù) ‚Äî ÏóÜÏúºÎ©¥ ÏïÑÎûò Î≤ÑÌäºÏúºÎ°ú Í≤ÄÏÉâ ÏãúÎèÑ")

    col_a, col_b = st.columns(2)
    with col_a:
        auto_add_to_rag = st.checkbox("ÌôàÌéòÏù¥ÏßÄ/Îâ¥Ïä§/Í≥µÍ≥†Î•º RAGÏóê Ï∂îÍ∞Ä", value=True)
    with col_b:
        diversity_k = st.slider("ÏßàÎ¨∏ ÌõÑÎ≥¥ Í∞úÏàò", 3, 8, 6, 1)

    if st.button("ÌöåÏÇ¨/ÏßÅÎ¨¥ ÏûêÎèô ÏÑ∏ÌåÖ"):
        if not auto_name.strip():
            st.warning("ÌöåÏÇ¨ Ïù¥Î¶ÑÏùÑ ÏûÖÎ†•Ìï¥ Ï£ºÏÑ∏Ïöî.")
        else:
            with st.spinner("ÌöåÏÇ¨¬∑ÏßÅÎ¨¥¬∑Í≥µÍ≥†¬∑Îâ¥Ïä§ ÏàòÏßë Ï§ë..."):
                # ÌöåÏÇ¨ Í∏∞Î≥∏(ÏúÑÌÇ§+ÌôàÌéòÏù¥ÏßÄ)
                wiki = fetch_wikipedia_summary(auto_name.strip(), auto_home.strip() or None) or {}
                site = fetch_site_snippets(auto_home.strip() or None, auto_name.strip()) if auto_home.strip() else {"values": [], "recent": [], "site_name": None, "about": None}

                # Í≥µÍ≥† URLÏù¥ ÏóÜÏúºÎ©¥ Í≤ÄÏÉâ ÏãúÎèÑ (Î¨∏Ï†ú 2 Ìï¥Í≤∞)
                jp_data = {"title": None, "responsibilities": [], "qualifications": [], "company_intro": None}
                discovered = []
                if job_url.strip():
                    discovered = [job_url.strip()]
                else:
                    if role_title.strip():
                        discovered = discover_job_posting_urls(auto_name.strip(), role_title.strip(), limit=4)

                if discovered:
                    jp_data = parse_job_posting(discovered[0])

                # Îâ¥Ïä§ (Î¨∏Ï†ú 3 Ìï¥Í≤∞)
                news_items = fetch_news(auto_name.strip(), max_items=6, lang="ko")

                # company Í∞ùÏ≤¥ Íµ¨ÏÑ±
                company = {
                    "company_name": site.get("site_name") or wiki.get("company_name") or auto_name.strip(),
                    "homepage": auto_home.strip() or None,
                    "wiki_summary": wiki.get("wiki_summary"),
                    "values": site.get("values", []),
                    "recent_projects": site.get("recent", []),
                    "role": role_title.strip(),
                    "role_requirements": jp_data["responsibilities"] or [],
                    "role_qualifications": jp_data["qualifications"] or [],
                    "job_url": discovered[0] if discovered else (job_url.strip() or None),
                    "company_intro": jp_data["company_intro"] or site.get("about"),
                    "news": news_items
                }
                st.session_state["company_override"] = company

                # RAG ÏûêÎèô Ìà¨ÏûÖ (Î¨∏Ï†ú 3, 5Ïùò Í∑ºÍ±∞ Í∞ïÌôî)
                if auto_add_to_rag:
                    texts = []
                    # ÌôàÌéòÏù¥ÏßÄ ÌÖçÏä§Ìä∏ Î™á Í≤ΩÎ°ú
                    if auto_home.strip():
                        for p in ["", "/about", "/values", "/mission", "/company"]:
                            u = auto_home.strip().rstrip("/") + p
                            try:
                                r = requests.get(u, timeout=6)
                                if r.status_code == 200 and "text/html" in r.headers.get("content-type",""):
                                    s = BeautifulSoup(r.text, "html.parser")
                                    txts = [_clean_text(t.get_text(" ")) for t in s.find_all(["h1","h2","h3","p","li"])]
                                    page = "\n".join([t for t in txts if len(t) > 10])
                                    if page: texts.append(page)
                            except Exception:
                                pass
                    # Îâ¥Ïä§ Î≥∏Î¨∏ÏùÄ ÎèÑÎ©îÏù∏ Ï∞®Îã®Ïù¥ ÏûàÏùÑ Ïàò ÏûàÏúºÎãà Ï†úÎ™©+ÎßÅÌÅ¨+ÎÇ†ÏßúÎ°úÎßå
                    if news_items:
                        news_text = "\n".join([f"[NEWS] {n['title']} ({n.get('pubDate','')}) {n['link']}" for n in news_items])
                        texts.append(news_text)
                    # Ï±ÑÏö© Í≥µÍ≥† ÌÖçÏä§Ìä∏
                    if jp_data["responsibilities"] or jp_data["qualifications"]:
                        job_text = "Ï£ºÏöî ÏóÖÎ¨¥:\n- " + "\n- ".join(jp_data["responsibilities"]) + "\nÏûêÍ≤© ÏöîÍ±¥:\n- " + "\n- ".join(jp_data["qualifications"])
                        texts.append(job_text)

                    if texts:
                        chs = []
                        for t in texts:
                            chs += chunk_text(t, 900, 150)
                        try:
                            embs = client.embeddings.create(model="text-embedding-3-small", input=chs)
                            embs = np.array([d.embedding for d in embs.data], dtype=np.float32)
                            if "rag_store" not in st.session_state:
                                st.session_state.rag_store = {"chunks": [], "embeds": None}
                            st.session_state.rag_store["chunks"] = (st.session_state.rag_store.get("chunks", []) or []) + chs
                            if st.session_state.rag_store.get("embeds") is None or st.session_state.rag_store["embeds"].size == 0:
                                st.session_state.rag_store["embeds"] = embs
                            else:
                                st.session_state.rag_store["embeds"] = np.vstack([st.session_state.rag_store["embeds"], embs])
                        except Exception:
                            st.info("ÏùºÎ∂Ä ÌÖçÏä§Ìä∏Îäî RAG Ïù∏Îç±Ïã±ÏóêÏÑú Ï†úÏô∏ÎêòÏóàÏäµÎãàÎã§(ÏöîÏ≤≠ Ï†úÌïú/Ï∞®Îã® Í∞ÄÎä•).")
            st.success("ÏûêÎèô ÏÑ∏ÌåÖ ÏôÑÎ£å!")

    st.markdown("---")
    st.markdown("#### RAG (ÏÑ†ÌÉù ÏóÖÎ°úÎìú)")
    rag_enabled = st.toggle("ÌöåÏÇ¨ Î¨∏ÏÑú Í∏∞Î∞ò ÏßàÎ¨∏/ÏΩîÏπ≠ ÏÇ¨Ïö©", value=True)
    chunk_size = st.slider("Ï≤≠ÌÅ¨ Í∏∏Ïù¥(Î¨∏Ïûê)", 400, 2000, 900, 100)
    chunk_overlap = st.slider("Ïò§Î≤ÑÎû©(Î¨∏Ïûê)", 0, 400, 150, 10)
    top_k = st.slider("Í≤ÄÏÉâ ÏÉÅÏúÑ K", 1, 8, 4, 1)
    st.caption("TXT/MD/PDF ÏóÖÎ°úÎìú Í∞ÄÎä• (ÏÑ∏ÏÖò Î©îÎ™®Î¶¨ ÎÇ¥ Ï≤òÎ¶¨)")
    docs = st.file_uploader("ÌöåÏÇ¨ Î¨∏ÏÑú ÏóÖÎ°úÎìú (Ïó¨Îü¨ ÌååÏùº Í∞ÄÎä•)", type=["txt", "md", "pdf"], accept_multiple_files=True)

# ---------- company Í≤∞Ï†ï ----------
if "company_override" in st.session_state:
    company = st.session_state["company_override"]
else:
    # Ï¥àÍ∏∞ ÎçîÎØ∏ (ÏàòÎèô ÏóÖÎ°úÎìú/Í∏∞Î≥∏ ÌååÏùº ÎåÄÏã† ÏûêÎèô ÏÑ∏ÌåÖ Ïú†ÎèÑ)
    company = {
        "company_name": "(ÌöåÏÇ¨Î™Ö ÎØ∏ÏÑ§Ï†ï)",
        "homepage": None,
        "wiki_summary": None,
        "values": [],
        "recent_projects": [],
        "role": role_title,
        "role_requirements": [],
        "role_qualifications": [],
        "job_url": None,
        "company_intro": None,
        "news": []
    }

# ---------- session states ----------
if "rag_store" not in st.session_state:
    st.session_state.rag_store = {"chunks": [], "embeds": None}
if "history" not in st.session_state:
    st.session_state.history = []
if "current_question" not in st.session_state:
    st.session_state.current_question = ""

# ---------- Upload ‚Üí RAG ----------
def embed_texts(client: OpenAI, embed_model: str, texts: List[str]) -> np.ndarray:
    if not texts:
        return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=embed_model, input=texts)
    return np.array([d.embedding for d in resp.data], dtype=np.float32)

if rag_enabled and docs:
    with st.spinner("Î¨∏ÏÑú Ï≤òÎ¶¨ Ï§ë..."):
        all_chunks = []
        for up in docs:
            text = read_file_to_text(up)
            if not text: continue
            all_chunks.extend(chunk_text(text, chunk_size, chunk_overlap))
        if all_chunks:
            embeds = embed_texts(client, EMBED_MODEL, all_chunks)
            st.session_state.rag_store["chunks"] = (st.session_state.rag_store.get("chunks", []) or []) + all_chunks
            if st.session_state.rag_store.get("embeds") is None or st.session_state.rag_store["embeds"].size == 0:
                st.session_state.rag_store["embeds"] = embeds
            else:
                st.session_state.rag_store["embeds"] = np.vstack([st.session_state.rag_store["embeds"], embeds])
            st.success(f"RAG Ï§ÄÎπÑ ÏôÑÎ£å: Ï≤≠ÌÅ¨ {len(all_chunks)}Í∞ú Ï∂îÍ∞Ä")
        else:
            st.info("ÏóÖÎ°úÎìú Î¨∏ÏÑúÏóêÏÑú Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏäµÎãàÎã§.")

# ---------- Retrieval helpers ----------
def cosine_topk(matrix: np.ndarray, query: np.ndarray, k: int = 4):
    if matrix.size == 0:
        return np.array([]), np.array([], dtype=int)
    qn = query / (np.linalg.norm(query, axis=1, keepdims=True) + 1e-12)
    mn = matrix / (np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12)
    sims = mn @ qn.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    scores = sims[idx]
    return scores, idx

def retrieve_supports(query_text: str, k: int) -> List[Tuple[str, float, str]]:
    store = st.session_state.rag_store
    chunks, embeds = store.get("chunks", []), store.get("embeds", None)
    if not rag_enabled or embeds is None or len(chunks) == 0:
        return []
    qv = embed_texts(client, EMBED_MODEL, [query_text])
    scores, idxs = cosine_topk(embeds, qv, k=k)
    return [("ÌöåÏÇ¨ÏûêÎ£å", float(s), chunks[int(i)]) for s, i in zip(scores, idxs)]

# ---------- Diversity helpers ----------
def _similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def pick_diverse(candidates: list[str], history: list[str], gamma: float = 0.25) -> str:
    # ÌõÑÎ≥¥Î≥Ñ Ï†êÏàò = ÌèâÍ∑† Ïú†ÏÇ¨ÎèÑ + gamma*ÌëúÏ§ÄÌé∏Ï∞® ‚Üí ÏµúÏÜå Ï†êÏàò ÏÑ†ÌÉù(Îã§ÏñëÏÑ± ÏµúÎåÄÌôî)
    if not candidates: return ""
    if not history: return random.choice(candidates)
    best = None; best_score = 1e9
    for q in candidates:
        sims = [ _similarity(q,h) for h in history ]
        if not sims: sims=[0.0]
        score = (sum(sims)/len(sims)) + gamma*(np.std(sims))
        if score < best_score:
            best_score = score; best = q
    return best

TYPE_INSTRUCTIONS = {
    "ÌñâÎèô(STAR)": "Í≥ºÍ±∞ Ïã§Î¨¥ ÏÇ¨Î°ÄÎ•º ÎÅåÏñ¥ÎÇ¥ÎèÑÎ°ù S(ÏÉÅÌô©)-T(Í≥ºÏ†ú)-A(ÌñâÎèô)-R(ÏÑ±Í≥º)Î•º Ïú†ÎèÑÌïòÎäî ÏßàÎ¨∏",
    "Í∏∞Ïà† Ïã¨Ï∏µ": "ÌïµÏã¨ Í∏∞Ïà†Ï†Å ÏùòÏÇ¨Í≤∞Ï†ï¬∑Ìä∏Î†àÏù¥ÎìúÏò§ÌîÑ¬∑ÏÑ±Îä•/ÎπÑÏö©/ÌíàÏßà ÏßÄÌëúÎ•º ÌååÍ≥†ÎìúÎäî Ïã¨Ï∏µ ÏßàÎ¨∏",
    "ÌïµÏã¨Í∞ÄÏπò Ï†ÅÌï©ÏÑ±": "ÌïµÏã¨Í∞ÄÏπòÏôÄ ÌÉúÎèÑÎ•º Í≤ÄÏ¶ùÌïòÎäî, ÏÉÅÌô©Í∏∞Î∞ò ÌñâÎèôÏùÑ Ïú†ÎèÑÌïòÎäî ÏßàÎ¨∏",
    "Ïó≠ÏßàÎ¨∏": "ÏßÄÏõêÏûêÍ∞Ä ÌöåÏÇ¨Î•º ÌèâÍ∞ÄÌï† Ïàò ÏûàÎèÑÎ°ù ÌÜµÏ∞∞Î†• ÏûàÎäî Ïó≠ÏßàÎ¨∏"
}

# ---------- Company context (Ï±ÑÏö©Í≥µÍ≥† Í∏∞Ï§Ä ÏöîÏïΩ: Î¨∏Ï†ú 5 Ìï¥Í≤∞) ----------
def build_company_context_for_prompt(c: dict) -> str:
    # ÌîÑÎ°¨ÌîÑÌä∏Ïö© (Í∞ÑÎûµ)
    base = textwrap.dedent(f"""
    [ÌöåÏÇ¨Î™Ö] {c.get('company_name','')}
    [ÌöåÏÇ¨ ÏÜåÍ∞ú] {c.get('company_intro') or c.get('wiki_summary') or ''}
    [Î™®Ïßë Î∂ÑÏïº] {c.get('role','')}
    [Ï£ºÏöî ÏóÖÎ¨¥] {", ".join(c.get('role_requirements', [])[:6])}
    [ÏûêÍ≤© ÏöîÍ±¥] {", ".join(c.get('role_qualifications', [])[:6])}
    [ÌïµÏã¨Í∞ÄÏπò] {", ".join(c.get('values', [])[:6])}
    [ÏµúÍ∑º Ïù¥Ïäà/Îâ¥Ïä§] {", ".join([_snippetize(n['title'], 70) for n in c.get('news', [])[:3]])}
    """).strip()
    return base

def build_company_summary_for_ui(c: dict) -> dict:
    return {
        "ÌöåÏÇ¨Î™Ö": c.get("company_name"),
        "Í∞ÑÎã® ÏÜåÍ∞ú": c.get("company_intro") or c.get("wiki_summary"),
        "Î™®Ïßë Î∂ÑÏïº": c.get("role"),
        "Ï£ºÏöî ÏóÖÎ¨¥(ÏöîÏïΩ)": c.get("role_requirements")[:6],
        "ÏûêÍ≤© ÏöîÍ±¥(ÏöîÏïΩ)": c.get("role_qualifications")[:6],
        "ÌïµÏã¨Í∞ÄÏπò(Ï∂îÏ†ï)": c.get("values")[:6],
        "ÌôàÌéòÏù¥ÏßÄ": c.get("homepage"),
        "Ï±ÑÏö© Í≥µÍ≥†": c.get("job_url"),
        "ÏµúÍ∑º Îâ¥Ïä§": [ n.get("title") for n in c.get("news", [])[:5] ],
    }

def build_focuses(company: dict, supports: List[Tuple[str,float,str]], k: int = 4) -> list[str]:
    # Ìè¨Ïª§Ïä§Îäî "ÏßÅÎ¨¥ ‚Üí Í≥µÍ≥†(ÏóÖÎ¨¥/ÏöîÍ±¥) ‚Üí Í∞ÄÏπò ‚Üí ÏµúÍ∑ºÏù¥Ïäà ‚Üí RAGÎ¨∏Ïû•" Ïö∞ÏÑ†
    pool = []
    if company.get("role"): pool.append(company["role"])
    pool += company.get("role_requirements", [])[:6]
    pool += company.get("role_qualifications", [])[:6]
    pool += company.get("values", [])[:6]
    pool += [ _snippetize(n['title'], 60) for n in company.get("news", [])[:4] ]
    for _,_,txt in (supports or [])[:3]:
        pool += [t.strip() for t in re.split(r"[‚Ä¢\-\n\.]", txt) if 6 < len(t.strip()) < 100][:3]
    pool = [p for p in pool if p]
    random.shuffle(pool)
    return pool[:k]

# ---------- Question generation (Î¨∏Ï†ú 3,4,5 & ÎßûÏ∂§Í∞ê Í∞ïÌôî) ----------
def gen_question(company: dict, qtype: str, level: str, supports: List[Tuple[str, float, str]], num_candidates: int = 6) -> str:
    ctx = build_company_context_for_prompt(company)
    focuses = build_focuses(company, supports, k=min(4, num_candidates))
    style = TYPE_INSTRUCTIONS.get(qtype, "Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† ÌñâÎèôÏùÑ Ïù¥ÎÅåÏñ¥ÎÇ¥Îäî ÏßàÎ¨∏")
    rag_note = ""
    if supports:
        joined = "\n".join([f"- ({s:.2f}) {txt[:200]}" for _, s, txt in supports[:3]])
        rag_note = f"\n[Í∑ºÍ±∞ Î∞úÏ∑å]\n{joined}"

    # ÎûúÎç§ÏÑ± Ìñ•ÏÉÅ: seedÎ•º ÏãúÍ∞Ñ/ÏÑ∏ÏÖò Í∏∞Î∞òÏúºÎ°ú ÏÑûÏùå (Î¨∏Ï†ú 4)
    seed = int(time.time()*1000) % 2_147_483_647
    random_factor = random.random()

    sys = f"""ÎÑàÎäî '{company.get('company_name','')}'Ïùò '{company.get('role','')}' Î©¥Ï†ëÍ¥ÄÏù¥Îã§.
ÌöåÏÇ¨ Îß•ÎùΩ, Ï±ÑÏö©Í≥µÍ≥†(Ï£ºÏöîÏóÖÎ¨¥/ÏûêÍ≤©ÏöîÍ±¥), ÏµúÍ∑º Îâ¥Ïä§/Ïù¥Ïäà, (ÏûàÎã§Î©¥) Í∑ºÍ±∞ Î¨∏ÏÑúÎ•º Î∞òÏòÅÌïòÏó¨ **{qtype}** Ïú†Ìòï({style})Ïùò ÏßàÎ¨∏ **{num_candidates}Í∞ú ÌõÑÎ≥¥**Î•º ÌïúÍµ≠Ïñ¥Î°ú ÏÉùÏÑ±ÌïòÎùº.
Í∞Å ÌõÑÎ≥¥Îäî ÏÑúÎ°ú **ÌòïÌÉú¬∑Í¥ÄÏ†ê¬∑ÌÇ§ÏõåÎìú**Í∞Ä Îã¨ÎùºÏïº ÌïúÎã§. ÎÇúÏù¥ÎèÑÎäî {level}.
ÏïÑÎûò 'Ìè¨Ïª§Ïä§' Ï§ë ÏµúÏÜå 1Í∞ú ÌÇ§ÏõåÎìúÎ•º ÏßàÏùòÎ¨∏Ïóê **Î™ÖÏãúÏ†ÅÏúºÎ°ú Ìè¨Ìï®**ÌïòÎùº.
ÏÇ¨ÏÜåÌïú Ïû¨Íµ¨ÏÑ±(ÏßÄÌëú/ÏàòÏπò/Í∏∞Í∞Ñ/Í∑úÎ™®/Î¶¨Ïä§ÌÅ¨ ÏöîÏù∏ Îì±)ÏùÑ ÏÑûÏñ¥ **ÏÑúÎ°ú Îã§Î•∏ ÏßàÎ¨∏**Ïù¥ ÎêòÎèÑÎ°ù ÌïúÎã§.
Ìè¨Îß∑: 1) ... 2) ... ... (Ìïú Ï§ÑÏî©)"""
    user = f"""[ÌöåÏÇ¨/ÏßÅÎ¨¥ Ïª®ÌÖçÏä§Ìä∏]
{ctx}
[Ìè¨Ïª§Ïä§(Î¨¥ÏûëÏúÑ ÏùºÎ∂Ä)]
- {chr(10).join(focuses)}{rag_note}
[ÎûúÎç§ÏãúÎìú] {seed}; rf={random_factor:.4f}"""

    resp = client.chat.completions.create(
        model=MODEL, temperature=0.95,  # Îã§ÏñëÏÑ± ‚Üë
        messages=[{"role":"system","content":sys},{"role":"user","content":user}]
    )
    raw = resp.choices[0].message.content.strip()
    cands = [re.sub(r'^\s*\d+\)\s*','',line).strip() for line in raw.splitlines() if re.match(r'^\s*\d+\)', line)]
    if not cands:
        cands = [l.strip("- ").strip() for l in raw.splitlines() if len(l.strip())>0][:num_candidates]

    # ÏµúÍ∑º ÏßàÎ¨∏Îì§Í≥ºÏùò Î∞òÏ§ëÎ≥µ ÏÑ†ÌÉù (Î¨∏Ï†ú 4)
    hist_qs = [h["question"] for h in st.session_state.get("history", [])][-10:]
    selected = pick_diverse(cands, hist_qs, gamma=0.35)
    return selected or (cands[0] if cands else "ÏßàÎ¨∏ ÏÉùÏÑ± Ïã§Ìå®")

# ---------- Coaching ----------
def coach_answer(company: dict, question: str, user_answer: str, supports: List[Tuple[str, float, str]]) -> Dict:
    ctx = build_company_context_for_prompt(company)
    rag_note = ""
    if supports:
        joined = "\n".join([f"- ({s:.3f}) {txt[:500]}" for (_, s, txt) in supports])
        rag_note = f"\n[ÌöåÏÇ¨ Í∑ºÍ±∞ Î¨∏ÏÑú Î∞úÏ∑å]\n{joined}\n"
    competencies = ["Î¨∏Ï†úÏ†ïÏùò", "Îç∞Ïù¥ÌÑ∞/ÏßÄÌëú", "Ïã§ÌñâÎ†•/Ï£ºÎèÑÏÑ±", "ÌòëÏóÖ/Ïª§ÎÆ§ÎãàÏºÄÏù¥ÏÖò", "Í≥†Í∞ùÍ∞ÄÏπò"]
    comp_str = ", ".join(competencies)
    sys = f"""ÎÑàÎäî ÌÜ±Ìã∞Ïñ¥ Î©¥Ï†ë ÏΩîÏπòÎã§. ÌïúÍµ≠Ïñ¥Î°ú ÏïÑÎûò ÌòïÏãùÏóê ÎßûÏ∂∞ ÎãµÌïòÎùº:
1) Ï¥ùÏ†ê: 0~10 Ï†ïÏàò 1Í∞ú
2) Í∞ïÏ†ê: 2~3Í∞ú Î∂àÎ¶ø
3) Î¶¨Ïä§ÌÅ¨: 2~3Í∞ú Î∂àÎ¶ø
4) Í∞úÏÑ† Ìè¨Ïù∏Ìä∏: 3Í∞ú Î∂àÎ¶ø (ÌñâÎèô¬∑ÏßÄÌëú¬∑ÏûÑÌå©Ìä∏ Ï§ëÏã¨)
5) ÏàòÏ†ïÎ≥∏ ÎãµÎ≥Ä: STAR(ÏÉÅÌô©-Í≥ºÏ†ú-ÌñâÎèô-ÏÑ±Í≥º) Íµ¨Ï°∞Î°ú ÏûêÏó∞Ïä§ÎüΩÍ≥† Í∞ÑÍ≤∞ÌïòÍ≤å
6) Ïó≠Îüâ Ï†êÏàò: [{comp_str}] Í∞ÅÍ∞Å 0~5 Ï†ïÏàò (Ìïú Ï§ÑÏóê ÏâºÌëúÎ°ú Íµ¨Î∂Ñ)
Ï±ÑÏ†ê Í∏∞Ï§ÄÏùÄ ÌöåÏÇ¨/ÏßÅÎ¨¥ Îß•ÎùΩ, Ï±ÑÏö©Í≥µÍ≥†(Ï£ºÏöîÏóÖÎ¨¥/ÏûêÍ≤©ÏöîÍ±¥), ÏßàÎ¨∏ ÎÇ¥ **Ìè¨Ïª§Ïä§/ÌÇ§ÏõåÎìú** Î∂ÄÌï© Ïó¨Î∂ÄÎ•º Ìè¨Ìï®ÌïúÎã§.
Ï∂îÍ∞Ä ÏÑ§Î™Ö Í∏àÏßÄ. ÌòïÏãù Ïú†ÏßÄ."""
    user = f"""[ÌöåÏÇ¨/ÏßÅÎ¨¥ Ïª®ÌÖçÏä§Ìä∏]
{ctx}
{rag_note}
[Î©¥Ï†ë ÏßàÎ¨∏]
{question}

[ÌõÑÎ≥¥Ïûê ÎãµÎ≥Ä]
{user_answer}
"""
    resp = client.chat.completions.create(
        model=MODEL, temperature=0.35,
        messages=[{"role":"system","content":sys},{"role":"user","content":user}]
    )
    content = resp.choices[0].message.content.strip()
    m = re.search(r'([0-9]{1,2})\s*(?:/10|Ï†ê|$)', content)
    score = None
    if m:
        try:
            score = int(m.group(1)); score = max(0, min(10, score))
        except: pass
    comp_scores = None
    nums = re.findall(r'\b([0-5])\b', content.splitlines()[-1])
    if len(nums) >= 5:
        comp_scores = [int(x) for x in nums[:5]]
    return {"raw": content, "score": score, "competencies": comp_scores}

# ---------- UI ----------
left, right = st.columns([1, 1])

with left:
    st.header("‚ë† Ï±ÑÏö©Í≥µÍ≥† Í∏∞Ï§Ä ÌöåÏÇ¨ ÏöîÏïΩ")
    st.json(build_company_summary_for_ui(company), expanded=True)

    st.header("‚ë° ÏßàÎ¨∏ ÏÉùÏÑ±")
    st.caption("‚ÄòÏßàÎ¨∏ ÏÉùÏÑ± ÌûåÌä∏‚ÄôÏóê ÌÇ§ÏõåÎìú(Ïòà: Ï†ÑÌôò ÌçºÎÑê, ÏÑ±Îä•-ÎπÑÏö© Ìä∏Î†àÏù¥ÎìúÏò§ÌîÑ) 1~2Í∞ú Ï†ïÎèÑÎßå ÎÑ£ÏúºÎ©¥ Îçî ÎßûÏ∂§ÌôîÎê©ÎãàÎã§.")
    q_type = st.selectbox("ÏßàÎ¨∏ Ïú†Ìòï", ["ÌñâÎèô(STAR)", "Í∏∞Ïà† Ïã¨Ï∏µ", "ÌïµÏã¨Í∞ÄÏπò Ï†ÅÌï©ÏÑ±", "Ïó≠ÏßàÎ¨∏"], index=0)
    level  = st.selectbox("ÎÇúÏù¥ÎèÑ/Ïó∞Ï∞®", ["Ï£ºÎãàÏñ¥", "ÎØ∏Îì§", "ÏãúÎãàÏñ¥"], index=0)
    prompt_hint = st.text_input("ÏßàÎ¨∏ ÏÉùÏÑ± ÌûåÌä∏(ÏÑ†ÌÉù)")

    if st.button("ÏÉà ÏßàÎ¨∏ Î∞õÍ∏∞", use_container_width=True):
        try:
            supports = []
            if rag_enabled and (docs or st.session_state.rag_store.get("chunks")):
                base_q = prompt_hint.strip() or f"{company.get('role','')} {' '.join(company.get('role_requirements', [])[:3])}"
                supports = retrieve_supports(base_q, top_k)
            q = gen_question(company, q_type, level, supports, num_candidates=diversity_k)
            st.session_state.current_question = q
            st.session_state.last_supports_q = supports
        except Exception as e:
            st.error(f"ÏßàÎ¨∏ ÏÉùÏÑ± Ïò§Î•ò: {e}")

    st.text_area("ÏßàÎ¨∏", height=110, value=st.session_state.get("current_question",""))

    if rag_enabled and st.session_state.get("last_supports_q"):
        with st.expander("ÏßàÎ¨∏ ÏÉùÏÑ±Ïóê ÏÇ¨Ïö©Îêú Í∑ºÍ±∞ Î≥¥Í∏∞"):
            for i, (_, sc, txt) in enumerate(st.session_state.last_supports_q, 1):
                st.markdown(f"**[{i}] sim={sc:.3f}**\n\n{txt[:600]}{'...' if len(txt)>600 else ''}")
                st.markdown("---")

with right:
    st.header("‚ë¢ ÎÇòÏùò ÎãµÎ≥Ä")
    answer = st.text_area("Ïó¨Í∏∞Ïóê ÎãµÎ≥ÄÏùÑ ÏûëÏÑ±ÌïòÏÑ∏Ïöî (STAR Í∂åÏû•: ÏÉÅÌô©-Í≥ºÏ†ú-ÌñâÎèô-ÏÑ±Í≥º)", height=180)
    if st.button("Ï±ÑÏ†ê & ÏΩîÏπ≠", type="primary", use_container_width=True):
        if not st.session_state.get("current_question"):
            st.warning("Î®ºÏ†Ä 'ÏÉà ÏßàÎ¨∏ Î∞õÍ∏∞'Î°ú ÏßàÎ¨∏ÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.")
        elif not answer.strip():
            st.warning("ÎãµÎ≥ÄÏùÑ ÏûëÏÑ±Ìï¥ Ï£ºÏÑ∏Ïöî.")
        else:
            with st.spinner("ÏΩîÏπ≠ Ï§ë..."):
                try:
                    supports = []
                    if rag_enabled and (docs or st.session_state.rag_store.get("chunks")):
                        q_for_rag = st.session_state["current_question"] + "\n" + answer[:800]
                        supports = retrieve_supports(q_for_rag, top_k)
                    res = coach_answer(company, st.session_state["current_question"], answer, supports)
                    st.session_state.history.append({
                        "ts": pd.Timestamp.now(),
                        "question": st.session_state["current_question"],
                        "user_answer": answer,
                        "score": res.get("score"),
                        "feedback": res.get("raw"),
                        "supports": supports,
                        "competencies": res.get("competencies")
                    })
                except Exception as e:
                    st.error(f"ÏΩîÏπ≠ Ïò§Î•ò: {e}")

# ---------- Results / Radar / CSV ----------
st.divider()
st.subheader("‚ë£ ÌîºÎìúÎ∞± Í≤∞Í≥º")
if st.session_state.history:
    last = st.session_state.history[-1]
    c1, c2 = st.columns([1, 3])
    with c1:
        st.metric("Ï¥ùÏ†ê(/10)", last.get("score", "‚Äî"))
    with c2:
        st.markdown(last.get("feedback", ""))

    if rag_enabled and last.get("supports"):
        with st.expander("ÏΩîÏπ≠Ïóê ÏÇ¨Ïö©Îêú Í∑ºÍ±∞ Î≥¥Í∏∞"):
            for i, (_, sc, txt) in enumerate(last["supports"], 1):
                st.markdown(f"**[{i}] sim={sc:.3f}**\n\n{txt[:800]}{'...' if len(txt)>800 else ''}")
                st.markdown("---")

st.divider()
st.subheader("‚ë§ Ïó≠Îüâ Î†àÏù¥Îçî (ÏÑ∏ÏÖò ÎàÑÏ†Å)")
competencies = ["Î¨∏Ï†úÏ†ïÏùò", "Îç∞Ïù¥ÌÑ∞/ÏßÄÌëú", "Ïã§ÌñâÎ†•/Ï£ºÎèÑÏÑ±", "ÌòëÏóÖ/Ïª§ÎÆ§ÎãàÏºÄÏù¥ÏÖò", "Í≥†Í∞ùÍ∞ÄÏπò"]

def compute_comp_df(hist):
    rows = []
    for h in hist:
        if h.get("competencies") and len(h["competencies"]) == 5:
            rows.append(h["competencies"])
    if not rows:
        return None
    return pd.DataFrame(rows, columns=competencies)

comp_df = compute_comp_df(st.session_state.history)
if comp_df is not None:
    avg_scores = comp_df.mean().values.tolist()
    if PLOTLY_OK:
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(
            r=avg_scores + [avg_scores[0]],
            theta=competencies + [competencies[0]],
            fill='toself',
            name='ÌèâÍ∑†(0~5)'
        ))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,5])), showlegend=False, height=420)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Plotly ÎØ∏ÏÑ§Ïπò ÏÉÅÌÉú ‚Äî ÎßâÎåÄ Í∑∏ÎûòÌîÑÎ°ú ÎåÄÏ≤¥Ìï©ÎãàÎã§.")
        st.bar_chart(pd.DataFrame({"score": avg_scores}, index=competencies))
    st.dataframe(comp_df, use_container_width=True)
else:
    st.info("ÏïÑÏßÅ Ïó≠Îüâ Ï†êÏàòÍ∞Ä ÌååÏã±Îêú ÏΩîÏπ≠ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.")

st.divider()
st.subheader("‚ë• ÏÑ∏ÏÖò Î¶¨Ìè¨Ìä∏ (CSV)")
def build_report_df(hist):
    rows = []
    for h in hist:
        row = {
            "timestamp": h.get("ts"),
            "question": h.get("question"),
            "user_answer": h.get("user_answer"),
            "score": h.get("score"),
            "feedback_raw": h.get("feedback"),
        }
        comps = h.get("competencies")
        if comps and len(comps) == 5:
            for k, v in zip(competencies, comps):
                row[f"comp_{k}"] = v
        sups = h.get("supports") or []
        row["supports_preview"] = " || ".join([s[2][:120].replace("\n"," ") for s in sups])
        rows.append(row)
    if not rows:
        return pd.DataFrame(columns=["timestamp","question","user_answer","score","feedback_raw","supports_preview"])
    return pd.DataFrame(rows)

report_df = build_report_df(st.session_state.history)
st.download_button("CSV Îã§Ïö¥Î°úÎìú", data=report_df.to_csv(index=False).encode("utf-8-sig"),
                   file_name="interview_session_report.csv", mime="text/csv")

st.caption("Tip) Í≥µÍ≥† URLÏùÑ ÏßÅÏ†ë ÎÑ£ÏúºÎ©¥ Ï†ïÌôïÎèÑÍ∞Ä ÌÅ¨Í≤å Ïò¨ÎùºÍ∞ëÎãàÎã§. Îâ¥Ïä§/ÌôàÌéòÏù¥ÏßÄ/Í≥µÍ≥† ÌÖçÏä§Ìä∏Îäî RAGÏóê ÏûêÎèô Ìà¨ÏûÖ ÏòµÏÖòÏúºÎ°ú Í∑ºÍ±∞ Í∏∞Î∞ò ÏßàÎ¨∏/ÏΩîÏπ≠ÏùÑ Í∞ïÌôîÌï† Ïàò ÏûàÏñ¥Ïöî.")
