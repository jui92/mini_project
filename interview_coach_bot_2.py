# -*- coding: utf-8 -*-
# ==========================================================
# íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜ (í…ìŠ¤íŠ¸ ì „ìš© / RAG + ë ˆì´ë” + CSV)
# - ì§ë¬´ ì„ íƒ & ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘(ê¶Œì¥: URL ì…ë ¥, ì—†ìœ¼ë©´ ê²€ìƒ‰ ì‹œë„)
# - íšŒì‚¬ ë‰´ìŠ¤/ìµœê·¼ ì´ìŠˆ ë°˜ì˜ (Google News RSS)
# - ì§ˆë¬¸ ë‹¤ì–‘ì„± ê°•í™”: í›„ë³´ Nê°œ ìƒì„± + ë°˜ì¤‘ë³µ ì„ íƒ + ë¬´ì‘ìœ„ í¬ì»¤ìŠ¤
# - ì±„ìš©ê³µê³  ê¸°ì¤€ ìš”ì•½(íšŒì‚¬/ê°„ë‹¨ ì†Œê°œ/ëª¨ì§‘ë¶„ì•¼/ì£¼ìš” ì—…ë¬´/ìê²© ìš”ê±´)
# - Streamlit Cloud í˜¸í™˜, Plotly/FAISS ì„ íƒì , ì‹œí¬ë¦¿ ì•ˆì „ ë¡œë”
# ==========================================================

import os, io, re, json, textwrap, urllib.parse, difflib, random, time
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timezone

import numpy as np
import pandas as pd
import streamlit as st

# ---------- Optional deps ----------
try:
    import pypdf
except Exception:
    pypdf = None

try:
    import plotly.graph_objects as go
    PLOTLY_OK = True
except Exception:
    PLOTLY_OK = False

try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

import requests
from bs4 import BeautifulSoup
try:
    import wikipedia
    try:
        wikipedia.set_lang("ko")
    except Exception:
        pass
except Exception:
    wikipedia = None

# ---------- Page config ----------
st.set_page_config(page_title="íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜", page_icon="ğŸ¯", layout="wide")

# ---------- Secrets loader ----------
def _secrets_file_exists() -> bool:
    candidates = [
        os.path.join(os.path.expanduser("~"), ".streamlit", "secrets.toml"),
        os.path.join(os.getcwd(), ".streamlit", "secrets.toml"),
    ]
    return any(os.path.exists(p) for p in candidates)

def load_api_key_from_env_or_secrets() -> Optional[str]:
    key = os.getenv("OPENAI_API_KEY")
    if key: return key
    try:
        if _secrets_file_exists() or hasattr(st, "secrets"):
            return st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        pass
    return None

# ---------- Text utils ----------
def _clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", t or "").strip()

def _snippetize(text: str, maxlen: int = 240) -> str:
    t = _clean_text(text)
    return t if len(t) <= maxlen else t[: maxlen - 1] + "â€¦"

def chunk_text(text: str, size: int = 900, overlap: int = 150) -> List[str]:
    text = re.sub(r"\s+", " ", text).strip()
    if not text: return []
    chunks, start = [], 0
    while start < len(text):
        end = min(len(text), start + size)
        chunks.append(text[start:end])
        if end == len(text): break
        start = max(0, end - overlap)
    return chunks

def read_file_to_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt", ".md")):
        for enc in ("utf-8", "cp949", "euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        if pypdf is None:
            st.warning("pypdfê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— pypdf ì¶”ê°€.")
            return ""
        try:
            reader = pypdf.PdfReader(io.BytesIO(data))
            return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
        except Exception as e:
            st.warning(f"PDF íŒŒì‹± ì‹¤íŒ¨({uploaded.name}): {e}")
            return ""
    return ""

# ---------- Company/domain utils ----------
VAL_KEYWORDS = [
    "í•µì‹¬ê°€ì¹˜","ê°€ì¹˜","ë¯¸ì…˜","ë¹„ì „","ë¬¸í™”","ì›ì¹™","ì² í•™","ê³ ê°","ë°ì´í„°","í˜ì‹ ",
    "values","mission","vision","culture","principles","philosophy","customer","data","innovation"
]

def _domain(u: str|None) -> str|None:
    if not u: return None
    try:
        if not u.startswith("http"): u = "https://" + u
        return urllib.parse.urlparse(u).netloc.lower().replace("www.","")
    except Exception:
        return None

def _name_similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()

# ---------- Wikipedia summary ----------
def fetch_wikipedia_summary(company_name: str, homepage: str|None=None) -> dict|None:
    if wikipedia is None:
        return None
    try:
        candidates = wikipedia.search(company_name, results=10)
        if not candidates:
            return None
        target_dom = _domain(homepage)
        best = None; best_score = -1.0
        for title in candidates:
            try:
                page = wikipedia.page(title, auto_suggest=False, redirect=True)
            except Exception:
                continue
            first = _clean_text((page.summary or "").split("\n")[0])
            score = _name_similarity(company_name, page.title)
            if any(k in first for k in ["íšŒì‚¬","ê¸°ì—…","Company","Corporation","Inc","Co., Ltd"]):
                score += 0.15
            page_dom = None
            try:
                page_dom = _domain(page.url)
            except Exception:
                pass
            if target_dom and page_dom and target_dom in page_dom:
                score += 0.25
            if score > best_score:
                best_score = score; best = (page, first)
        if not best: return None
        page, first = best
        return {
            "company_name": page.title,
            "wiki_summary": first
        }
    except Exception:
        return None

# ---------- Simple site scrape for values/recent ----------
def fetch_site_snippets(base_url: str | None, company_name: str | None = None) -> dict:
    if not base_url:
        return {"values": [], "recent": [], "site_name": None, "about": None}
    url0 = base_url.strip()
    if not url0.startswith("http"): url0 = "https://" + url0
    cand_paths = ["", "/", "/about", "/company", "/about-us", "/mission", "/values", "/culture"]
    values_found, recent_found = [], []
    site_name, about_para = None, None

    for path in cand_paths:
        url = url0.rstrip("/") + path
        try:
            r = requests.get(url, timeout=6)
            if r.status_code != 200 or "text/html" not in r.headers.get("content-type", ""):
                continue
            soup = BeautifulSoup(r.text, "html.parser")

            # site name / about
            if site_name is None:
                og = soup.find("meta", {"property":"og:site_name"}) or soup.find("meta", {"name":"application-name"})
                if og and og.get("content"): site_name = _clean_text(og["content"])
                elif soup.title and soup.title.string: site_name = _clean_text(soup.title.string.split("|")[0])
            if about_para is None:
                # hero/lead ë‹¨ë½ ì¶”ì •
                hero = soup.find(["p","div"], class_=re.compile(r"(lead|hero|intro)", re.I)) if soup else None
                if hero:
                    about_para = _snippetize(hero.get_text(" "))

            for tag in soup.find_all(["h1","h2","h3","p","li"]):
                txt = _clean_text(tag.get_text(separator=" "))
                if 10 <= len(txt) <= 240:
                    if any(k.lower() in txt.lower() for k in VAL_KEYWORDS):
                        values_found.append(txt)
                    if any(k in txt for k in ["í”„ë¡œì íŠ¸","ê°œë°œ","ì¶œì‹œ","ì„±ê³¼","project","launched","release","delivered","improved"]):
                        recent_found.append(txt)
        except Exception:
            continue

    # name check
    if company_name and site_name and _name_similarity(company_name, site_name) < 0.35:
        values_found, recent_found = [], []

    # dedup & trim
    def dedup(lst):
        seen=set(); out=[]
        for x in lst:
            if x not in seen: seen.add(x); out.append(x)
        return out
    values_found = dedup(values_found)[:5]
    recent_found = dedup(recent_found)[:5]
    trimmed=[]
    for v in values_found:
        v2 = v.split(":",1)[-1]
        if len(v2)>60 and "," in v2:
            trimmed += [p.strip() for p in v2.split(",") if 2<=len(p.strip())<=24][:6]
        else:
            trimmed.append(v2[:60])
    return {"values": trimmed[:6], "recent": recent_found, "site_name": site_name, "about": about_para}

# ---------- Google News RSS (ìµœê·¼ ë‰´ìŠ¤) ----------
def fetch_news(company_name: str, max_items: int = 6, lang: str = "ko") -> List[dict]:
    # Google News RSS (ê³µì‹ API ì•„ë‹˜) â€” ë‹¨ìˆœ íŒŒì‹±
    q = urllib.parse.quote(company_name)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    items = []
    try:
        r = requests.get(url, timeout=8)
        if r.status_code != 200:
            return []
        soup = BeautifulSoup(r.text, "xml")
        for it in soup.find_all("item")[:max_items]:
            title = _clean_text(it.title.get_text()) if it.title else ""
            link = it.link.get_text() if it.link else ""
            pub = it.pubDate.get_text() if it.pubDate else ""
            items.append({"title": title, "link": link, "pubDate": pub})
    except Exception:
        return []
    return items

# ---------- Job posting discovery & parsing ----------
SEARCH_ENGINES = [
    # DuckDuckGo HTML endpoint (ì‹¬í”Œ, ì°¨ë‹¨ ê°€ëŠ¥ì„± ë‚®ìŒ)
    "https://duckduckgo.com/html/?q={query}"
]
JOB_SITES = [
    "wanted.co.kr", "saramin.co.kr", "jobkorea.co.kr", "rocketpunch.com",
    "indeed.com", "linkedin.com", "recruit.navercorp.com", "kakao.recruit", "naver"
]

def discover_job_posting_urls(company_name: str, role: str, limit: int = 5) -> List[str]:
    # ê°„ë‹¨ ê²€ìƒ‰: "company role site:wanted.co.kr OR site:saramin.co.kr ..."
    site_part = " OR ".join([f'site:{d}' for d in JOB_SITES])
    q = f'{company_name} {role} ({site_part})'
    urls = []
    for engine in SEARCH_ENGINES:
        url = engine.format(query=urllib.parse.quote(q))
        try:
            r = requests.get(url, timeout=8, headers={"User-Agent":"Mozilla/5.0"})
            if r.status_code != 200: 
                continue
            soup = BeautifulSoup(r.text, "html.parser")
            for a in soup.find_all("a", href=True):
                href = a["href"]
                # DDGëŠ” ë¦¬ë‹¤ì´ë ‰íŠ¸ ë§í¬ì¼ ìˆ˜ ìˆìŒ
                if href.startswith("/l/?kh=-1&uddg="):
                    href = urllib.parse.unquote(href.split("/l/?kh=-1&uddg=")[-1])
                dom = _domain(href)
                if not dom: 
                    continue
                if any(d in dom for d in JOB_SITES):
                    if href not in urls:
                        urls.append(href)
                if len(urls) >= limit:
                    break
        except Exception:
            continue
    return urls[:limit]

def _extract_json_ld_job(soup: BeautifulSoup) -> Optional[dict]:
    # schema.org JobPosting JSON-LD ì¶”ì¶œ
    for s in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(s.string or "")
            if isinstance(data, list):
                for obj in data:
                    typ = obj.get("@type") if isinstance(obj, dict) else None
                    if (isinstance(typ, list) and "JobPosting" in typ) or typ == "JobPosting":
                        return obj
            elif isinstance(data, dict):
                typ = data.get("@type")
                if (isinstance(typ, list) and "JobPosting" in typ) or typ == "JobPosting":
                    return data
        except Exception:
            continue
    return None

def parse_job_posting(url: str) -> dict:
    # ê³µê³  í˜ì´ì§€ì—ì„œ ëª¨ì§‘ë¶„ì•¼/ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´ ì¶”ì¶œ (1) JSON-LD ìš°ì„  (2) í—¤ë”©/í‚¤ì›Œë“œ íœ´ë¦¬ìŠ¤í‹±
    out = {"title": None, "responsibilities": [], "qualifications": [], "company_intro": None}
    try:
        r = requests.get(url, timeout=10, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
            return out
        soup = BeautifulSoup(r.text, "html.parser")

        # (1) JSON-LD JobPosting
        jp = _extract_json_ld_job(soup)
        if jp:
            out["title"] = jp.get("title")
            # desc â†’ ë¬¸ì¥/ë¶ˆë¦¿ ë¶„í•´
            desc = _clean_text(jp.get("description", ""))
            if desc:
                bullets = re.split(r"[â€¢\-\nâ€¢Â·â–ªï¸â–¶ï¸]+", desc)
                bullets = [b.strip(" -â€¢Â·â–ªï¸â–¶ï¸") for b in bullets if len(b.strip()) > 3]
                # ê°„ë‹¨ ê·œì¹™ìœ¼ë¡œ responsibilities/qualifications ë¶„í• 
                for b in bullets:
                    if any(k in b for k in ["ìê²©", "ìš”ê±´", "requirements", "qualification", "í•„ìˆ˜", "ìš°ëŒ€"]):
                        out["qualifications"].append(b)
                    else:
                        out["responsibilities"].append(b)

        # (2) íœ´ë¦¬ìŠ¤í‹±: í—¤ë”ì— ê¸°ë°˜í•œ ì„¹ì…˜ ì¶”ì¶œ
        sections = {}
        for h in soup.find_all(re.compile("^h[1-4]$")):
            head = _clean_text(h.get_text())
            if not head: 
                continue
            nxt = []
            sib = h.find_next_sibling()
            stop_at = {"h1","h2","h3","h4"}
            while sib and sib.name not in stop_at:
                if sib.name in {"p","li","ul","ol","div"}:
                    txt = _clean_text(sib.get_text(" "))
                    if len(txt) > 5: nxt.append(txt)
                sib = sib.find_next_sibling()
            if nxt:
                sections[head] = " ".join(nxt)

        # í‚¤ì›Œë“œ ë§¤ì¹­
        resp_keys = ["ì£¼ìš” ì—…ë¬´","ë‹´ë‹¹ ì—…ë¬´","ì—…ë¬´","Responsibilities","What you will do","Role"]
        qual_keys = ["ìê²© ìš”ê±´","ì§€ì› ìê²©","ìš°ëŒ€","Requirements","Qualifications","Must have","Preferred"]
        def pick(keys):
            for k in sections:
                if any(kk.lower() in k.lower() for kk in keys):
                    return sections[k]
            return None

        if not out["responsibilities"]:
            resp = pick(resp_keys)
            if resp:
                out["responsibilities"] = [x for x in re.split(r"[â€¢\-\nâ€¢Â·â–ªï¸â–¶ï¸]+", resp) if len(x.strip())>3][:12]

        if not out["qualifications"]:
            qual = pick(qual_keys)
            if qual:
                out["qualifications"] = [x for x in re.split(r"[â€¢\-\nâ€¢Â·â–ªï¸â–¶ï¸]+", qual) if len(x.strip())>3][:12]

        # íšŒì‚¬ ì†Œê°œ ì¶”ì •
        meta_desc = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
        if meta_desc and meta_desc.get("content"):
            out["company_intro"] = _snippetize(meta_desc["content"], 220)
    except Exception:
        pass
    # ìµœì¢… ì •ë¦¬
    out["responsibilities"] = [ _snippetize(x, 140) for x in out["responsibilities"] ][:12]
    out["qualifications"]  = [ _snippetize(x, 140) for x in out["qualifications"]   [:12]]
    return out

# ---------- OpenAI client ----------
with st.sidebar:
    st.title("ğŸ¯ ê°€ìƒ ë©´ì ‘ ì½”ì¹˜")

    API_KEY = load_api_key_from_env_or_secrets()
    if not API_KEY:
        st.info("í™˜ê²½ë³€ìˆ˜/Secretsì—ì„œ í‚¤ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ì— ì…ë ¥í•˜ë©´ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•´ìš”.")
        API_KEY = st.text_input("OPENAI_API_KEY", type="password")

    MODEL = st.selectbox("ì±— ëª¨ë¸", ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small", "text-embedding-3-large"], index=0)

    # ë””ë²„ê·¸
    _openai_ver = None; _httpx_ver = None
    try:
        import openai as _openai_pkg; _openai_ver = getattr(_openai_pkg, "__version__", None)
    except Exception: pass
    try:
        import httpx as _httpx_pkg; _httpx_ver = getattr(_httpx_pkg, "__version__", None)
    except Exception: pass
    with st.expander("ë””ë²„ê·¸: ì‹œí¬ë¦¿/ë²„ì „ ìƒíƒœ"):
        st.write({
            "env_has_key": bool(os.getenv("OPENAI_API_KEY")),
            "api_key_provided": bool(API_KEY),
            "openai_version": _openai_ver,
            "httpx_version": _httpx_ver,
        })

if not API_KEY:
    st.error("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. (Cloud: App â†’ Settings â†’ Secrets)")
    st.stop()

try:
    client = OpenAI(api_key=API_KEY, timeout=30.0)
except TypeError:
    st.error("OpenAI ì´ˆê¸°í™” TypeError. requirements.txtì—ì„œ openai==1.44.0, httpx==0.27.2ë¡œ ê³ ì • í›„ Clear cache â†’ Reboot í•´ì£¼ì„¸ìš”.")
    st.stop()
except Exception as e:
    st.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    st.stop()

# ---------- Sidebar: íšŒì‚¬/ì§ë¬´ + ìë™ í”„ë¡œí•„ + ê³µê³ /ë‰´ìŠ¤ + RAG ----------
with st.sidebar:
    st.markdown("---")
    st.markdown("#### íšŒì‚¬/ì§ë¬´ ì„¤ì •")

    # ì§ë¬´ ì§ì ‘ ì„ íƒ/ì…ë ¥ (ë¬¸ì œ 1 í•´ê²°)
    role_title = st.text_input("ì§€ì› ì§ë¬´ëª… (ì˜ˆ: ë°ì´í„° ì• ë„ë¦¬ìŠ¤íŠ¸, ML ì—”ì§€ë‹ˆì–´)")

    st.markdown("#### ğŸ” ìë™ í”„ë¡œí•„ ìƒì„± (íšŒì‚¬/í™ˆí˜ì´ì§€/ì±„ìš©ê³µê³ )")
    auto_name = st.text_input("íšŒì‚¬ ì´ë¦„ (ì˜ˆ: ë„¤ì´ë²„, Kakao, Samsung SDS)")
    auto_home = st.text_input("í™ˆí˜ì´ì§€ URL (ì„ íƒ)")
    job_url = st.text_input("ì±„ìš© ê³µê³  URL (ì„ íƒ) â€” ì—†ìœ¼ë©´ ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ê²€ìƒ‰ ì‹œë„")

    col_a, col_b = st.columns(2)
    with col_a:
        auto_add_to_rag = st.checkbox("í™ˆí˜ì´ì§€/ë‰´ìŠ¤/ê³µê³ ë¥¼ RAGì— ì¶”ê°€", value=True)
    with col_b:
        diversity_k = st.slider("ì§ˆë¬¸ í›„ë³´ ê°œìˆ˜", 3, 8, 6, 1)

    if st.button("íšŒì‚¬/ì§ë¬´ ìë™ ì„¸íŒ…"):
        if not auto_name.strip():
            st.warning("íšŒì‚¬ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        else:
            with st.spinner("íšŒì‚¬Â·ì§ë¬´Â·ê³µê³ Â·ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
                # íšŒì‚¬ ê¸°ë³¸(ìœ„í‚¤+í™ˆí˜ì´ì§€)
                wiki = fetch_wikipedia_summary(auto_name.strip(), auto_home.strip() or None) or {}
                site = fetch_site_snippets(auto_home.strip() or None, auto_name.strip()) if auto_home.strip() else {"values": [], "recent": [], "site_name": None, "about": None}

                # ê³µê³  URLì´ ì—†ìœ¼ë©´ ê²€ìƒ‰ ì‹œë„ (ë¬¸ì œ 2 í•´ê²°)
                jp_data = {"title": None, "responsibilities": [], "qualifications": [], "company_intro": None}
                discovered = []
                if job_url.strip():
                    discovered = [job_url.strip()]
                else:
                    if role_title.strip():
                        discovered = discover_job_posting_urls(auto_name.strip(), role_title.strip(), limit=4)

                if discovered:
                    jp_data = parse_job_posting(discovered[0])

                # ë‰´ìŠ¤ (ë¬¸ì œ 3 í•´ê²°)
                news_items = fetch_news(auto_name.strip(), max_items=6, lang="ko")

                # company ê°ì²´ êµ¬ì„±
                company = {
                    "company_name": site.get("site_name") or wiki.get("company_name") or auto_name.strip(),
                    "homepage": auto_home.strip() or None,
                    "wiki_summary": wiki.get("wiki_summary"),
                    "values": site.get("values", []),
                    "recent_projects": site.get("recent", []),
                    "role": role_title.strip(),
                    "role_requirements": jp_data["responsibilities"] or [],
                    "role_qualifications": jp_data["qualifications"] or [],
                    "job_url": discovered[0] if discovered else (job_url.strip() or None),
                    "company_intro": jp_data["company_intro"] or site.get("about"),
                    "news": news_items
                }
                st.session_state["company_override"] = company

                # RAG ìë™ íˆ¬ì… (ë¬¸ì œ 3, 5ì˜ ê·¼ê±° ê°•í™”)
                if auto_add_to_rag:
                    texts = []
                    # í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ ëª‡ ê²½ë¡œ
                    if auto_home.strip():
                        for p in ["", "/about", "/values", "/mission", "/company"]:
                            u = auto_home.strip().rstrip("/") + p
                            try:
                                r = requests.get(u, timeout=6)
                                if r.status_code == 200 and "text/html" in r.headers.get("content-type",""):
                                    s = BeautifulSoup(r.text, "html.parser")
                                    txts = [_clean_text(t.get_text(" ")) for t in s.find_all(["h1","h2","h3","p","li"])]
                                    page = "\n".join([t for t in txts if len(t) > 10])
                                    if page: texts.append(page)
                            except Exception:
                                pass
                    # ë‰´ìŠ¤ ë³¸ë¬¸ì€ ë„ë©”ì¸ ì°¨ë‹¨ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì œëª©+ë§í¬+ë‚ ì§œë¡œë§Œ
                    if news_items:
                        news_text = "\n".join([f"[NEWS] {n['title']} ({n.get('pubDate','')}) {n['link']}" for n in news_items])
                        texts.append(news_text)
                    # ì±„ìš© ê³µê³  í…ìŠ¤íŠ¸
                    if jp_data["responsibilities"] or jp_data["qualifications"]:
                        job_text = "ì£¼ìš” ì—…ë¬´:\n- " + "\n- ".join(jp_data["responsibilities"]) + "\nìê²© ìš”ê±´:\n- " + "\n- ".join(jp_data["qualifications"])
                        texts.append(job_text)

                    if texts:
                        chs = []
                        for t in texts:
                            chs += chunk_text(t, 900, 150)
                        try:
                            embs = client.embeddings.create(model="text-embedding-3-small", input=chs)
                            embs = np.array([d.embedding for d in embs.data], dtype=np.float32)
                            if "rag_store" not in st.session_state:
                                st.session_state.rag_store = {"chunks": [], "embeds": None}
                            st.session_state.rag_store["chunks"] = (st.session_state.rag_store.get("chunks", []) or []) + chs
                            if st.session_state.rag_store.get("embeds") is None or st.session_state.rag_store["embeds"].size == 0:
                                st.session_state.rag_store["embeds"] = embs
                            else:
                                st.session_state.rag_store["embeds"] = np.vstack([st.session_state.rag_store["embeds"], embs])
                        except Exception:
                            st.info("ì¼ë¶€ í…ìŠ¤íŠ¸ëŠ” RAG ì¸ë±ì‹±ì—ì„œ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤(ìš”ì²­ ì œí•œ/ì°¨ë‹¨ ê°€ëŠ¥).")
            st.success("ìë™ ì„¸íŒ… ì™„ë£Œ!")

    st.markdown("---")
    st.markdown("#### RAG (ì„ íƒ ì—…ë¡œë“œ)")
    rag_enabled = st.toggle("íšŒì‚¬ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸/ì½”ì¹­ ì‚¬ìš©", value=True)
    chunk_size = st.slider("ì²­í¬ ê¸¸ì´(ë¬¸ì)", 400, 2000, 900, 100)
    chunk_overlap = st.slider("ì˜¤ë²„ë©(ë¬¸ì)", 0, 400, 150, 10)
    top_k = st.slider("ê²€ìƒ‰ ìƒìœ„ K", 1, 8, 4, 1)
    st.caption("TXT/MD/PDF ì—…ë¡œë“œ ê°€ëŠ¥ (ì„¸ì…˜ ë©”ëª¨ë¦¬ ë‚´ ì²˜ë¦¬)")
    docs = st.file_uploader("íšŒì‚¬ ë¬¸ì„œ ì—…ë¡œë“œ (ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥)", type=["txt", "md", "pdf"], accept_multiple_files=True)

# ---------- company ê²°ì • ----------
if "company_override" in st.session_state:
    company = st.session_state["company_override"]
else:
    # ì´ˆê¸° ë”ë¯¸ (ìˆ˜ë™ ì—…ë¡œë“œ/ê¸°ë³¸ íŒŒì¼ ëŒ€ì‹  ìë™ ì„¸íŒ… ìœ ë„)
    company = {
        "company_name": "(íšŒì‚¬ëª… ë¯¸ì„¤ì •)",
        "homepage": None,
        "wiki_summary": None,
        "values": [],
        "recent_projects": [],
        "role": role_title,
        "role_requirements": [],
        "role_qualifications": [],
        "job_url": None,
        "company_intro": None,
        "news": []
    }

# ---------- session states ----------
if "rag_store" not in st.session_state:
    st.session_state.rag_store = {"chunks": [], "embeds": None}
if "history" not in st.session_state:
    st.session_state.history = []
if "current_question" not in st.session_state:
    st.session_state.current_question = ""

# ---------- Upload â†’ RAG ----------
def embed_texts(client: OpenAI, embed_model: str, texts: List[str]) -> np.ndarray:
    if not texts:
        return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=embed_model, input=texts)
    return np.array([d.embedding for d in resp.data], dtype=np.float32)

if rag_enabled and docs:
    with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
        all_chunks = []
        for up in docs:
            text = read_file_to_text(up)
            if not text: continue
            all_chunks.extend(chunk_text(text, chunk_size, chunk_overlap))
        if all_chunks:
            embeds = embed_texts(client, EMBED_MODEL, all_chunks)
            st.session_state.rag_store["chunks"] = (st.session_state.rag_store.get("chunks", []) or []) + all_chunks
            if st.session_state.rag_store.get("embeds") is None or st.session_state.rag_store["embeds"].size == 0:
                st.session_state.rag_store["embeds"] = embeds
            else:
                st.session_state.rag_store["embeds"] = np.vstack([st.session_state.rag_store["embeds"], embeds])
            st.success(f"RAG ì¤€ë¹„ ì™„ë£Œ: ì²­í¬ {len(all_chunks)}ê°œ ì¶”ê°€")
        else:
            st.info("ì—…ë¡œë“œ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ---------- Retrieval helpers ----------
def cosine_topk(matrix: np.ndarray, query: np.ndarray, k: int = 4):
    if matrix.size == 0:
        return np.array([]), np.array([], dtype=int)
    qn = query / (np.linalg.norm(query, axis=1, keepdims=True) + 1e-12)
    mn = matrix / (np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12)
    sims = mn @ qn.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    scores = sims[idx]
    return scores, idx

def retrieve_supports(query_text: str, k: int) -> List[Tuple[str, float, str]]:
    store = st.session_state.rag_store
    chunks, embeds = store.get("chunks", []), store.get("embeds", None)
    if not rag_enabled or embeds is None or len(chunks) == 0:
        return []
    qv = embed_texts(client, EMBED_MODEL, [query_text])
    scores, idxs = cosine_topk(embeds, qv, k=k)
    return [("íšŒì‚¬ìë£Œ", float(s), chunks[int(i)]) for s, i in zip(scores, idxs)]

# ---------- Diversity helpers ----------
def _similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def pick_diverse(candidates: list[str], history: list[str], gamma: float = 0.25) -> str:
    # í›„ë³´ë³„ ì ìˆ˜ = í‰ê·  ìœ ì‚¬ë„ + gamma*í‘œì¤€í¸ì°¨ â†’ ìµœì†Œ ì ìˆ˜ ì„ íƒ(ë‹¤ì–‘ì„± ìµœëŒ€í™”)
    if not candidates: return ""
    if not history: return random.choice(candidates)
    best = None; best_score = 1e9
    for q in candidates:
        sims = [ _similarity(q,h) for h in history ]
        if not sims: sims=[0.0]
        score = (sum(sims)/len(sims)) + gamma*(np.std(sims))
        if score < best_score:
            best_score = score; best = q
    return best

TYPE_INSTRUCTIONS = {
    "í–‰ë™(STAR)": "ê³¼ê±° ì‹¤ë¬´ ì‚¬ë¡€ë¥¼ ëŒì–´ë‚´ë„ë¡ S(ìƒí™©)-T(ê³¼ì œ)-A(í–‰ë™)-R(ì„±ê³¼)ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸",
    "ê¸°ìˆ  ì‹¬ì¸µ": "í•µì‹¬ ê¸°ìˆ ì  ì˜ì‚¬ê²°ì •Â·íŠ¸ë ˆì´ë“œì˜¤í”„Â·ì„±ëŠ¥/ë¹„ìš©/í’ˆì§ˆ ì§€í‘œë¥¼ íŒŒê³ ë“œëŠ” ì‹¬ì¸µ ì§ˆë¬¸",
    "í•µì‹¬ê°€ì¹˜ ì í•©ì„±": "í•µì‹¬ê°€ì¹˜ì™€ íƒœë„ë¥¼ ê²€ì¦í•˜ëŠ”, ìƒí™©ê¸°ë°˜ í–‰ë™ì„ ìœ ë„í•˜ëŠ” ì§ˆë¬¸",
    "ì—­ì§ˆë¬¸": "ì§€ì›ìê°€ íšŒì‚¬ë¥¼ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í†µì°°ë ¥ ìˆëŠ” ì—­ì§ˆë¬¸"
}

# ---------- Company context (ì±„ìš©ê³µê³  ê¸°ì¤€ ìš”ì•½: ë¬¸ì œ 5 í•´ê²°) ----------
def build_company_context_for_prompt(c: dict) -> str:
    # í”„ë¡¬í”„íŠ¸ìš© (ê°„ëµ)
    base = textwrap.dedent(f"""
    [íšŒì‚¬ëª…] {c.get('company_name','')}
    [íšŒì‚¬ ì†Œê°œ] {c.get('company_intro') or c.get('wiki_summary') or ''}
    [ëª¨ì§‘ ë¶„ì•¼] {c.get('role','')}
    [ì£¼ìš” ì—…ë¬´] {", ".join(c.get('role_requirements', [])[:6])}
    [ìê²© ìš”ê±´] {", ".join(c.get('role_qualifications', [])[:6])}
    [í•µì‹¬ê°€ì¹˜] {", ".join(c.get('values', [])[:6])}
    [ìµœê·¼ ì´ìŠˆ/ë‰´ìŠ¤] {", ".join([_snippetize(n['title'], 70) for n in c.get('news', [])[:3]])}
    """).strip()
    return base

def build_company_summary_for_ui(c: dict) -> dict:
    return {
        "íšŒì‚¬ëª…": c.get("company_name"),
        "ê°„ë‹¨ ì†Œê°œ": c.get("company_intro") or c.get("wiki_summary"),
        "ëª¨ì§‘ ë¶„ì•¼": c.get("role"),
        "ì£¼ìš” ì—…ë¬´(ìš”ì•½)": c.get("role_requirements")[:6],
        "ìê²© ìš”ê±´(ìš”ì•½)": c.get("role_qualifications")[:6],
        "í•µì‹¬ê°€ì¹˜(ì¶”ì •)": c.get("values")[:6],
        "í™ˆí˜ì´ì§€": c.get("homepage"),
        "ì±„ìš© ê³µê³ ": c.get("job_url"),
        "ìµœê·¼ ë‰´ìŠ¤": [ n.get("title") for n in c.get("news", [])[:5] ],
    }

def build_focuses(company: dict, supports: List[Tuple[str,float,str]], k: int = 4) -> list[str]:
    # í¬ì»¤ìŠ¤ëŠ” "ì§ë¬´ â†’ ê³µê³ (ì—…ë¬´/ìš”ê±´) â†’ ê°€ì¹˜ â†’ ìµœê·¼ì´ìŠˆ â†’ RAGë¬¸ì¥" ìš°ì„ 
    pool = []
    if company.get("role"): pool.append(company["role"])
    pool += company.get("role_requirements", [])[:6]
    pool += company.get("role_qualifications", [])[:6]
    pool += company.get("values", [])[:6]
    pool += [ _snippetize(n['title'], 60) for n in company.get("news", [])[:4] ]
    for _,_,txt in (supports or [])[:3]:
        pool += [t.strip() for t in re.split(r"[â€¢\-\n\.]", txt) if 6 < len(t.strip()) < 100][:3]
    pool = [p for p in pool if p]
    random.shuffle(pool)
    return pool[:k]

# ---------- Question generation (ë¬¸ì œ 3,4,5 & ë§ì¶¤ê° ê°•í™”) ----------
def gen_question(company: dict, qtype: str, level: str, supports: List[Tuple[str, float, str]], num_candidates: int = 6) -> str:
    ctx = build_company_context_for_prompt(company)
    focuses = build_focuses(company, supports, k=min(4, num_candidates))
    style = TYPE_INSTRUCTIONS.get(qtype, "êµ¬ì²´ì ì´ê³  í–‰ë™ì„ ì´ëŒì–´ë‚´ëŠ” ì§ˆë¬¸")
    rag_note = ""
    if supports:
        joined = "\n".join([f"- ({s:.2f}) {txt[:200]}" for _, s, txt in supports[:3]])
        rag_note = f"\n[ê·¼ê±° ë°œì·Œ]\n{joined}"

    # ëœë¤ì„± í–¥ìƒ: seedë¥¼ ì‹œê°„/ì„¸ì…˜ ê¸°ë°˜ìœ¼ë¡œ ì„ìŒ (ë¬¸ì œ 4)
    seed = int(time.time()*1000) % 2_147_483_647
    random_factor = random.random()

    sys = f"""ë„ˆëŠ” '{company.get('company_name','')}'ì˜ '{company.get('role','')}' ë©´ì ‘ê´€ì´ë‹¤.
íšŒì‚¬ ë§¥ë½, ì±„ìš©ê³µê³ (ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´), ìµœê·¼ ë‰´ìŠ¤/ì´ìŠˆ, (ìˆë‹¤ë©´) ê·¼ê±° ë¬¸ì„œë¥¼ ë°˜ì˜í•˜ì—¬ **{qtype}** ìœ í˜•({style})ì˜ ì§ˆë¬¸ **{num_candidates}ê°œ í›„ë³´**ë¥¼ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ë¼.
ê° í›„ë³´ëŠ” ì„œë¡œ **í˜•íƒœÂ·ê´€ì Â·í‚¤ì›Œë“œ**ê°€ ë‹¬ë¼ì•¼ í•œë‹¤. ë‚œì´ë„ëŠ” {level}.
ì•„ë˜ 'í¬ì»¤ìŠ¤' ì¤‘ ìµœì†Œ 1ê°œ í‚¤ì›Œë“œë¥¼ ì§ˆì˜ë¬¸ì— **ëª…ì‹œì ìœ¼ë¡œ í¬í•¨**í•˜ë¼.
ì‚¬ì†Œí•œ ì¬êµ¬ì„±(ì§€í‘œ/ìˆ˜ì¹˜/ê¸°ê°„/ê·œëª¨/ë¦¬ìŠ¤í¬ ìš”ì¸ ë“±)ì„ ì„ì–´ **ì„œë¡œ ë‹¤ë¥¸ ì§ˆë¬¸**ì´ ë˜ë„ë¡ í•œë‹¤.
í¬ë§·: 1) ... 2) ... ... (í•œ ì¤„ì”©)"""
    user = f"""[íšŒì‚¬/ì§ë¬´ ì»¨í…ìŠ¤íŠ¸]
{ctx}
[í¬ì»¤ìŠ¤(ë¬´ì‘ìœ„ ì¼ë¶€)]
- {chr(10).join(focuses)}{rag_note}
[ëœë¤ì‹œë“œ] {seed}; rf={random_factor:.4f}"""

    resp = client.chat.completions.create(
        model=MODEL, temperature=0.95,  # ë‹¤ì–‘ì„± â†‘
        messages=[{"role":"system","content":sys},{"role":"user","content":user}]
    )
    raw = resp.choices[0].message.content.strip()
    cands = [re.sub(r'^\s*\d+\)\s*','',line).strip() for line in raw.splitlines() if re.match(r'^\s*\d+\)', line)]
    if not cands:
        cands = [l.strip("- ").strip() for l in raw.splitlines() if len(l.strip())>0][:num_candidates]

    # ìµœê·¼ ì§ˆë¬¸ë“¤ê³¼ì˜ ë°˜ì¤‘ë³µ ì„ íƒ (ë¬¸ì œ 4)
    hist_qs = [h["question"] for h in st.session_state.get("history", [])][-10:]
    selected = pick_diverse(cands, hist_qs, gamma=0.35)
    return selected or (cands[0] if cands else "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")

# ---------- Coaching ----------
def coach_answer(company: dict, question: str, user_answer: str, supports: List[Tuple[str, float, str]]) -> Dict:
    ctx = build_company_context_for_prompt(company)
    rag_note = ""
    if supports:
        joined = "\n".join([f"- ({s:.3f}) {txt[:500]}" for (_, s, txt) in supports])
        rag_note = f"\n[íšŒì‚¬ ê·¼ê±° ë¬¸ì„œ ë°œì·Œ]\n{joined}\n"
    competencies = ["ë¬¸ì œì •ì˜", "ë°ì´í„°/ì§€í‘œ", "ì‹¤í–‰ë ¥/ì£¼ë„ì„±", "í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ê³ ê°ê°€ì¹˜"]
    comp_str = ", ".join(competencies)
    sys = f"""ë„ˆëŠ” í†±í‹°ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. í•œêµ­ì–´ë¡œ ì•„ë˜ í˜•ì‹ì— ë§ì¶° ë‹µí•˜ë¼:
1) ì´ì : 0~10 ì •ìˆ˜ 1ê°œ
2) ê°•ì : 2~3ê°œ ë¶ˆë¦¿
3) ë¦¬ìŠ¤í¬: 2~3ê°œ ë¶ˆë¦¿
4) ê°œì„  í¬ì¸íŠ¸: 3ê°œ ë¶ˆë¦¿ (í–‰ë™Â·ì§€í‘œÂ·ì„íŒ©íŠ¸ ì¤‘ì‹¬)
5) ìˆ˜ì •ë³¸ ë‹µë³€: STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼) êµ¬ì¡°ë¡œ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ
6) ì—­ëŸ‰ ì ìˆ˜: [{comp_str}] ê°ê° 0~5 ì •ìˆ˜ (í•œ ì¤„ì— ì‰¼í‘œë¡œ êµ¬ë¶„)
ì±„ì  ê¸°ì¤€ì€ íšŒì‚¬/ì§ë¬´ ë§¥ë½, ì±„ìš©ê³µê³ (ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´), ì§ˆë¬¸ ë‚´ **í¬ì»¤ìŠ¤/í‚¤ì›Œë“œ** ë¶€í•© ì—¬ë¶€ë¥¼ í¬í•¨í•œë‹¤.
ì¶”ê°€ ì„¤ëª… ê¸ˆì§€. í˜•ì‹ ìœ ì§€."""
    user = f"""[íšŒì‚¬/ì§ë¬´ ì»¨í…ìŠ¤íŠ¸]
{ctx}
{rag_note}
[ë©´ì ‘ ì§ˆë¬¸]
{question}

[í›„ë³´ì ë‹µë³€]
{user_answer}
"""
    resp = client.chat.completions.create(
        model=MODEL, temperature=0.35,
        messages=[{"role":"system","content":sys},{"role":"user","content":user}]
    )
    content = resp.choices[0].message.content.strip()
    m = re.search(r'([0-9]{1,2})\s*(?:/10|ì |$)', content)
    score = None
    if m:
        try:
            score = int(m.group(1)); score = max(0, min(10, score))
        except: pass
    comp_scores = None
    nums = re.findall(r'\b([0-5])\b', content.splitlines()[-1])
    if len(nums) >= 5:
        comp_scores = [int(x) for x in nums[:5]]
    return {"raw": content, "score": score, "competencies": comp_scores}

# ---------- UI ----------
left, right = st.columns([1, 1])

with left:
    st.header("â‘  ì±„ìš©ê³µê³  ê¸°ì¤€ íšŒì‚¬ ìš”ì•½")
    st.json(build_company_summary_for_ui(company), expanded=True)

    st.header("â‘¡ ì§ˆë¬¸ ìƒì„±")
    st.caption("â€˜ì§ˆë¬¸ ìƒì„± íŒíŠ¸â€™ì— í‚¤ì›Œë“œ(ì˜ˆ: ì „í™˜ í¼ë„, ì„±ëŠ¥-ë¹„ìš© íŠ¸ë ˆì´ë“œì˜¤í”„) 1~2ê°œ ì •ë„ë§Œ ë„£ìœ¼ë©´ ë” ë§ì¶¤í™”ë©ë‹ˆë‹¤.")
    q_type = st.selectbox("ì§ˆë¬¸ ìœ í˜•", ["í–‰ë™(STAR)", "ê¸°ìˆ  ì‹¬ì¸µ", "í•µì‹¬ê°€ì¹˜ ì í•©ì„±", "ì—­ì§ˆë¬¸"], index=0)
    level  = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´", "ë¯¸ë“¤", "ì‹œë‹ˆì–´"], index=0)
    prompt_hint = st.text_input("ì§ˆë¬¸ ìƒì„± íŒíŠ¸(ì„ íƒ)")

    if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", use_container_width=True):
        try:
            supports = []
            if rag_enabled and (docs or st.session_state.rag_store.get("chunks")):
                base_q = prompt_hint.strip() or f"{company.get('role','')} {' '.join(company.get('role_requirements', [])[:3])}"
                supports = retrieve_supports(base_q, top_k)
            q = gen_question(company, q_type, level, supports, num_candidates=diversity_k)
            st.session_state.current_question = q
            st.session_state.last_supports_q = supports
        except Exception as e:
            st.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")

    st.text_area("ì§ˆë¬¸", height=110, value=st.session_state.get("current_question",""))

    if rag_enabled and st.session_state.get("last_supports_q"):
        with st.expander("ì§ˆë¬¸ ìƒì„±ì— ì‚¬ìš©ëœ ê·¼ê±° ë³´ê¸°"):
            for i, (_, sc, txt) in enumerate(st.session_state.last_supports_q, 1):
                st.markdown(f"**[{i}] sim={sc:.3f}**\n\n{txt[:600]}{'...' if len(txt)>600 else ''}")
                st.markdown("---")

with right:
    st.header("â‘¢ ë‚˜ì˜ ë‹µë³€")
    answer = st.text_area("ì—¬ê¸°ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš” (STAR ê¶Œì¥: ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)", height=180)
    if st.button("ì±„ì  & ì½”ì¹­", type="primary", use_container_width=True):
        if not st.session_state.get("current_question"):
            st.warning("ë¨¼ì € 'ìƒˆ ì§ˆë¬¸ ë°›ê¸°'ë¡œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        elif not answer.strip():
            st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ì½”ì¹­ ì¤‘..."):
                try:
                    supports = []
                    if rag_enabled and (docs or st.session_state.rag_store.get("chunks")):
                        q_for_rag = st.session_state["current_question"] + "\n" + answer[:800]
                        supports = retrieve_supports(q_for_rag, top_k)
                    res = coach_answer(company, st.session_state["current_question"], answer, supports)
                    st.session_state.history.append({
                        "ts": pd.Timestamp.now(),
                        "question": st.session_state["current_question"],
                        "user_answer": answer,
                        "score": res.get("score"),
                        "feedback": res.get("raw"),
                        "supports": supports,
                        "competencies": res.get("competencies")
                    })
                except Exception as e:
                    st.error(f"ì½”ì¹­ ì˜¤ë¥˜: {e}")

# ---------- Results / Radar / CSV ----------
st.divider()
st.subheader("â‘£ í”¼ë“œë°± ê²°ê³¼")
if st.session_state.history:
    last = st.session_state.history[-1]
    c1, c2 = st.columns([1, 3])
    with c1:
        st.metric("ì´ì (/10)", last.get("score", "â€”"))
    with c2:
        st.markdown(last.get("feedback", ""))

    if rag_enabled and last.get("supports"):
        with st.expander("ì½”ì¹­ì— ì‚¬ìš©ëœ ê·¼ê±° ë³´ê¸°"):
            for i, (_, sc, txt) in enumerate(last["supports"], 1):
                st.markdown(f"**[{i}] sim={sc:.3f}**\n\n{txt[:800]}{'...' if len(txt)>800 else ''}")
                st.markdown("---")

st.divider()
st.subheader("â‘¤ ì—­ëŸ‰ ë ˆì´ë” (ì„¸ì…˜ ëˆ„ì )")
competencies = ["ë¬¸ì œì •ì˜", "ë°ì´í„°/ì§€í‘œ", "ì‹¤í–‰ë ¥/ì£¼ë„ì„±", "í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ê³ ê°ê°€ì¹˜"]

def compute_comp_df(hist):
    rows = []
    for h in hist:
        if h.get("competencies") and len(h["competencies"]) == 5:
            rows.append(h["competencies"])
    if not rows:
        return None
    return pd.DataFrame(rows, columns=competencies)

comp_df = compute_comp_df(st.session_state.history)
if comp_df is not None:
    avg_scores = comp_df.mean().values.tolist()
    if PLOTLY_OK:
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(
            r=avg_scores + [avg_scores[0]],
            theta=competencies + [competencies[0]],
            fill='toself',
            name='í‰ê· (0~5)'
        ))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,5])), showlegend=False, height=420)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Plotly ë¯¸ì„¤ì¹˜ ìƒíƒœ â€” ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        st.bar_chart(pd.DataFrame({"score": avg_scores}, index=competencies))
    st.dataframe(comp_df, use_container_width=True)
else:
    st.info("ì•„ì§ ì—­ëŸ‰ ì ìˆ˜ê°€ íŒŒì‹±ëœ ì½”ì¹­ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("â‘¥ ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV)")
def build_report_df(hist):
    rows = []
    for h in hist:
        row = {
            "timestamp": h.get("ts"),
            "question": h.get("question"),
            "user_answer": h.get("user_answer"),
            "score": h.get("score"),
            "feedback_raw": h.get("feedback"),
        }
        comps = h.get("competencies")
        if comps and len(comps) == 5:
            for k, v in zip(competencies, comps):
                row[f"comp_{k}"] = v
        sups = h.get("supports") or []
        row["supports_preview"] = " || ".join([s[2][:120].replace("\n"," ") for s in sups])
        rows.append(row)
    if not rows:
        return pd.DataFrame(columns=["timestamp","question","user_answer","score","feedback_raw","supports_preview"])
    return pd.DataFrame(rows)

report_df = build_report_df(st.session_state.history)
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=report_df.to_csv(index=False).encode("utf-8-sig"),
                   file_name="interview_session_report.csv", mime="text/csv")

st.caption("Tip) ê³µê³  URLì„ ì§ì ‘ ë„£ìœ¼ë©´ ì •í™•ë„ê°€ í¬ê²Œ ì˜¬ë¼ê°‘ë‹ˆë‹¤. ë‰´ìŠ¤/í™ˆí˜ì´ì§€/ê³µê³  í…ìŠ¤íŠ¸ëŠ” RAGì— ìë™ íˆ¬ì… ì˜µì…˜ìœ¼ë¡œ ê·¼ê±° ê¸°ë°˜ ì§ˆë¬¸/ì½”ì¹­ì„ ê°•í™”í•  ìˆ˜ ìˆì–´ìš”.")
