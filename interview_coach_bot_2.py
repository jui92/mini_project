# -*- coding: utf-8 -*-
# ============================================
# íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜ (í…ìŠ¤íŠ¸ ì „ìš© / RAG + ë ˆì´ë” + CSV)
# - Streamlit Cloud í˜¸í™˜
# - ì‹œí¬ë¦¿ ì•ˆì „ ë¡œë”(í™˜ê²½ë³€ìˆ˜ â†’ secrets â†’ ì‚¬ì´ë“œë°” ì…ë ¥)
# - "íšŒì‚¬ ì´ë¦„ë§Œ" ìë™ í”„ë¡œí•„(ìœ„í‚¤ + í™ˆí˜ì´ì§€) + RAG ìë™ íˆ¬ì…(ì˜µì…˜)
# - íšŒì‚¬/ë„ë©”ì¸ ì •í•©ì„± ìŠ¤ì½”ì–´ë§ & ë¶ˆì¼ì¹˜ ì°¨ë‹¨
# - ì§ˆë¬¸ ë‹¤ì–‘ì„±: í›„ë³´ 5ê°œ ìƒì„± â†’ ìµœê·¼ ì§ˆë¬¸ê³¼ ìœ ì‚¬ë„ ìµœì†Œ í›„ë³´ ì„ íƒ
# - ì§ˆë¬¸ ìœ í˜•ë³„ ì§€ì‹œ ê°•ë„ ê°•í™” + í¬ì»¤ìŠ¤(ê°€ì¹˜/ìš”êµ¬ì—­ëŸ‰/í”„ë¡œì íŠ¸/RAGë¬¸ì¥) ë°˜ì˜
# ============================================

import os, io, re, json, textwrap, urllib.parse, difflib, random
from typing import List, Dict, Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st

# ------------------------------
# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (optional)
# ------------------------------
try:
    import pypdf
except Exception:
    pypdf = None

# ------------------------------
# Plotly (optional: ì—†ìœ¼ë©´ ë§‰ëŒ€ ê·¸ë˜í”„ fallback)
# ------------------------------
try:
    import plotly.graph_objects as go
    PLOTLY_OK = True
except Exception:
    PLOTLY_OK = False

# ------------------------------
# OpenAI SDK (>=1.x)
# ------------------------------
try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# ------------------------------
# ì›¹ ìš”ì•½ ë„êµ¬ (Wikipedia + simple site scrape)
# ------------------------------
import requests
from bs4 import BeautifulSoup
try:
    import wikipedia
    try:
        wikipedia.set_lang("ko")
    except Exception:
        pass
except Exception:
    wikipedia = None  # wikipedia íŒ¨í‚¤ì§€ê°€ ì—†ìœ¼ë©´ ìœ„í‚¤ ê²½ë¡œëŠ” ìƒëµ

# ============================================
# ê¸°ë³¸ ì„¤ì •
# ============================================
st.set_page_config(page_title="íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜", page_icon="ğŸ¯", layout="wide")

# ============================================
# ì‹œí¬ë¦¿ ë¡œë”
# ============================================
def _secrets_file_exists() -> bool:
    candidates = [
        os.path.join(os.path.expanduser("~"), ".streamlit", "secrets.toml"),
        os.path.join(os.getcwd(), ".streamlit", "secrets.toml"),
    ]
    return any(os.path.exists(p) for p in candidates)

def load_api_key_from_env_or_secrets() -> Optional[str]:
    key = os.getenv("OPENAI_API_KEY")
    if key: return key
    try:
        if _secrets_file_exists() or hasattr(st, "secrets"):
            return st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        pass
    return None

# ============================================
# í…ìŠ¤íŠ¸ ìœ í‹¸
# ============================================
def _clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", t or "").strip()

def chunk_text(text: str, size: int = 900, overlap: int = 150) -> List[str]:
    text = re.sub(r"\s+", " ", text).strip()
    if not text: return []
    chunks, start = [], 0
    while start < len(text):
        end = min(len(text), start + size)
        chunks.append(text[start:end])
        if end == len(text): break
        start = max(0, end - overlap)
    return chunks

def read_file_to_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt", ".md")):
        for enc in ("utf-8", "cp949", "euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        if pypdf is None:
            st.warning("pypdfê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— pypdf ì¶”ê°€.")
            return ""
        try:
            reader = pypdf.PdfReader(io.BytesIO(data))
            return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
        except Exception as e:
            st.warning(f"PDF íŒŒì‹± ì‹¤íŒ¨({uploaded.name}): {e}")
            return ""
    return ""

# ============================================
# ì´ë¦„/ë„ë©”ì¸ ì •í•©ì„± ìœ í‹¸ (ì˜¤íƒ ë°©ì§€)
# ============================================
VAL_KEYWORDS = [
    "í•µì‹¬ê°€ì¹˜","ê°€ì¹˜","ë¯¸ì…˜","ë¹„ì „","ë¬¸í™”","ì›ì¹™","ì² í•™","ê³ ê°","ë°ì´í„°","í˜ì‹ ",
    "values","mission","vision","culture","principles","philosophy","customer","data","innovation"
]

def _domain(u: str|None) -> str|None:
    if not u: return None
    try:
        if not u.startswith("http"): u = "https://"+u
        return urllib.parse.urlparse(u).netloc.lower().replace("www.","")
    except Exception:
        return None

def _name_similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()

# ============================================
# Wikipedia ìš”ì•½ (íšŒì‚¬Â·ë„ë©”ì¸ ì¼ì¹˜ ìŠ¤ì½”ì–´ë§)
# ============================================
def fetch_wikipedia_summary(company_name: str, homepage: str|None=None) -> dict|None:
    if wikipedia is None:
        return None
    try:
        candidates = wikipedia.search(company_name, results=10)
        if not candidates:
            return None
        target_dom = _domain(homepage)
        best = None; best_score = -1.0

        for title in candidates:
            try:
                page = wikipedia.page(title, auto_suggest=False, redirect=True)
            except Exception:
                continue
            first = _clean_text((page.summary or "").split("\n")[0])
            # ì´ë¦„ ìœ ì‚¬ë„
            score = _name_similarity(company_name, page.title)
            # ê¸°ì—…ì„± ì‹ í˜¸
            if any(k in first for k in ["íšŒì‚¬","ê¸°ì—…","Company","Corporation","Inc","Co., Ltd"]):
                score += 0.15
            # ë„ë©”ì¸ ì¼ì¹˜ ë³´ë„ˆìŠ¤
            page_dom = None
            try:
                # í˜ì´ì§€ URLì˜ ë„ë©”ì¸ì„ ê·¸ëŒ€ë¡œ ì“°ê¸° ì–´ë µë‹¤ë©´ ë³´ë„ˆìŠ¤ ìƒëµ
                page_dom = _domain(page.url)
            except Exception:
                pass
            if target_dom and page_dom and target_dom in page_dom:
                score += 0.25

            if score > best_score:
                best_score = score
                best = (page, first)

        if not best: return None
        page, first = best
        return {
            "company_name": page.title,
            "values": [],
            "role": "",
            "role_requirements": [],
            "recent_projects": [],
            "wiki_summary": first,
            "homepage": None
        }
    except Exception:
        return None

# ============================================
# í™ˆí˜ì´ì§€ ìŠ¤ë‹ˆí« (íšŒì‚¬ëª… ê²€ì¦ í¬í•¨)
# ============================================
def fetch_site_snippets(base_url: str | None, company_name: str | None = None) -> dict:
    if not base_url:
        return {"values": [], "recent": [], "site_name": None}
    url0 = base_url.strip()
    if not url0.startswith("http"): url0 = "https://" + url0

    cand_paths = ["", "/", "/about", "/company", "/about-us", "/mission", "/values", "/culture"]
    values_found, recent_found = [], []
    site_name = None

    for path in cand_paths:
        url = url0.rstrip("/") + path
        try:
            r = requests.get(url, timeout=6)
            if r.status_code != 200 or "text/html" not in r.headers.get("content-type", ""):
                continue
            soup = BeautifulSoup(r.text, "html.parser")

            # ì‚¬ì´íŠ¸ëª… í›„ë³´
            if site_name is None:
                og = soup.find("meta", {"property":"og:site_name"}) or soup.find("meta", {"name":"application-name"})
                if og and og.get("content"): site_name = _clean_text(og["content"])
                elif soup.title and soup.title.string: site_name = _clean_text(soup.title.string.split("|")[0])

            # í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            for tag in soup.find_all(["h1","h2","h3","p","li"]):
                txt = _clean_text(tag.get_text(separator=" "))
                if 10 <= len(txt) <= 240:
                    if any(k.lower() in txt.lower() for k in VAL_KEYWORDS):
                        values_found.append(txt)
                    if any(k in txt for k in ["í”„ë¡œì íŠ¸","ê°œë°œ","ì¶œì‹œ","ì„±ê³¼","project","launched","release","delivered","improved"]):
                        recent_found.append(txt)
        except Exception:
            continue

    # ì´ë¦„ ê²€ì¦: ë„ˆë¬´ ë‹¤ë¥´ë©´ íê¸°(ì˜¤íƒ ë°©ì§€)
    if company_name and site_name and _name_similarity(company_name, site_name) < 0.35:
        values_found, recent_found = [], []

    # ì •ë¦¬
    def dedup(lst):
        seen=set(); out=[]
        for x in lst:
            if x not in seen: seen.add(x); out.append(x)
        return out
    values_found = dedup(values_found)[:5]
    recent_found = dedup(recent_found)[:5]

    # values ê¸¸ì´Â·í˜•íƒœ ì •ë¦¬
    trimmed=[]
    for v in values_found:
        v2 = v.split(":",1)[-1]
        if len(v2)>60 and "," in v2:
            trimmed += [p.strip() for p in v2.split(",") if 2<=len(p.strip())<=24][:6]
        else:
            trimmed.append(v2[:60])
    return {"values": trimmed[:6], "recent": recent_found, "site_name": site_name}

# ============================================
# ìë™ í”„ë¡œí•„ ë¹Œë” (í™ˆí˜ì´ì§€ ìš°ì„  + ìœ„í‚¤ í•©ì„±)
# ============================================
def build_auto_company(company_name: str, homepage: str | None) -> dict:
    wiki = fetch_wikipedia_summary(company_name, homepage)
    site = fetch_site_snippets(homepage, company_name) if homepage else {"values": [], "recent": [], "site_name": None}

    decided_name = site.get("site_name") or company_name
    if wiki and _name_similarity(decided_name, wiki.get("company_name","")) >= 0.55:
        decided_name = wiki["company_name"]

    values = site["values"] or (wiki["values"] if wiki else [])
    recents = site["recent"] or (wiki["recent_projects"] if wiki else [])
    wiki_summary = wiki["wiki_summary"] if wiki else None

    return {
        "company_name": decided_name,
        "values": values,
        "role": "",
        "role_requirements": [],
        "recent_projects": recents,
        "language": "ko",
        "wiki_summary": wiki_summary,
        "homepage": homepage
    }

# ============================================
# OpenAI ì¤€ë¹„
# ============================================
with st.sidebar:
    st.title("ğŸ¯ ê°€ìƒ ë©´ì ‘ ì½”ì¹˜")

    API_KEY = load_api_key_from_env_or_secrets()
    if not API_KEY:
        st.info("í™˜ê²½ë³€ìˆ˜/Secretsì—ì„œ í‚¤ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ì— ì…ë ¥í•˜ë©´ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•´ìš”.")
        API_KEY = st.text_input("OPENAI_API_KEY", type="password")

    MODEL = st.selectbox("ì±— ëª¨ë¸", ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small", "text-embedding-3-large"], index=0)

    # ë””ë²„ê·¸
    _openai_ver = None; _httpx_ver = None
    try:
        import openai as _openai_pkg; _openai_ver = getattr(_openai_pkg, "__version__", None)
    except Exception: pass
    try:
        import httpx as _httpx_pkg; _httpx_ver = getattr(_httpx_pkg, "__version__", None)
    except Exception: pass
    with st.expander("ë””ë²„ê·¸: ì‹œí¬ë¦¿/ë²„ì „ ìƒíƒœ"):
        st.write({
            "env_has_key": bool(os.getenv("OPENAI_API_KEY")),
            "api_key_provided": bool(API_KEY),
            "openai_version": _openai_ver,
            "httpx_version": _httpx_ver,
        })

if not API_KEY:
    st.error("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. (Cloud: App â†’ Settings â†’ Secrets)")
    st.stop()

try:
    client = OpenAI(api_key=API_KEY, timeout=30.0)
except TypeError:
    st.error("OpenAI ì´ˆê¸°í™” TypeError. requirements.txtì—ì„œ openai==1.44.0, httpx==0.27.2ë¡œ ê³ ì • í›„ Clear cache â†’ Reboot í•´ì£¼ì„¸ìš”.")
    st.stop()
except Exception as e:
    st.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    st.stop()

# ============================================
# ì‚¬ì´ë“œë°”: íšŒì‚¬/ì§ë¬´ + ìë™ í”„ë¡œí•„ + ë¬¸ì„œ ì—…ë¡œë“œ
# ============================================
with st.sidebar:
    st.markdown("---")
    st.markdown("#### íšŒì‚¬/ì§ë¬´ ì„¤ì •")

    data_dir = "data/companies"
    os.makedirs(data_dir, exist_ok=True)
    defaults = {
        "acme.json": {
            "company_name": "ACME",
            "values": ["ê³ ê°ì§‘ì°©", "ë°ì´í„°ê¸°ë°˜", "ì£¼ë„ì  ì‹¤í–‰"],
            "role": "ë°ì´í„° ì• ë„ë¦¬ìŠ¤íŠ¸",
            "role_requirements": ["SQL/EDA", "ì§€í‘œ ì„¤ê³„", "ë¦¬í¬íŒ…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜"],
            "recent_projects": ["êµ¬ë… ì „í™˜ í¼ë„ ìµœì í™”", "ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ íŒŒì¼ëŸ¿"]
        },
        "contoso.json": {
            "company_name": "Contoso",
            "values": ["ì†Œìœ ê°", "í˜‘ì—…", "ê³ ê° ì„±ê³µ"],
            "role": "ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´",
            "role_requirements": ["ëª¨ë¸ ì„œë¹™/ëª¨ë‹ˆí„°ë§", "ì„±ëŠ¥-ë¹„ìš© ìµœì í™”", "A/Bí…ŒìŠ¤íŠ¸"],
            "recent_projects": ["ì¶”ì²œ ì‹œìŠ¤í…œ ë¦¬ë­í‚¹", "ì‹¤ì‹œê°„ í”¼ë“œ ë­í‚¹"]
        }
    }
    for fn, payload in defaults.items():
        path = os.path.join(data_dir, fn)
        if not os.path.exists(path):
            with open(path, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)

    files = [f for f in os.listdir(data_dir) if f.endswith(".json")]
    company_file = st.selectbox("íšŒì‚¬ í”„ë¡œí•„ íŒŒì¼", files, index=0)

    uploaded_company = st.file_uploader("ë˜ëŠ” íšŒì‚¬ í”„ë¡œí•„ JSON ì—…ë¡œë“œ", type=["json"])

    # ğŸ” ìë™ í”„ë¡œí•„ ìƒì„±
    st.markdown("#### ğŸ” ìë™ í”„ë¡œí•„ ìƒì„± (ì´ë¦„/í™ˆí˜ì´ì§€)")
    auto_name = st.text_input("íšŒì‚¬ ì´ë¦„ (ì˜ˆ: ë„¤ì´ë²„, Kakao, Samsung SDS)")
    auto_home = st.text_input("í™ˆí˜ì´ì§€ URL (ì„ íƒ, ì˜ˆ: https://www.kakaoenterprise.com)")
    auto_add_to_rag = st.checkbox("í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ RAGì— ìë™ ì¶”ê°€", value=True)
    diversity_k = st.slider("ì§ˆë¬¸ í›„ë³´ ê°œìˆ˜(ë‹¤ì–‘ì„±)", 3, 7, 5, 1)

    if st.button("ìë™ í”„ë¡œí•„ ë§Œë“¤ê¸°"):
        if not auto_name.strip():
            st.warning("íšŒì‚¬ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        else:
            with st.spinner("íšŒì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘... (Wikipedia + í™ˆí˜ì´ì§€ ìš”ì•½)"):
                auto_company = build_auto_company(auto_name.strip(), auto_home.strip() or None)
            st.session_state["company_override"] = auto_company
            st.success("ìë™ í”„ë¡œí•„ ìƒì„± ì™„ë£Œ! (íšŒì‚¬ ìš”ì•½ì— ì ìš©ë¨)")

            # ì„ íƒ: í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ RAGì— ë„£ê¸°
            if auto_add_to_rag and auto_home.strip():
                try:
                    pages = [auto_home.strip(), auto_home.strip().rstrip("/")+"/about",
                             auto_home.strip().rstrip("/")+"/values",
                             auto_home.strip().rstrip("/")+"/mission"]
                    texts = []
                    for u in pages:
                        try:
                            r = requests.get(u, timeout=6)
                            if r.status_code == 200 and "text/html" in r.headers.get("content-type",""):
                                s = BeautifulSoup(r.text, "html.parser")
                                txts = [_clean_text(t.get_text(" ")) for t in s.find_all(["h1","h2","h3","p","li"])]
                                texts.append("\n".join([t for t in txts if len(t) > 10]))
                        except Exception:
                            pass
                    full = "\n\n".join(texts)
                    if full:
                        chs = chunk_text(full, 900, 150)
                        embs = client.embeddings.create(model="text-embedding-3-small", input=chs)
                        embs = np.array([d.embedding for d in embs.data], dtype=np.float32)
                        if "rag_store" not in st.session_state:
                            st.session_state.rag_store = {"chunks": [], "embeds": None}
                        st.session_state.rag_store["chunks"] = (st.session_state.rag_store.get("chunks", []) or []) + chs
                        if st.session_state.rag_store.get("embeds") is None or st.session_state.rag_store["embeds"].size == 0:
                            st.session_state.rag_store["embeds"] = embs
                        else:
                            st.session_state.rag_store["embeds"] = np.vstack([st.session_state.rag_store["embeds"], embs])
                        st.info(f"RAGì— í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ {len(chs)}ê°œ ì²­í¬ ì¶”ê°€")
                except Exception:
                    st.warning("í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ RAG ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ìˆì–´ ì¼ë¶€ë§Œ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

    st.markdown("#### ì§ˆë¬¸ ì˜µì…˜")
    q_type = st.selectbox("ì§ˆë¬¸ ìœ í˜•", ["í–‰ë™(STAR)", "ê¸°ìˆ  ì‹¬ì¸µ", "í•µì‹¬ê°€ì¹˜ ì í•©ì„±", "ì—­ì§ˆë¬¸"], index=0)
    level = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´", "ë¯¸ë“¤", "ì‹œë‹ˆì–´"], index=0)

    st.markdown("#### RAG (ì„ íƒ)")
    rag_enabled = st.toggle("íšŒì‚¬ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸/ì½”ì¹­ ì‚¬ìš©", value=True)
    chunk_size = st.slider("ì²­í¬ ê¸¸ì´(ë¬¸ì)", 400, 2000, 900, 100)
    chunk_overlap = st.slider("ì˜¤ë²„ë©(ë¬¸ì)", 0, 400, 150, 10)
    top_k = st.slider("ê²€ìƒ‰ ìƒìœ„ K", 1, 8, 4, 1)
    st.caption("TXT/MD/PDF ì—…ë¡œë“œ ê°€ëŠ¥ (ì„¸ì…˜ ë©”ëª¨ë¦¬ ë‚´ ì²˜ë¦¬)")
    docs = st.file_uploader("íšŒì‚¬ ë¬¸ì„œ ì—…ë¡œë“œ (ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥)", type=["txt", "md", "pdf"], accept_multiple_files=True)

# ============================================
# íšŒì‚¬ í”„ë¡œí•„ ì„ íƒ ìš°ì„ ìˆœìœ„: ìë™ ìƒì„± > ì—…ë¡œë“œ > ê¸°ë³¸ ì˜ˆì‹œ
# ============================================
if uploaded_company is not None:
    try:
        company = json.load(uploaded_company)
    except Exception as e:
        st.error(f"íšŒì‚¬ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        st.stop()
else:
    company = json.load(open(os.path.join("data/companies", company_file), "r", encoding="utf-8"))

if "company_override" in st.session_state:
    company = st.session_state["company_override"]

# ============================================
# ì„¸ì…˜ ìƒíƒœ
# ============================================
if "rag_store" not in st.session_state:
    st.session_state.rag_store = {"chunks": [], "embeds": None}
if "history" not in st.session_state:
    st.session_state.history = []  # [{ts, question, user_answer, score, feedback, competencies, supports}]
if "current_question" not in st.session_state:
    st.session_state.current_question = ""

# ============================================
# ì—…ë¡œë“œ ë¬¸ì„œ â†’ RAG ì¸ë±ìŠ¤ ì¤€ë¹„
# ============================================
def embed_texts(client: OpenAI, embed_model: str, texts: List[str]) -> np.ndarray:
    if not texts:
        return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=embed_model, input=texts)
    return np.array([d.embedding for d in resp.data], dtype=np.float32)

if rag_enabled and docs:
    with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
        all_chunks = []
        for up in docs:
            text = read_file_to_text(up)
            if not text: continue
            all_chunks.extend(chunk_text(text, chunk_size, chunk_overlap))
        if all_chunks:
            embeds = embed_texts(client, EMBED_MODEL, all_chunks)
            st.session_state.rag_store["chunks"] = (st.session_state.rag_store.get("chunks", []) or []) + all_chunks
            if st.session_state.rag_store.get("embeds") is None or st.session_state.rag_store["embeds"].size == 0:
                st.session_state.rag_store["embeds"] = embeds
            else:
                st.session_state.rag_store["embeds"] = np.vstack([st.session_state.rag_store["embeds"], embeds])
            st.success(f"RAG ì¤€ë¹„ ì™„ë£Œ: ì²­í¬ {len(all_chunks)}ê°œ ì¶”ê°€")
        else:
            st.info("ì—…ë¡œë“œ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ============================================
# ê²€ìƒ‰ / ì§ˆë¬¸ ë‹¤ì–‘ì„± ìœ í‹¸
# ============================================
def cosine_topk(matrix: np.ndarray, query: np.ndarray, k: int = 4):
    if matrix.size == 0:
        return np.array([]), np.array([], dtype=int)
    qn = query / (np.linalg.norm(query, axis=1, keepdims=True) + 1e-12)
    mn = matrix / (np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12)
    sims = mn @ qn.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    scores = sims[idx]
    return scores, idx

def retrieve_supports(query_text: str, k: int) -> List[Tuple[str, float, str]]:
    store = st.session_state.rag_store
    chunks, embeds = store.get("chunks", []), store.get("embeds", None)
    if not rag_enabled or embeds is None or len(chunks) == 0:
        return []
    qv = embed_texts(client, EMBED_MODEL, [query_text])
    scores, idxs = cosine_topk(embeds, qv, k=k)
    return [("ì—…ë¡œë“œë¬¸ì„œ/í™ˆí˜ì´ì§€", float(s), chunks[int(i)]) for s, i in zip(scores, idxs)]

def _similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def pick_diverse(candidates: list[str], history: list[str]) -> str:
    if not candidates: return ""
    if not history: return random.choice(candidates)
    best = None; best_score = 1e9
    for q in candidates:
        sim = max((_similarity(q, h) for h in history), default=0.0)
        if sim < best_score:
            best_score = sim; best = q
    return best

TYPE_INSTRUCTIONS = {
    "í–‰ë™(STAR)": "ê³¼ê±° ì‹¤ë¬´ ì‚¬ë¡€ë¥¼ ëŒì–´ë‚´ë„ë¡ S(ìƒí™©)-T(ê³¼ì œ)-A(í–‰ë™)-R(ì„±ê³¼)ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸",
    "ê¸°ìˆ  ì‹¬ì¸µ": "í•µì‹¬ ê¸°ìˆ ì  ì˜ì‚¬ê²°ì •Â·íŠ¸ë ˆì´ë“œì˜¤í”„Â·ì„±ëŠ¥/ë¹„ìš©/í’ˆì§ˆ ì§€í‘œë¥¼ íŒŒê³ ë“œëŠ” ì‹¬ì¸µ ì§ˆë¬¸",
    "í•µì‹¬ê°€ì¹˜ ì í•©ì„±": "í•µì‹¬ê°€ì¹˜ì™€ íƒœë„ë¥¼ ê²€ì¦í•˜ëŠ”, ìƒí™©ê¸°ë°˜ í–‰ë™ì„ ìœ ë„í•˜ëŠ” ì§ˆë¬¸",
    "ì—­ì§ˆë¬¸": "ì§€ì›ìê°€ íšŒì‚¬ë¥¼ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í†µì°°ë ¥ ìˆëŠ” ì—­ì§ˆë¬¸"
}

def build_company_context(c: dict) -> str:
    base = textwrap.dedent(f"""
    [íšŒì‚¬ëª…] {c.get('company_name','')}
    [í•µì‹¬ê°€ì¹˜] {", ".join(c.get('values', []))}
    [ì§ë¬´] {c.get('role','')}
    [ì£¼ìš” ìš”êµ¬ì—­ëŸ‰] {", ".join(c.get('role_requirements', []))}
    [ìµœê·¼ í”„ë¡œì íŠ¸] {", ".join(c.get('recent_projects', []))}
    """).strip()
    if c.get("wiki_summary"):
        base += f"\n[ìœ„í‚¤ ìš”ì•½] {c['wiki_summary']}"
    if c.get("homepage"):
        base += f"\n[í™ˆí˜ì´ì§€] {c['homepage']}"
    return base

def build_focuses(company: dict, supports: List[Tuple[str,float,str]], k: int = 3) -> list[str]:
    pool = []
    pool += company.get("values", [])
    pool += company.get("role_requirements", [])
    pool += company.get("recent_projects", [])
    if company.get("wiki_summary"): pool += [company["wiki_summary"]]
    for _,_,txt in supports[:3]:
        pool += [t.strip() for t in re.split(r"[â€¢\-\n\.]", txt) if 6 < len(t.strip()) < 100][:3]
    pool = [p for p in pool if p]
    random.shuffle(pool)
    return pool[:k]

# ============================================
# ì§ˆë¬¸ ìƒì„± (í›„ë³´ 5ê°œ â†’ ë‹¤ì–‘ì„± ì„ íƒ)
# ============================================
def gen_question(company: dict, qtype: str, level: str, supports: List[Tuple[str, float, str]], num_candidates: int = 5) -> str:
    ctx = build_company_context(company)
    focuses = build_focuses(company, supports, k=min(3, num_candidates))
    style = TYPE_INSTRUCTIONS.get(qtype, "êµ¬ì²´ì ì´ê³  í–‰ë™ì„ ì´ëŒì–´ë‚´ëŠ” ì§ˆë¬¸")
    rag_note = ""
    if supports:
        joined = "\n".join([f"- ({s:.2f}) {txt[:200]}" for _, s, txt in supports[:3]])
        rag_note = f"\n[ê·¼ê±° ë°œì·Œ]\n{joined}"

    sys = f"""ë„ˆëŠ” '{company.get('company_name','')}'ì˜ '{company.get('role','')}' ë©´ì ‘ê´€ì´ë‹¤.
íšŒì‚¬ ë§¥ë½ê³¼ (ìˆë‹¤ë©´) ê·¼ê±° ë¬¸ì„œë¥¼ ë°˜ì˜í•˜ì—¬ **{qtype}** ìœ í˜•({style})ì˜ ì§ˆë¬¸ **{num_candidates}ê°œ í›„ë³´**ë¥¼ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ë¼.
ê° í›„ë³´ëŠ” ì„œë¡œ **í˜•íƒœ/ê´€ì /í‚¤ì›Œë“œ**ê°€ ë‹¬ë¼ì•¼ í•œë‹¤. ë‚œì´ë„ëŠ” {level}.
ë°˜ë“œì‹œ ì•„ë˜ 'í¬ì»¤ìŠ¤' ì¤‘ ìµœì†Œ 1ê°œ í‚¤ì›Œë“œë¥¼ ë³¸ë¬¸ì— **ëª…ì‹œì ìœ¼ë¡œ í¬í•¨**í•˜ë¼.
í¬ë§·: 1) ... 2) ... 3) ... ... (í•œ ì¤„ì”©)"""
    user = f"""[íšŒì‚¬ ì»¨í…ìŠ¤íŠ¸]
{ctx}
[í¬ì»¤ìŠ¤]
- {chr(10).join(focuses)}{rag_note}"""

    resp = client.chat.completions.create(
        model=MODEL, temperature=0.9,
        messages=[{"role":"system","content":sys},{"role":"user","content":user}]
    )
    raw = resp.choices[0].message.content.strip()
    cands = [re.sub(r'^\s*\d+\)\s*','',line).strip() for line in raw.splitlines() if re.match(r'^\s*\d+\)', line)]
    if not cands:
        cands = [l.strip("- ").strip() for l in raw.splitlines() if len(l.strip())>0][:num_candidates]

    hist_qs = [h["question"] for h in st.session_state.get("history", [])][-8:]
    selected = pick_diverse(cands, hist_qs)
    return selected or (cands[0] if cands else "í•´ë‹¹ ì§ë¬´ì™€ íšŒì‚¬ ë§¥ë½ì„ ë°˜ì˜í•œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# ============================================
# ì½”ì¹­
# ============================================
def coach_answer(company: dict, question: str, user_answer: str, supports: List[Tuple[str, float, str]]) -> Dict:
    ctx = build_company_context(company)
    rag_note = ""
    if supports:
        joined = "\n".join([f"- ({s:.3f}) {txt[:500]}" for (_, s, txt) in supports])
        rag_note = f"\n[íšŒì‚¬ ê·¼ê±° ë¬¸ì„œ ë°œì·Œ]\n{joined}\n"
    competencies = ["ë¬¸ì œì •ì˜", "ë°ì´í„°/ì§€í‘œ", "ì‹¤í–‰ë ¥/ì£¼ë„ì„±", "í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ê³ ê°ê°€ì¹˜"]
    comp_str = ", ".join(competencies)
    sys = f"""ë„ˆëŠ” í†±í‹°ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. í•œêµ­ì–´ë¡œ ì•„ë˜ í˜•ì‹ì— ë§ì¶° ë‹µí•˜ë¼:
1) ì´ì : 0~10 ì •ìˆ˜ 1ê°œ
2) ê°•ì : 2~3ê°œ ë¶ˆë¦¿
3) ë¦¬ìŠ¤í¬: 2~3ê°œ ë¶ˆë¦¿
4) ê°œì„  í¬ì¸íŠ¸: 3ê°œ ë¶ˆë¦¿ (í–‰ë™Â·ì§€í‘œÂ·ì„íŒ©íŠ¸ ì¤‘ì‹¬)
5) ìˆ˜ì •ë³¸ ë‹µë³€: STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼) êµ¬ì¡°ë¡œ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ
6) ì—­ëŸ‰ ì ìˆ˜: [{comp_str}] ê°ê° 0~5 ì •ìˆ˜ (í•œ ì¤„ì— ì‰¼í‘œë¡œ êµ¬ë¶„)
ì±„ì  ê¸°ì¤€ì€ íšŒì‚¬ ë§¥ë½, ì§ˆë¬¸ ì˜ë„, ê·¸ë¦¬ê³  ì§ˆë¬¸ ë‚´ **í¬ì»¤ìŠ¤/í‚¤ì›Œë“œ**ì— ë¶€í•©í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í¬í•¨í•œë‹¤.
ì¶”ê°€ ì„¤ëª… ê¸ˆì§€. í˜•ì‹ ìœ ì§€."""
    user = f"""[íšŒì‚¬ ì»¨í…ìŠ¤íŠ¸]
{ctx}
{rag_note}
[ë©´ì ‘ ì§ˆë¬¸]
{question}

[í›„ë³´ì ë‹µë³€]
{user_answer}
"""
    resp = client.chat.completions.create(
        model=MODEL, temperature=0.4,
        messages=[{"role":"system","content":sys},{"role":"user","content":user}]
    )
    content = resp.choices[0].message.content.strip()
    m = re.search(r'([0-9]{1,2})\s*(?:/10|ì |$)', content)
    score = None
    if m:
        try:
            score = int(m.group(1)); score = max(0, min(10, score))
        except: pass
    comp_scores = None
    nums = re.findall(r'\b([0-5])\b', content.splitlines()[-1])
    if len(nums) >= 5:
        comp_scores = [int(x) for x in nums[:5]]
    return {"raw": content, "score": score, "competencies": comp_scores}

# ============================================
# UI ë³¸ë¬¸
# ============================================
left, right = st.columns([1, 1])

# íšŒì‚¬ í”„ë¡œí•„ ì ìš©
if uploaded_company is not None:
    pass  # ì´ë¯¸ ìœ„ì—ì„œ company ì„¤ì •ë¨
if "company_override" in st.session_state:
    company = st.session_state["company_override"]

with left:
    st.header("â‘  ì§ˆë¬¸ ìƒì„±")
    st.markdown("**íšŒì‚¬ ìš”ì•½**")
    st.json(company, expanded=False)

    prompt_hint = st.text_input("ì§ˆë¬¸ ìƒì„± íŒíŠ¸(ì„ íƒ)", placeholder="ì˜ˆ: ì „í™˜ í¼ë„ / ìš´ì˜ ìë™í™” / ëª¨ë¸ ì„±ëŠ¥-ë¹„ìš© íŠ¸ë ˆì´ë“œì˜¤í”„")
    if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", use_container_width=True):
        try:
            supports = []
            if rag_enabled and (docs or st.session_state.rag_store.get("chunks")):
                base_q = prompt_hint.strip() or f"{company.get('role','')} {', '.join(company.get('values', []))}"
                supports = retrieve_supports(base_q, top_k)
            q = gen_question(company, q_type, level, supports, num_candidates=st.session_state.get("diversity_k", 5))
            st.session_state.current_question = q
            st.session_state.last_supports_q = supports
        except Exception as e:
            st.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")

    st.text_area("ì§ˆë¬¸", height=110, value=st.session_state.get("current_question",""))

    if rag_enabled and st.session_state.get("last_supports_q"):
        with st.expander("ì§ˆë¬¸ ìƒì„±ì— ì‚¬ìš©ëœ ê·¼ê±° ë³´ê¸°"):
            for i, (_, sc, txt) in enumerate(st.session_state.last_supports_q, 1):
                st.markdown(f"**[{i}] sim={sc:.3f}**\n\n{txt[:600]}{'...' if len(txt)>600 else ''}")
                st.markdown("---")

with right:
    st.header("â‘¡ ë‚˜ì˜ ë‹µë³€")
    answer = st.text_area("ì—¬ê¸°ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš” (STAR ê¶Œì¥: ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)", height=160)
    if st.button("ì±„ì  & ì½”ì¹­", type="primary", use_container_width=True):
        if not st.session_state.get("current_question"):
            st.warning("ë¨¼ì € 'ìƒˆ ì§ˆë¬¸ ë°›ê¸°'ë¡œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        elif not answer.strip():
            st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ì½”ì¹­ ì¤‘..."):
                try:
                    supports = []
                    if rag_enabled and (docs or st.session_state.rag_store.get("chunks")):
                        q_for_rag = st.session_state["current_question"] + "\n" + answer[:800]
                        supports = retrieve_supports(q_for_rag, top_k)
                    res = coach_answer(company, st.session_state["current_question"], answer, supports)
                    st.session_state.history.append({
                        "ts": pd.Timestamp.now(),
                        "question": st.session_state["current_question"],
                        "user_answer": answer,
                        "score": res.get("score"),
                        "feedback": res.get("raw"),
                        "supports": supports,
                        "competencies": res.get("competencies")
                    })
                except Exception as e:
                    st.error(f"ì½”ì¹­ ì˜¤ë¥˜: {e}")

# ============================================
# ê²°ê³¼ / ë ˆì´ë” / ë¦¬í¬íŠ¸
# ============================================
st.divider()
st.subheader("â‘¢ í”¼ë“œë°± ê²°ê³¼")
if st.session_state.history:
    last = st.session_state.history[-1]
    c1, c2 = st.columns([1, 3])
    with c1:
        st.metric("ì´ì (/10)", last.get("score", "â€”"))
    with c2:
        st.markdown(last.get("feedback", ""))

    if rag_enabled and last.get("supports"):
        with st.expander("ì½”ì¹­ì— ì‚¬ìš©ëœ ê·¼ê±° ë³´ê¸°"):
            for i, (_, sc, txt) in enumerate(last["supports"], 1):
                st.markdown(f"**[{i}] sim={sc:.3f}**\n\n{txt[:800]}{'...' if len(txt)>800 else ''}")
                st.markdown("---")

st.divider()
st.subheader("â‘£ ì—­ëŸ‰ ë ˆì´ë” (ì„¸ì…˜ ëˆ„ì )")
competencies = ["ë¬¸ì œì •ì˜", "ë°ì´í„°/ì§€í‘œ", "ì‹¤í–‰ë ¥/ì£¼ë„ì„±", "í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ê³ ê°ê°€ì¹˜"]

def compute_comp_df(hist):
    rows = []
    for h in hist:
        if h.get("competencies") and len(h["competencies"]) == 5:
            rows.append(h["competencies"])
    if not rows:
        return None
    return pd.DataFrame(rows, columns=competencies)

comp_df = compute_comp_df(st.session_state.history)
if comp_df is not None:
    avg_scores = comp_df.mean().values.tolist()
    if PLOTLY_OK:
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(
            r=avg_scores + [avg_scores[0]],
            theta=competencies + [competencies[0]],
            fill='toself',
            name='í‰ê· (0~5)'
        ))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,5])), showlegend=False, height=420)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Plotly ë¯¸ì„¤ì¹˜ ìƒíƒœ â€” ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        st.bar_chart(pd.DataFrame({"score": avg_scores}, index=competencies))
    st.dataframe(comp_df, use_container_width=True)
else:
    st.info("ì•„ì§ ì—­ëŸ‰ ì ìˆ˜ê°€ íŒŒì‹±ëœ ì½”ì¹­ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("â‘¤ ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV)")
def build_report_df(hist):
    rows = []
    for h in hist:
        row = {
            "timestamp": h.get("ts"),
            "question": h.get("question"),
            "user_answer": h.get("user_answer"),
            "score": h.get("score"),
            "feedback_raw": h.get("feedback"),
        }
        comps = h.get("competencies")
        if comps and len(comps) == 5:
            for k, v in zip(competencies, comps):
                row[f"comp_{k}"] = v
        sups = h.get("supports") or []
        row["supports_preview"] = " || ".join([s[2][:120].replace("\n"," ") for s in sups])
        rows.append(row)
    if not rows:
        return pd.DataFrame(columns=["timestamp","question","user_answer","score","feedback_raw","supports_preview"])
    return pd.DataFrame(rows)

report_df = build_report_df(st.session_state.history)
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=report_df.to_csv(index=False).encode("utf-8-sig"),
                   file_name="interview_session_report.csv", mime="text/csv")

st.caption("Tip: ìë™ í”„ë¡œí•„ì€ ìœ„í‚¤/í™ˆí˜ì´ì§€ ê°€ìš©ì„±ì— ë”°ë¼ ì •í™•ë„ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•„ìš”í•˜ë©´ íšŒì‚¬ ë¬¸ì„œë¥¼ ì¶”ê°€ ì—…ë¡œë“œí•˜ì—¬ RAGë¥¼ ê°•í™”í•˜ì„¸ìš”.")
