# -*- coding: utf-8 -*-
import os, re, json, textwrap, time, urllib.parse, html
from typing import Optional, Tuple, Dict, List

import requests
from bs4 import BeautifulSoup
import html2text
import streamlit as st

# ==============================
# Page config
# ==============================
st.set_page_config(page_title="ì§€ì› íšŒì‚¬ íŠ¹í™” ì·¨ì—… ì¤€ë¹„ ì½”ì¹˜ (v1 - ì›ë¬¸ í™•ë³´ ì§‘ì¤‘)", page_icon="ğŸ§­", layout="wide")

# ==============================
# Helpers: secrets/env
# ==============================
def get_secret(name: str, default: Optional[str] = None) -> Optional[str]:
    v = os.getenv(name)
    if v:
        return v
    try:
        return st.secrets.get(name, default)
    except Exception:
        return default

NAVER_CLIENT_ID = get_secret("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = get_secret("NAVER_CLIENT_SECRET")

# ==============================
# Core 1: Robust text fetchers
# - Jina Reader (SPA/ë”ë³´ê¸° ìš°ì„ )
# - WebBaseLoader ìœ ì‚¬(ì •ì  HTML + html2text)
# - BS4 fallback
# ==============================
def normalize_url(u: str) -> Optional[str]:
    if not u:
        return None
    u = u.strip()
    if not re.match(r"^https?://", u):
        u = "https://" + u
    # remove fragments
    u = urllib.parse.urlsplit(u)
    u = urllib.parse.urlunsplit((u.scheme, u.netloc, u.path, u.query, ""))  # drop fragment
    return u

def http_get(url: str, timeout: int = 12) -> Optional[requests.Response]:
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36",
            "Accept-Language": "ko, en;q=0.9",
        }
        r = requests.get(url, headers=headers, timeout=timeout)
        if r.status_code == 200 and "text/html" in r.headers.get("content-type", ""):
            return r
    except Exception:
        pass
    return None

def fetch_jina(url: str, timeout: int = 15) -> str:
    """
    Jina Reader: ë™ì  ë Œë”ë§/ë”ë³´ê¸° í…ìŠ¤íŠ¸ê¹Œì§€ í”„ë¦¬ë Œë”í•œ ê²°ê³¼ë¥¼ textë¡œ ë°˜í™˜.
    """
    try:
        # jina reader: https://r.jina.ai/http://example.com
        # (httpsë„ http ì ‘ë‘ ì‚¬ìš© ê¶Œì¥. ì›ë³¸ì´ httpsì—¬ë„ ìƒê´€ ì—†ìŒ)
        prox = f"https://r.jina.ai/http://{urllib.parse.urlsplit(url).netloc}{urllib.parse.urlsplit(url).path}"
        if urllib.parse.urlsplit(url).query:
            prox += f"?{urllib.parse.urlsplit(url).query}"
        r = http_get(prox, timeout=timeout)
        if r and r.text:
            return r.text.strip()
    except Exception:
        pass
    return ""

def html_to_text(html_str: str) -> str:
    try:
        conv = html2text.HTML2Text()
        conv.ignore_links = True
        conv.ignore_images = True
        conv.body_width = 0
        txt = conv.handle(html_str)
        txt = re.sub(r"\n{3,}", "\n\n", txt)
        return txt.strip()
    except Exception:
        return ""

def fetch_static(url: str, timeout: int = 12) -> str:
    """
    WebBaseLoader ìœ ì‚¬: ì •ì  HTMLì„ ë°›ì•„ html2textë¡œ ë³€í™˜
    """
    r = http_get(url, timeout=timeout)
    if not r:
        return ""
    return html_to_text(r.text)

def fetch_bs4_blocks(url: str, timeout: int = 12) -> str:
    r = http_get(url, timeout=timeout)
    if not r:
        return ""
    soup = BeautifulSoup(r.text, "lxml")

    # ê¸´ ë³¸ë¬¸ ìœ„ì£¼ë¡œ í•©ì¹˜ê¸°
    blocks = []
    for sel in ["article", "section", "main", "div", "ul", "ol"]:
        for el in soup.select(sel):
            txt = el.get_text(" ", strip=True)
            if txt and len(txt) > 300:
                blocks.append(txt)
    if not blocks:
        # ì „ì²´ì—ì„œë¼ë„
        txt = soup.get_text(" ", strip=True)
        return txt[:150000]
    # ì¤‘ë³µ ì œê±° + í•©ì¹˜ê¸°
    seen = set(); out=[]
    for b in blocks:
        b = re.sub(r"\s+", " ", b)
        if b not in seen:
            seen.add(b); out.append(b)
    return "\n\n".join(out)[:150000]

def fetch_job_text_all(url: str) -> Tuple[str, Dict]:
    """
    í•˜ë‚˜ì˜ URLì—ì„œ 3ë‹¨ê³„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ í™•ë³´.
    ë°˜í™˜: (ê²°ê³¼ í…ìŠ¤íŠ¸, ë””ë²„ê·¸ë©”íƒ€)
    """
    url = normalize_url(url)
    if not url:
        return "", {"error": "invalid_url"}

    jina_txt = fetch_jina(url)
    webbase_txt = fetch_static(url) if not jina_txt else ""
    bs4_txt = fetch_bs4_blocks(url) if (not jina_txt and not webbase_txt) else ""

    # íŠ¹ìˆ˜: ì›í‹°ë“œ JSON-LD ë³´ê°•
    enrich = ""
    try:
        if "wanted.co.kr" in url:
            r = http_get(url)
            if r:
                soup = BeautifulSoup(r.text, "lxml")
                for s in soup.find_all("script", {"type": "application/ld+json"}):
                    try:
                        data = json.loads(s.string or "{}")
                        if isinstance(data, dict) and data.get("@type") == "JobPosting":
                            desc = data.get("description") or ""
                            desc = BeautifulSoup(desc, "lxml").get_text(" ", strip=True)
                            if len(desc) > 200:
                                enrich = desc
                                break
                        elif isinstance(data, list):
                            for obj in data:
                                if isinstance(obj, dict) and obj.get("@type") == "JobPosting":
                                    desc = BeautifulSoup(obj.get("description",""), "lxml").get_text(" ", strip=True)
                                    if len(desc) > 200:
                                        enrich = desc
                                        break
                    except Exception:
                        continue
    except Exception:
        pass

    # ì¡°í•© (ìš°ì„ ìˆœìœ„: Jina > WebBase > BS4 > Enrich)
    base = jina_txt or webbase_txt or bs4_txt or ""
    if enrich and enrich not in base:
        base = base + "\n\n" + enrich

    lens = {
        "jina": len(jina_txt),
        "webbase": len(webbase_txt),
        "bs4": len(bs4_txt),
        "enrich": len(enrich),
    }
    return base.strip(), {"url_final": url, "lens": lens}

# ==============================
# Core 2: ì±„ìš© URL íƒìƒ‰
# - Naver OpenAPI (í‚¤ ìˆìœ¼ë©´)
# - DuckDuckGo HTML í´ë°±
# - ëª©ë¡ â†’ ìƒì„¸í™”
# ==============================
JOB_SITES = [
    "wanted.co.kr", "saramin.co.kr", "jobkorea.co.kr",
    "rocketpunch.com", "indeed.com", "linkedin.com"
]

def naver_search_web(query: str, display: int = 10) -> List[str]:
    if not (NAVER_CLIENT_ID and NAVER_CLIENT_SECRET):
        return []
    try:
        url = "https://openapi.naver.com/v1/search/webkr.json"
        headers = {
            "X-Naver-Client-Id": NAVER_CLIENT_ID,
            "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
        }
        params = {"query": query, "display": display, "sort": "date"}
        r = requests.get(url, headers=headers, params=params, timeout=10)
        if r.status_code != 200:
            return []
        data = r.json()
        out = []
        for it in data.get("items", []):
            link = it.get("link")
            if link: out.append(link)
        return out
    except Exception:
        return []

def ddg_search(query: str, limit: int = 10) -> List[str]:
    try:
        url = f"https://duckduckgo.com/html/?q={urllib.parse.quote(query)}"
        r = http_get(url, timeout=10)
        if not r: return []
        soup = BeautifulSoup(r.text, "lxml")
        out=[]
        for a in soup.select("a.result__a, a.result__url, a[href]"):
            href = a.get("href")
            if not href: continue
            # duckduckgo redirect decode
            if href.startswith("/l/?kh=") and "uddg=" in href:
                href = urllib.parse.unquote(href.split("uddg=")[-1])
            if re.match(r"^https?://", href):
                out.append(href)
            if len(out) >= limit: break
        return out
    except Exception:
        return []

def first_detail_from_list(url: str) -> str:
    """
    ëª©ë¡ URLì´ë©´ ì²« ìƒì„¸ ê³µê³  ë§í¬ë¡œ í•œ ë²ˆ ë” ì§„ì…
    """
    try:
        r = http_get(url, timeout=8)
        if not r: return url
        soup = BeautifulSoup(r.text, "lxml")
        # ì‚¬ì´íŠ¸ë³„ ëŒ€ì¶© ë§ëŠ” ì…€ë ‰í„°ë“¤
        cand = []
        # ì›í‹°ë“œ
        cand += [a["href"] for a in soup.select("a.JobCard, a[href*='/wd/']") if a.has_attr("href")]
        # ì‚¬ëŒì¸/ì¡ì½”ë¦¬ì•„
        cand += [a["href"] for a in soup.select("a[href*='view.asp'], a[href*='/Recruit/']:not([href*='Search'])") if a.has_attr("href")]
        # ë¡œì¼“í€ì¹˜/ê¸°íƒ€
        cand += [a["href"] for a in soup.select("a[href*='/companies/'], a[href*='/jobs/']") if a.has_attr("href")]

        for h in cand:
            if not re.match(r"^https?://", h):
                h = urllib.parse.urljoin(url, h)
            d = urllib.parse.urlsplit(h).netloc
            if any(s in d for s in JOB_SITES):
                return normalize_url(h)
    except Exception:
        pass
    return url

def discover_job_urls(company: str, role: str, limit: int = 8) -> List[str]:
    queries = []
    if role:
        queries.append(f"{company} {role} ì±„ìš©")
        queries.append(f"{company} {role} ê³µê³ ")
    queries.append(f"{company} ì±„ìš© ê³µê³ ")
    queries.append(f"{company} hiring jobs")

    urls = []
    # 1) NAVER
    for q in queries:
        for lk in naver_search_web(q, display=10):
            d = urllib.parse.urlsplit(lk).netloc
            if any(s in d for s in JOB_SITES) and lk not in urls:
                urls.append(lk)
            if len(urls) >= limit: break
        if len(urls) >= limit: break

    # 2) DuckDuckGo
    if len(urls) < 2:
        for q in queries:
            for lk in ddg_search(q, limit=12):
                d = urllib.parse.urlsplit(lk).netloc
                if any(s in d for s in JOB_SITES) and lk not in urls:
                    urls.append(lk)
                if len(urls) >= limit: break
            if len(urls) >= limit: break

    # ëª©ë¡ â†’ ìƒì„¸í™”
    detail = []
    for u in urls[:limit]:
        detail.append(first_detail_from_list(u))
    # ì¤‘ë³µ ì œê±°
    seen=set(); out=[]
    for u in detail:
        if u not in seen:
            seen.add(u); out.append(u)
    return out[:limit]

def pick_best_fetchable(urls: List[str], min_len: int = 800) -> Tuple[Optional[str], Dict]:
    """
    ì—¬ëŸ¬ URL ì¤‘ í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œë¡œ ê¸¸ê²Œ ë½‘ì„ ìˆ˜ ìˆëŠ” 'ê°€ì¥ ì¢‹ì€' í›„ë³´ 1ê°œ ì„ íƒ
    """
    best = None
    best_meta = {}
    best_score = -1
    for u in urls:
        txt, meta = fetch_job_text_all(u)
        score = len(txt)
        if score > best_score:
            best = u; best_meta = meta; best_score = score
        if score >= min_len:
            break
    return best, {"tried": urls, "chosen_meta": best_meta}

# ==============================
# UI â€” ì‚¬ì´ë“œë°”: ì„¤ì •/ë„ì›€
# ==============================
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •/ë„ì›€")
    st.markdown("- NAVER í‚¤ê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ í’ˆì§ˆâ†‘ (ì„ íƒ)")
    st.json({
        "NAVER_CLIENT_ID": bool(NAVER_CLIENT_ID),
        "NAVER_CLIENT_SECRET": bool(NAVER_CLIENT_SECRET),
    })
    st.caption("ì›ë¬¸ í™•ë³´ê°€ ìš°ì„  ê³¼ì œì…ë‹ˆë‹¤. ì´í›„ ìì†Œì„œ/RAG/ì§ˆë¬¸/ì±„ì ì€ ì´ ì›ë¬¸ì„ ê·¼ê±°ë¡œ ì—°ê²°í•˜ë©´ ë©ë‹ˆë‹¤.")

st.title("ì§€ì› íšŒì‚¬ íŠ¹í™” ì·¨ì—… ì¤€ë¹„ ì½”ì¹˜ Â· ì›ë¬¸ í™•ë³´ ì§‘ì¤‘íŒ")

# ==============================
# ë‹¨ìœ„ A) ì›ë¬¸ í…ŒìŠ¤íŠ¸(ì§ì ‘ URL)
# ==============================
st.header("ë‹¨ìœ„ A) ì›ë¬¸ í…ŒìŠ¤íŠ¸ (ì§ì ‘ URL)")
test_url = st.text_input("í…ŒìŠ¤íŠ¸í•  ì±„ìš© ìƒì„¸ URLì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="https://www.wanted.co.kr/wd/123456")
if st.button("ğŸ“„ í…ìŠ¤íŠ¸ ì‹¤í–‰", type="secondary"):
    if not test_url.strip():
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ì›ë¬¸ ìˆ˜ì§‘ ì¤‘..."):
            txt, meta = fetch_job_text_all(test_url)
        st.json({"url_final": meta.get("url_final"), "lens": meta.get("lens")})
        st.write(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(txt)}")
        st.text_area("ë¯¸ë¦¬ë³´ê¸°(ì• 3000ì)", value=txt[:3000], height=220)
        st.download_button("ì›ë¬¸ ë‹¤ìš´ë¡œë“œ", data=txt.encode("utf-8"), file_name="raw_job_text.txt", mime="text/plain")

st.divider()

# ==============================
# ë‹¨ìœ„ B) íšŒì‚¬ëª… + ì§ë¬´ë¡œ ì±„ìš© URL ìë™ íƒìƒ‰
# ==============================
st.header("ë‹¨ìœ„ B) íšŒì‚¬ëª…+ì§ë¬´ â†’ ì±„ìš© URL ìë™íƒìƒ‰")
col = st.columns(3)
with col[0]:
    company = st.text_input("íšŒì‚¬ ì´ë¦„", placeholder="ì˜ˆ: í™”í•´ê¸€ë¡œë²Œ / NAVER / ì¹´ì¹´ì˜¤ ë“±")
with col[1]:
    role = st.text_input("ì§€ì› ì§ë¬´ëª…", placeholder="ì˜ˆ: Data Engineer / Data Analyst ...")
with col[2]:
    min_len = st.number_input("ìµœì†Œ ê¸¸ì´(ë¬¸ì)", value=800, min_value=0, step=100)

auto_state = st.empty()
if st.button("ğŸ” ì±„ìš© URL ì°¾ê¸°", type="primary"):
    if not company.strip():
        st.warning("íšŒì‚¬ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ê²€ìƒ‰ â†’ í›„ë³´URL ëª¨ìœ¼ëŠ” ì¤‘..."):
            urls = discover_job_urls(company.strip(), role.strip(), limit=8)
        st.write("í›„ë³´ URL:", urls if urls else "(ì—†ìŒ)")

        chosen, choose_meta = (None, {})
        if urls:
            with st.spinner("í›„ë³´ URL ì‹¤ì œë¡œ í…ìŠ¤íŠ¸ ë½‘íˆëŠ”ì§€ ê²€ì‚¬ì¤‘..."):
                chosen, choose_meta = pick_best_fetchable(urls, min_len=min_len)

        if chosen:
            st.success("ìƒì„¸ URL ì„ íƒ ì™„ë£Œ")
            st.code(chosen, language="text")
            st.json(choose_meta)
            st.session_state["chosen_job_url"] = chosen
        else:
            st.error("í…ìŠ¤íŠ¸ë¥¼ ì¶©ë¶„íˆ ë½‘ì„ ìˆ˜ ìˆëŠ” URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë¡œê·¸ì¸/ì°¨ë‹¨/SPA ê°€ëŠ¥)")

st.divider()

# ==============================
# ë‹¨ìœ„ C) íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´ â€” 'ì›ë¬¸ ê·¸ëŒ€ë¡œ' ì¶œë ¥
# ==============================
st.header("ë‹¨ìœ„ C) íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´ (ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶œë ¥)")

col2 = st.columns(2)
with col2[0]:
    st.subheader("íšŒì‚¬ ìš”ì•½ (ì›ë¬¸ ì „ì²´)")
    raw_company = ""
    # íšŒì‚¬ ì†Œê°œëŠ” ê³µì‹ í™ˆí˜ì´ì§€ or ë‰´ìŠ¤ê°€ í•„ìš”í•˜ì§€ë§Œ
    # ì—¬ê¸°ì„  'ì±„ìš© ê³µê³  ì›ë¬¸'ì„ ìš°ì„  í‘œì¤€ ì†ŒìŠ¤ë¡œ ì‚¬ìš© (í›„ì† ë‹¨ê³„ì—ì„œ ë³„ë„ ë³´ê°•)
    # ì‚¬ìš©ìê°€ ì§ì ‘ íšŒì‚¬ í™ˆí˜ì´ì§€ URLì„ ë„£ìœ¼ë©´ ê·¸ ì›ë¬¸ë„ í•¨ê»˜ ë³´ì—¬ì¤Œ
    home_url = st.text_input("(ì„ íƒ) íšŒì‚¬ í™ˆí˜ì´ì§€ ë˜ëŠ” íšŒì‚¬ ì†Œê°œê°€ ìˆëŠ” URL")
    if st.button("ğŸ  íšŒì‚¬ ì†Œê°œ URL ì›ë¬¸ ë¶ˆëŸ¬ì˜¤ê¸°", key="btn_home"):
        if not home_url.strip():
            st.warning("íšŒì‚¬ ì†Œê°œ URLì„ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            with st.spinner("íšŒì‚¬ ì†Œê°œ ì›ë¬¸ ìˆ˜ì§‘..."):
                raw_company, meta = fetch_job_text_all(home_url.strip())
            st.json(meta)
            st.write(f"íšŒì‚¬ì†Œê°œ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_company)}")
            st.text_area("íšŒì‚¬ ì†Œê°œ ì›ë¬¸", value=raw_company, height=360)
            if raw_company:
                st.download_button("íšŒì‚¬ ì†Œê°œ ì›ë¬¸ ë‹¤ìš´ë¡œë“œ", data=raw_company.encode("utf-8"),
                                   file_name="raw_company_text.txt", mime="text/plain")

with col2[1]:
    st.subheader("ì±„ìš© ìš”ê±´ (ì›ë¬¸ ì „ì²´)")
    job_url = st.text_input("(ì„ íƒ) ì±„ìš© ê³µê³  ìƒì„¸ URL", value=st.session_state.get("chosen_job_url",""))
    if st.button("ğŸ§¾ ì±„ìš© ê³µê³  ì›ë¬¸ ë¶ˆëŸ¬ì˜¤ê¸°", key="btn_job"):
        if not job_url.strip():
            st.warning("ì±„ìš© ê³µê³  URLì„ ì…ë ¥í•˜ê±°ë‚˜, ìœ„ì—ì„œ ìë™íƒìƒ‰ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        else:
            with st.spinner("ì±„ìš© ê³µê³  ì›ë¬¸ ìˆ˜ì§‘..."):
                raw_job, meta = fetch_job_text_all(job_url.strip())
            st.json(meta)
            st.write(f"ì±„ìš©ìš”ê±´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_job)}")
            st.text_area("ì±„ìš© ìš”ê±´ ì›ë¬¸", value=raw_job, height=360)
            if raw_job:
                st.download_button("ì±„ìš© ìš”ê±´ ì›ë¬¸ ë‹¤ìš´ë¡œë“œ", data=raw_job.encode("utf-8"),
                                   file_name="raw_job_text.txt", mime="text/plain")

st.info("â€» ì§€ê¸ˆ ë‹¨ê³„ëŠ” 'ì›ë¬¸ í™•ë³´'ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. í›„ì†ìœ¼ë¡œ ì´ ì›ë¬¸ì„ RAGì— ë„£ì–´ ìì†Œì„œ/ì§ˆë¬¸/ì±„ì ìœ¼ë¡œ í™•ì¥í•˜ë©´ ë©ë‹ˆë‹¤.")
