# -*- coding: utf-8 -*-
import os, re, json, urllib.parse
from typing import Optional, Tuple, Dict, List

import requests
from bs4 import BeautifulSoup
import html2text
import streamlit as st

# -----------------------------
# Streamlit page
# -----------------------------
st.set_page_config(page_title="ì±„ìš© ê³µê³  íŒŒì„œ (ì§ì ‘ URL â†’ êµ¬ì¡°í™” ìš”ì•½)", page_icon="ğŸ§¾", layout="wide")
st.title("ì±„ìš© ê³µê³  íŒŒì„œ Â· ì§ì ‘ URL â†’ íšŒì‚¬ ìš”ì•½/ì±„ìš© ìš”ê±´")

# -----------------------------
# HTTP helpers
# -----------------------------
def normalize_url(u: str) -> Optional[str]:
    if not u: return None
    u = u.strip()
    if not re.match(r"^https?://", u): u = "https://" + u
    parts = urllib.parse.urlsplit(u)
    return urllib.parse.urlunsplit((parts.scheme, parts.netloc, parts.path, parts.query, ""))

def http_get(url: str, timeout: int = 12) -> Optional[requests.Response]:
    try:
        r = requests.get(
            url,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36",
                "Accept-Language": "ko, en;q=0.9",
            },
            timeout=timeout,
        )
        if r.status_code == 200 and "text/html" in r.headers.get("content-type", ""):
            return r
    except Exception:
        pass
    return None

# -----------------------------
# Text extraction (Jina â†’ WebBase â†’ BS4)
# -----------------------------
def fetch_jina_text(url: str, timeout: int = 15) -> str:
    """í”„ë¦¬ë Œë” í…ìŠ¤íŠ¸(ë”ë³´ê¸°/ë™ì  í¬í•¨ ê°€ëŠ¥)."""
    try:
        parts = urllib.parse.urlsplit(url)
        prox = f"https://r.jina.ai/http://{parts.netloc}{parts.path}"
        if parts.query: prox += f"?{parts.query}"
        r = http_get(prox, timeout=timeout)
        return r.text.strip() if r else ""
    except Exception:
        return ""

def html_to_text(html_str: str) -> str:
    conv = html2text.HTML2Text()
    conv.ignore_links = True
    conv.ignore_images = True
    conv.body_width = 0
    txt = conv.handle(html_str)
    txt = re.sub(r"\n{3,}", "\n\n", txt)
    return txt.strip()

def fetch_webbase_text(url: str) -> str:
    r = http_get(url, timeout=12)
    if not r: return ""
    return html_to_text(r.text)

def fetch_bs4_text(url: str) -> Tuple[str, Optional[BeautifulSoup]]:
    r = http_get(url, timeout=12)
    if not r: return "", None
    soup = BeautifulSoup(r.text, "lxml")

    # ê¸´ ë¸”ë¡ ìœ„ì£¼ë¡œ
    blocks = []
    for sel in ["article", "section", "main", "div", "ul", "ol"]:
        for el in soup.select(sel):
            txt = el.get_text(" ", strip=True)
            if txt and len(txt) > 300:
                txt = re.sub(r"\s+", " ", txt)
                blocks.append(txt)
    if not blocks:
        all_txt = soup.get_text(" ", strip=True)
        return all_txt[:120000], soup

    seen, out = set(), []
    for b in blocks:
        if b not in seen:
            seen.add(b); out.append(b)
    return ("\n\n".join(out)[:120000], soup)

def fetch_all_text(url: str) -> Tuple[str, Dict, Optional[BeautifulSoup]]:
    """ìµœëŒ€ì¹˜ í…ìŠ¤íŠ¸ì™€ ë””ë²„ê·¸ ë©”íƒ€, soup ë°˜í™˜"""
    url = normalize_url(url)
    if not url:
        return "", {"error":"invalid_url"}, None

    jina = fetch_jina_text(url)
    if jina:
        # soupëŠ” ë³„ë„ë¡œ
        _, soup = fetch_bs4_text(url)
        return jina, {"source":"jina","len":len(jina),"url_final":url}, soup

    webbase = fetch_webbase_text(url)
    if webbase:
        _, soup = fetch_bs4_text(url)
        return webbase, {"source":"webbase","len":len(webbase),"url_final":url}, soup

    bs, soup = fetch_bs4_text(url)
    return bs, {"source":"bs4","len":len(bs),"url_final":url}, soup

# -----------------------------
# Section parsing
# -----------------------------
H_ROLE = [r"ëª¨ì§‘\s*ë¶„ì•¼", r"ì±„ìš©\s*ë¶„ì•¼", r"Position", r"Role", r"ì§ë¬´\s*ëª…", r"Job\s*Title"]
H_RESP = [r"ì£¼ìš”\s*ì—…ë¬´", r"ë‹´ë‹¹\s*ì—…ë¬´", r"ì—…ë¬´(?!\S)", r"Responsibilities?", r"What you will do"]
H_QUAL = [r"ìê²©\s*ìš”ê±´", r"ì§€ì›\s*ìê²©", r"í•„ìˆ˜\s*ìš”ê±´", r"Requirements?", r"Qualifications?"]
H_PREF = [r"ìš°ëŒ€\s*ì‚¬í•­", r"ìš°ëŒ€\s*ì¡°ê±´", r"Preferred", r"Nice to have", r"Plus"]

HEADER_PATTERNS = [
    ("role", H_ROLE),
    ("resp", H_RESP),
    ("qual", H_QUAL),
    ("pref", H_PREF),
]

BULLET_RX = re.compile(r"^\s*(?:[-*â€¢Â·â–ªâ–¶]|[0-9]+\.)\s+")

def split_lines(text: str) -> List[str]:
    lines = [re.sub(r"\s+", " ", l).strip() for l in text.splitlines()]
    return [l for l in lines if l]

def pick_first(lines: List[str], patterns: List[str]) -> Optional[int]:
    for i, ln in enumerate(lines):
        s = ln.lower()
        for pat in patterns:
            if re.search(pat, s, re.I):
                return i
    return None

def extract_sections_from_text(text: str) -> Dict[str, List[str]]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ [role, resp, qual, pref] êµ¬ê°„ì„ ì°¾ì•„ ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬
    """
    lines = split_lines(text)
    idx = {}
    for key, pats in HEADER_PATTERNS:
        pos = pick_first(lines, pats)
        if pos is not None: idx[key] = pos

    if not idx:
        return {"role":[], "resp":[], "qual":[], "pref":[]}

    # ì„¹ì…˜ ê²½ê³„ ê³„ì‚°
    order = sorted(idx.items(), key=lambda x: x[1])
    bounds = []
    for i, (k, start) in enumerate(order):
        end = order[i+1][1] if i+1 < len(order) else len(lines)
        bounds.append((k, start, end))

    out = {"role":[], "resp":[], "qual":[], "pref":[]}
    for k, s, e in bounds:
        chunk = lines[s+1:e]   # í—¤ë” ë‹¤ìŒë¶€í„°
        bullets = []
        cur = ""
        for ln in chunk:
            if BULLET_RX.match(ln):
                if cur: bullets.append(cur.strip()); cur = ""
                bullets.append(BULLET_RX.sub("", ln).strip())
            else:
                # ë¬¸ì¥ì´ ê¸¸ë©´ ì´ì–´ë¶™ì´ê¸°
                if cur:
                    cur += " " + ln
                else:
                    cur = ln
        if cur: bullets.append(cur.strip())

        # ë„ˆë¬´ ì§§ì€ ë¼ì¸ì€ ì œê±°
        bullets = [b for b in bullets if len(b) > 3]
        out[k] = bullets[:20]
    return out

def extract_company_meta(soup: Optional[BeautifulSoup]) -> Dict[str, str]:
    meta = {"company_name":"", "company_intro": "", "job_title":""}
    if not soup: return meta

    # íšŒì‚¬ëª… í›„ë³´
    # og:site_name, application-name, title ë¶„ë¦¬
    cand = []
    og = soup.find("meta", {"property":"og:site_name"})
    if og and og.get("content"): cand.append(og["content"])
    app = soup.find("meta", {"name":"application-name"})
    if app and app.get("content"): cand.append(app["content"])
    if soup.title and soup.title.string: cand.append(soup.title.string)

    # ê°„ë‹¨ ì •ì œ
    cand = [re.split(r"[\-\|\Â·\â€”]", c)[0].strip() for c in cand if c]
    cand = [c for c in cand if 2 <= len(c) <= 40]
    meta["company_name"] = cand[0] if cand else ""

    # ì†Œê°œ = meta description ìš°ì„ 
    md = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
    if md and md.get("content"):
        intro = md["content"].strip()
        meta["company_intro"] = re.sub(r"\s+", " ", intro)[:500]

    # ì§ë¬´ëª… í›„ë³´: h1/h2/og:title
    jt = ""
    ogt = soup.find("meta", {"property":"og:title"})
    if ogt and ogt.get("content"): jt = ogt["content"]
    if not jt:
        h1 = soup.find("h1")
        if h1 and h1.get_text(): jt = h1.get_text(strip=True)
    if not jt:
        h2 = soup.find("h2")
        if h2 and h2.get_text(): jt = h2.get_text(strip=True)

    jt = re.sub(r"\s+", " ", jt).strip()
    meta["job_title"] = jt[:120]
    return meta

# -----------------------------
# UI â€” Direct URL mode only
# -----------------------------
st.header("1) ì±„ìš© ê³µê³  URL ì…ë ¥")
url = st.text_input("ì±„ìš© ê³µê³  ìƒì„¸ URL", placeholder="ì˜ˆ: https://www.wanted.co.kr/wd/123456")

col_btn = st.columns(2)
with col_btn[0]:
    run = st.button("ì›ë¬¸ ê°€ì ¸ì˜¤ê¸°", type="primary")
with col_btn[1]:
    show_raw = st.checkbox("ì›ë¬¸ ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ", value=True)

if run:
    if not url.strip():
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ì›ë¬¸ ìˆ˜ì§‘ ë° êµ¬ì¡°í™” ì¤‘..."):
            text_all, meta, soup = fetch_all_text(url.strip())
            company_meta = extract_company_meta(soup)
            sections = extract_sections_from_text(text_all)

        # ë””ë²„ê·¸
        with st.expander("ë””ë²„ê·¸: ì›ë¬¸ ìˆ˜ì§‘ ìƒíƒœ/ë©”íƒ€"):
            st.json({"fetch_meta": meta, "company_meta": company_meta})
            st.write(f"ì›ë¬¸ ê¸¸ì´: {len(text_all)}")

        # ë ˆì´ì•„ì›ƒ
        st.header("2) íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´ (êµ¬ì¡°í™” ì¶œë ¥)")

        st.markdown("### íšŒì‚¬ëª…")
        st.write(company_meta.get("company_name") or "N/A")

        st.markdown("### ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ(ìš”ì•½)")
        st.write(company_meta.get("company_intro") or "ë©”íƒ€ ì„¤ëª…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        st.markdown("### ëª¨ì§‘ ë¶„ì•¼(ì§ë¬´ëª…)")
        job_title = company_meta.get("job_title") or (sections["role"][0] if sections["role"] else "")
        st.write(job_title if job_title else "ë³¸ë¬¸ì—ì„œ ì§ë¬´ëª…ì„ í™•ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        st.markdown("### ì£¼ìš” ì—…ë¬´")
        resp = sections.get("resp") or []
        if resp:
            for b in resp: st.markdown(f"- {b}")
        else:
            st.write("ë³¸ë¬¸ì—ì„œ 'ì£¼ìš” ì—…ë¬´' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        st.markdown("### ìê²© ìš”ê±´")
        qual = sections.get("qual") or []
        if qual:
            for b in qual: st.markdown(f"- {b}")
        else:
            st.write("ë³¸ë¬¸ì—ì„œ 'ìê²© ìš”ê±´' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        st.markdown("### ìš°ëŒ€ ì‚¬í•­")
        pref = sections.get("pref") or []
        if pref:
            for b in pref: st.markdown(f"- {b}")
        else:
            st.write("ë³¸ë¬¸ì—ì„œ 'ìš°ëŒ€ ì‚¬í•­' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # ì›ë¬¸ ë³´ê¸°/ë‹¤ìš´ë¡œë“œ
        if show_raw:
            st.divider()
            st.subheader("ì›ë¬¸ í…ìŠ¤íŠ¸(ì „ì²´)")
            st.text_area("ì›ë¬¸", value=text_all[:30000], height=300)
        st.download_button(
            "ì›ë¬¸ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
            data=text_all.encode("utf-8"),
            file_name="job_posting_raw.txt",
            mime="text/plain",
        )

        # JSON ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
        result = {
            "source_url": meta.get("url_final"),
            "fetch_source": meta.get("source"),
            "company_name": company_meta.get("company_name"),
            "company_intro": company_meta.get("company_intro"),
            "job_title": job_title,
            "responsibilities": resp,
            "qualifications": qual,
            "preferences": pref,
        }
        st.download_button(
            "êµ¬ì¡°í™” ê²°ê³¼(JSON) ë‹¤ìš´ë¡œë“œ",
            data=json.dumps(result, ensure_ascii=False, indent=2).encode("utf-8"),
            file_name="job_posting_structured.json",
            mime="application/json",
        )

# ì•ˆë‚´
st.info(
    "âš ï¸ ë™ì  ë Œë”ë§/ë¡œê·¸ì¸/ë´‡ì°¨ë‹¨ í˜ì´ì§€ëŠ” ì¼ë¶€ ëˆ„ë½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
    "- ìš°ì„ ìˆœìœ„: **Jina Reader â†’ ì •ì  HTML â†’ BS4**\n"
    "- ì„¹ì…˜ í—¤ë” í‚¤ì›Œë“œ: ì£¼ìš”ì—…ë¬´/ë‹´ë‹¹ì—…ë¬´, ìê²©ìš”ê±´/ì§€ì›ìê²©, ìš°ëŒ€ì‚¬í•­/Preferred ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ìë™ ë¶„ë¦¬í•©ë‹ˆë‹¤."
)
