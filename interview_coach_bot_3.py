# -*- coding: utf-8 -*-
# íŒŒì¼ëª… ì˜ˆ: interview_job_fulltext_app.py

import os, re, io, json, html, textwrap, urllib.parse
from typing import Tuple, Dict, List, Optional

import streamlit as st
import requests
from bs4 import BeautifulSoup

# ì„ íƒ ì˜ì¡´ì„± (ì—†ì–´ë„ ë™ì‘)
try:
    from langchain_community.document_loaders import WebBaseLoader
    LC_OK = True
except Exception:
    LC_OK = False

# =========================
# Page / Secrets
# =========================
st.set_page_config(page_title="íšŒì‚¬ ìš”ì•½ Â· ì±„ìš© ìš”ê±´ ì›ë¬¸ ìˆ˜ì§‘/ìš”ì•½", page_icon="ğŸ§²", layout="wide")

def _get(key: str) -> Optional[str]:
    v = os.getenv(key)
    if v: return v
    try:
        return st.secrets.get(key, None)
    except Exception:
        return None

OPENAI_API_KEY = _get("OPENAI_API_KEY")
NAVER_ID        = _get("NAVER_CLIENT_ID")
NAVER_SECRET    = _get("NAVER_CLIENT_SECRET")

# =========================
# Utils
# =========================
def _clean(s: str) -> str:
    if not s: return ""
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def _abs(url: str) -> str:
    u = url.strip()
    if not u.startswith("http"): u = "https://" + u
    return u

# =========================
# Portal-specific collectors
# =========================
def wanted_full_text(url: str) -> Tuple[str, Dict]:
    """ì›í‹°ë“œ ìƒì„¸(/wd/<id>)ì˜ í¼ì³ì§„ ë³¸ë¬¸ì„ JSON APIë¡œ ìˆ˜ì§‘."""
    m = re.search(r"/wd/(\d+)", url)
    if not m:
        return "", {"wanted": "no_id"}

    jid = m.group(1)
    endpoints = [
        f"https://www.wanted.co.kr/api/v4/jobs/{jid}?locale=ko-KR",
        f"https://www.wanted.co.kr/api/v2/jobs/{jid}?locale=ko-KR",
        f"https://www.wanted.co.kr/api/v4/jobs/{jid}",
        f"https://www.wanted.co.kr/api/v2/jobs/{jid}",
    ]
    hdr = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json, text/plain, */*",
        "Referer": url,
        "X-Wanted-Language": "ko-KR",
        "Accept-Language": "ko-KR,ko;q=0.9",
    }
    for ep in endpoints:
        try:
            r = requests.get(ep, headers=hdr, timeout=10)
            if r.status_code != 200 or "application/json" not in r.headers.get("content-type",""):
                continue
            data = r.json()
            texts = []
            keys = [
                "detail","description","intro","qualification","prefer","requirements",
                "mainTasks","responsibility","responsibilities","summary","content",
                "job_detail","jobDescription"
            ]
            def walk(obj):
                if isinstance(obj, dict):
                    for k,v in obj.items():
                        if isinstance(v,(dict,list)):
                            walk(v)
                        else:
                            if isinstance(k,str) and isinstance(v,str):
                                ks = k.lower()
                                if any(sub in ks for sub in keys):
                                    s = _clean(v)
                                    if len(s)>3: texts.append(s)
                elif isinstance(obj, list):
                    for it in obj: walk(it)
            walk(data)

            if not texts:
                blob = json.dumps(data, ensure_ascii=False)
                cand = re.findall(r'["\'](?:detail|description|qualification|prefer|requirements)["\']\s*:\s*"(.*?)"', blob, flags=re.S)
                texts += [_clean(x) for x in cand]

            if texts:
                return "\n\n".join(dict.fromkeys(texts)), {"source":"wanted+json","url_final":url}
        except Exception:
            continue
    return "", {"wanted":"fail"}

def saramin_full_text(url: str) -> Tuple[str, Dict]:
    """ì‚¬ëŒì¸ ìƒì„¸(SSR + ì¼ë¶€ ì ‘í˜). ëŒ€í‘œ ì»¨í…Œì´ë„ˆ ëª¨ì•„ ì›ë¬¸ ì¡°ë¦½."""
    if "saramin.co.kr" not in url:
        return "", {"saramin":"skip"}
    try:
        r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        if r.status_code != 200: return "", {"saramin":"http_"+str(r.status_code)}
        soup = BeautifulSoup(r.text, "html.parser")
        sections = []
        for sel in ["#job_summary",".user_content",".wrap_jview",".cont",".content"]:
            for c in soup.select(sel):
                txt=_clean(c.get_text(" "))
                if len(txt)>50: sections.append(txt)
        if not sections:
            for h in soup.select("h2,h3,h4"):
                title=_clean(h.get_text(" "))
                if not title: continue
                buf=[title]
                sib=h.find_next_sibling()
                stop={"h2","h3","h4"}
                while sib and getattr(sib,"name",None) not in stop:
                    if getattr(sib,"name","") in {"p","ul","ol","li","div","section"}:
                        s=_clean(sib.get_text(" "))
                        if len(s)>2: buf.append(s)
                    sib=getattr(sib,"next_sibling",None)
                if len(" ".join(buf))>50: sections.append("\n".join(buf))
        if sections:
            return "\n\n".join(dict.fromkeys(sections)), {"source":"saramin+raw","url_final":url}
    except Exception:
        pass
    return "", {"saramin":"fail"}

def jobkorea_full_text(url: str) -> Tuple[str, Dict]:
    """ì¡ì½”ë¦¬ì•„ ìƒì„¸(SSR). ëŒ€í‘œ ì»¨í…Œì´ë„ˆ ìˆ˜ì§‘."""
    if "jobkorea.co.kr" not in url:
        return "", {"jobkorea":"skip"}
    try:
        r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        if r.status_code != 200: return "", {"jobkorea":"http_"+str(r.status_code)}
        soup = BeautifulSoup(r.text, "html.parser")
        sections = []
        for sel in [".recruit-info",".detail",".tbCompanyInfo",".readSum",".section",".devView"]:
            for c in soup.select(sel):
                txt=_clean(c.get_text(" "))
                if len(txt)>50: sections.append(txt)
        if sections:
            return "\n\n".join(dict.fromkeys(sections)), {"source":"jobkorea+raw","url_final":url}
    except Exception:
        pass
    return "", {"jobkorea":"fail"}

# =========================
# Generic loaders (Jina â†’ WebBase â†’ BS4)
# =========================
def get_full_page_text(url: str) -> Tuple[str, Dict]:
    u = _abs(url)
    meta = {"url_final": u}

    # 1) í¬í„¸ ì „ìš©
    if "wanted.co.kr/wd/" in u:
        t, _ = wanted_full_text(u)
        if t:
            meta.update({"source":"wanted+raw","lens":{"jina":0,"webbase":len(t),"bs4":len(t)}})
            return t, meta
    if "saramin.co.kr" in u:
        t, _ = saramin_full_text(u)
        if t:
            meta.update({"source":"saramin+raw","lens":{"jina":0,"webbase":len(t),"bs4":len(t)}})
            return t, meta
    if "jobkorea.co.kr" in u:
        t, _ = jobkorea_full_text(u)
        if t:
            meta.update({"source":"jobkorea+raw","lens":{"jina":0,"webbase":len(t),"bs4":len(t)}})
            return t, meta

    # 2) Jina í”„ë¦¬ë Œë”
    try:
        ep = "https://r.jina.ai/http://" + u.replace("https://","").replace("http://","")
        r = requests.get(ep, headers={"User-Agent":"Mozilla/5.0"}, timeout=12)
        if r.status_code == 200 and len(r.text.strip())>200:
            t = _clean(r.text)
            meta.update({"source":"jina","lens":{"jina":len(t)}})
            return t, meta
    except Exception:
        pass

    # 3) WebBaseLoader
    if LC_OK:
        try:
            docs = WebBaseLoader(u).load()
            txt = "\n\n".join(d.page_content for d in docs)
            if len(txt.strip())>50:
                meta.update({"source":"webbase","lens":{"webbase":len(txt)}})
                return _clean(txt), meta
        except Exception:
            pass

    # 4) BS4
    try:
        r = requests.get(u, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        if r.status_code == 200 and "text/html" in r.headers.get("content-type",""):
            soup = BeautifulSoup(r.text, "html.parser")
            for tag in soup(["script","style","noscript"]): tag.extract()
            txt = _clean(soup.get_text(" "))
            if len(txt)>50:
                meta.update({"source":"bs4","lens":{"bs4":len(txt)}})
                return txt, meta
    except Exception:
        pass

    meta.update({"source":"none","lens":{"jina":0,"webbase":0,"bs4":0}})
    return "", meta

# =========================
# Search (Naver â†’ DuckDuckGo)
# =========================
JOB_SITES = ["wanted.co.kr","saramin.co.kr","jobkorea.co.kr","rocketpunch.com","linkedin.com","indeed.com"]

def naver_search_web(query: str, display: int = 5) -> List[str]:
    if not (NAVER_ID and NAVER_SECRET): return []
    url = "https://openapi.naver.com/v1/search/webkr.json"
    headers = {"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET}
    params = {"query": query, "display": display, "sort": "date"}
    try:
        r = requests.get(url, headers=headers, params=params, timeout=6)
        if r.status_code!=200: return []
        js = r.json()
        out=[]
        for it in js.get("items",[]):
            link = it.get("link")
            if link and link not in out: out.append(link)
        return out
    except Exception:
        return []

def duckduck_search(query: str, display: int = 10) -> List[str]:
    url = f"https://duckduckgo.com/html/?q={urllib.parse.quote(query)}"
    try:
        r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=8)
        if r.status_code!=200: return []
        soup = BeautifulSoup(r.text, "html.parser")
        out=[]
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if href.startswith("/l/?kh=-1&uddg="):
                href = urllib.parse.unquote(href.split("/l/?kh=-1&uddg=")[-1])
            dom = urllib.parse.urlparse(href).netloc.lower()
            if any(s in dom for s in JOB_SITES):
                out.append(href)
            if len(out)>=display: break
        return out
    except Exception:
        return []

def discover_job_url(company: str, role: str, limit: int = 6) -> List[str]:
    q1 = f"{company} {role} ì±„ìš©"
    site_part = " OR ".join([f"site:{s}" for s in JOB_SITES])
    q2 = f"{company} {role} ({site_part})"
    urls=[]
    if NAVER_ID and NAVER_SECRET:
        urls += naver_search_web(q1, display=6)
        urls += naver_search_web(q2, display=6)
    if not urls:
        urls += duckduck_search(q2, display=10)
    seen=set(); out=[]
    for u in urls:
        try:
            d = urllib.parse.urlparse(u).netloc.lower()
            if any(s in d for s in JOB_SITES):
                if u not in seen:
                    seen.add(u); out.append(u)
        except Exception: pass
        if len(out)>=limit: break
    return out

# =========================
# OpenAI summarizer (optional)
# =========================
OPENAI_READY = False
if OPENAI_API_KEY:
    try:
        from openai import OpenAI
        client = OpenAI(api_key=OPENAI_API_KEY, timeout=40.0)
        OPENAI_READY = True
    except Exception:
        OPENAI_READY = False

def llm_summarize_sections(raw_text: str, company: str) -> Dict[str, List[str] | str]:
    """ì›ë¬¸ì—ì„œ íšŒì‚¬ì†Œê°œ/ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´/ìš°ëŒ€ì‚¬í•­ì„ ìš”ì•½ìœ¼ë¡œ ìƒì„±."""
    if not (OPENAI_READY and raw_text.strip()):
        return {"intro":"", "resp":[], "qual":[], "pref":[]}
    sys = ("ë„ˆëŠ” ì±„ìš©ê³µê³ ë¥¼ ì½ê³  ì„¹ì…˜ë³„ í•µì‹¬ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ëŠ” ë„ìš°ë¯¸ë‹¤. "
           "ê°€ëŠ¥í•˜ë©´ ê³µê³ ì˜ ë¬¸êµ¬ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ì§€ ë§ê³  ê°„ê²°íˆ ì¬ì„œìˆ í•˜ë˜, ì˜ë¯¸ëŠ” ìœ ì§€í•œë‹¤.")
    user = f"""
[íšŒì‚¬ëª…] {company}

[ì±„ìš©ê³µê³  ì›ë¬¸]
{raw_text[:12000]}

[ìš”ì²­]
1) ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ: 2~3ë¬¸ì¥
2) ì£¼ìš” ì—…ë¬´: ë¶ˆë¦¿ 5~8ê°œ
3) ìê²© ìš”ê±´: ë¶ˆë¦¿ 5~8ê°œ
4) ìš°ëŒ€ ì‚¬í•­: ë¶ˆë¦¿ 5~8ê°œ
JSONìœ¼ë¡œë§Œ ë‹µí•˜ë¼. í‚¤ëŠ” intro(resp/qual/pref)ì´ë©° resp/qual/prefëŠ” ë¦¬ìŠ¤íŠ¸.
"""
    try:
        r = client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.2,
            messages=[{"role":"system","content":sys},{"role":"user","content":user}]
        )
        content = r.choices[0].message.content.strip()
        js = json.loads(content)
        intro = _clean(js.get("intro",""))
        resp  = [_clean(x) for x in js.get("resp",[]) if _clean(x)]
        qual  = [_clean(x) for x in js.get("qual",[]) if _clean(x)]
        pref  = [_clean(x) for x in js.get("pref",[]) if _clean(x)]
        return {"intro":intro,"resp":resp[:12],"qual":qual[:12],"pref":pref[:12]}
    except Exception:
        return {"intro":"","resp":[],"qual":[],"pref":[]}

# =========================
# UI
# =========================
st.title("ğŸ§² íšŒì‚¬ ìš”ì•½ Â· ì±„ìš© ìš”ê±´ (ì›ë¬¸ ìˆ˜ì§‘ + ìš”ì•½)")

with st.sidebar:
    st.header("ì…ë ¥")
    company = st.text_input("íšŒì‚¬ëª…", placeholder="ì˜ˆ: ë§ˆì´ë¦¬ì–¼íŠ¸ë¦½ / í™”í•´ / ì¹´ì¹´ì˜¤ë±…í¬")
    role    = st.text_input("ì§€ì› ì§ë¬´ëª…", placeholder="ì˜ˆ: Data Analyst / ML Engineer")
    job_url = st.text_input("ì±„ìš© ê³µê³  URL(ì„ íƒ)", placeholder="ìƒì„¸ URLì„ ëª¨ë¥´ë©´ ë¹„ì›Œë‘ì„¸ìš”")
    st.caption("URLì´ ì—†ìœ¼ë©´ ê²€ìƒ‰â†’í›„ë³´ ì¤‘ ì²« ë²ˆì§¸ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
    btn_go  = st.button("íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°", type="primary", use_container_width=True)

# ì„¸ì…˜ ìƒíƒœ
if "raw_job_text" not in st.session_state:
    st.session_state.raw_job_text = ""
if "job_url_final" not in st.session_state:
    st.session_state.job_url_final = ""
if "meta_collect" not in st.session_state:
    st.session_state.meta_collect = {}

# ì‹¤í–‰
if btn_go:
    urls = [job_url] if job_url.strip() else discover_job_url(company, role, limit=6)
    chosen = None
    for u in urls:
        if not u: continue
        chosen = u; break
    if not chosen:
        st.warning("ê³µê³  URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. URLì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì›ë¬¸ ìˆ˜ì§‘ ì¤‘â€¦(í¬í„¸ ì „ìš© â†’ Jina â†’ WebBase â†’ BS4)"):
            txt, meta = get_full_page_text(chosen)
            st.session_state.raw_job_text  = txt
            st.session_state.job_url_final = meta.get("url_final") or chosen
            st.session_state.meta_collect  = meta
        if not st.session_state.raw_job_text:
            st.warning("ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ì¸/ë´‡ì°¨ë‹¨/ë™ì  ë Œë”ë§ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.success("ì›ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ!")

# í‘œì‹œ: ì›ë¬¸ ì „ì²´
if st.session_state.raw_job_text:
    st.info("ì•„ë˜ëŠ” ì±„ìš© ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ **ì›ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸**ì…ë‹ˆë‹¤. (ì ‘í˜ í¬í•¨, ê°€ëŠ¥í•œ í•œ ëª¨ë‘)")
else:
    st.warning("ì›ë¬¸ì´ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ íšŒì‚¬/ì§ë¬´ë¥¼ ì…ë ¥í•˜ê³  ë¶ˆëŸ¬ì˜¤ê¸°ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

c1, c2 = st.columns(2)
with c1:
    st.subheader("íšŒì‚¬ ìš”ì•½ (ì›ë¬¸ ì „ì²´)")
    st.text_area("íšŒì‚¬ ìš”ì•½ ì›ë¬¸", value=st.session_state.raw_job_text, height=420)
    st.download_button("íšŒì‚¬ ìš”ì•½ ì›ë¬¸ ë‹¤ìš´ë¡œë“œ",
                       data=st.session_state.raw_job_text.encode("utf-8-sig"),
                       file_name="company_fulltext.txt", use_container_width=True)
with c2:
    st.subheader("ì±„ìš© ìš”ê±´ (ì›ë¬¸ ì „ì²´)")
    st.text_area("ì±„ìš© ìš”ê±´ ì›ë¬¸", value=st.session_state.raw_job_text, height=420)
    st.download_button("ì±„ìš© ìš”ê±´ ì›ë¬¸ ë‹¤ìš´ë¡œë“œ",
                       data=st.session_state.raw_job_text.encode("utf-8-sig"),
                       file_name="job_requirements_fulltext.txt", use_container_width=True)

# í‘œì‹œ: ìš”ì•½ ì„¹ì…˜
st.divider()
st.subheader("ìš”ì•½ ì„¹ì…˜ (íšŒì‚¬ ì†Œê°œ / ì£¼ìš”ì—…ë¬´ / ìê²©ìš”ê±´ / ìš°ëŒ€ì‚¬í•­)")

if st.session_state.raw_job_text and OPENAI_READY:
    with st.spinner("ìš”ì•½ ìƒì„± ì¤‘â€¦"):
        summ = llm_summarize_sections(st.session_state.raw_job_text, company or "")
    intro = summ.get("intro","")
    resp  = summ.get("resp",[])
    qual  = summ.get("qual",[])
    pref  = summ.get("pref",[])
    st.markdown(f"**íšŒì‚¬ëª…:** {company or '-'}")
    if intro: st.markdown(f"**ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ(ìš”ì•½)**\n\n{intro}")
    cc1, cc2, cc3 = st.columns(3)
    with cc1:
        st.markdown("**ì£¼ìš”ì—…ë¬´(ìš”ì•½)**")
        if resp: st.markdown("\n".join([f"- {x}" for x in resp]))
        else: st.caption("ìš”ì•½ ê°€ëŠ¥í•œ ì£¼ìš”ì—…ë¬´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    with cc2:
        st.markdown("**ìê²©ìš”ê±´(ìš”ì•½)**")
        if qual: st.markdown("\n".join([f"- {x}" for x in qual]))
        else: st.caption("ìš”ì•½ ê°€ëŠ¥í•œ ìê²©ìš”ê±´ì´ ì—†ìŠµë‹ˆë‹¤.")
    with cc3:
        st.markdown("**ìš°ëŒ€ì‚¬í•­(ìš”ì•½)**")
        if pref: st.markdown("\n".join([f"- {x}" for x in pref]))
        else: st.caption("ìš”ì•½ ê°€ëŠ¥í•œ ìš°ëŒ€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    if not OPENAI_READY:
        st.info("ìš”ì•½ ì„¹ì…˜ì„ ì‚¬ìš©í•˜ë ¤ë©´ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        st.info("ë¨¼ì € ì›ë¬¸ì„ ìˆ˜ì§‘í•˜ì„¸ìš”.")

# ë””ë²„ê·¸: ê²½ë¡œ/ìƒíƒœ
st.divider()
with st.expander("ë””ë²„ê·¸: ì›ë¬¸ ìˆ˜ì§‘ ê²½ë¡œ/ìƒíƒœ"):
    st.write({
        "url_final": st.session_state.get("job_url_final",""),
        "source":    st.session_state.get("meta_collect",{}).get("source",""),
        "lens":      st.session_state.get("meta_collect",{}).get("lens",{}),
    })
    st.caption("sourceê°€ wanted+raw/saramin+raw/jobkorea+rawë©´ í¬í„¸ ì „ìš© ìˆ˜ì§‘ê¸°ê°€ ë™ì‘í•˜ì—¬ ì ‘íŒ ë³¸ë¬¸ê¹Œì§€ í¬í•¨í•©ë‹ˆë‹¤.")

# ìê°€ì§„ë‹¨: ì§ì ‘ URL í…ŒìŠ¤íŠ¸
st.divider()
with st.expander("ğŸ§ª ì›ë¬¸ í…ŒìŠ¤íŠ¸(ì§ì ‘ URL)"):
    test_url = st.text_input("í…ŒìŠ¤íŠ¸í•  ì±„ìš© ìƒì„¸ URLì„ ì…ë ¥í•˜ì„¸ìš”", key="test_url")
    if st.button("í…ŒìŠ¤íŠ¸ ì‹¤í–‰"):
        if not test_url.strip():
            st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            txt, meta = get_full_page_text(test_url.strip())
            st.write(meta)
            st.write(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(txt)}")
            st.text_area("ë¯¸ë¦¬ë³´ê¸°(ì• 3000ì)", value=txt[:3000], height=300)

st.caption("Tip) URLì´ ì—†ìœ¼ë©´ ê²€ìƒ‰ í›„ë³´ê°€ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ëŠ¥í•˜ë©´ ìƒì„¸ ê³µê³  URLì„ ì§ì ‘ ë„£ì–´ì£¼ì„¸ìš”.")
