###############################################################################################
# Job_Helper_Bot (ì •ë°€ í¬ë¡¤ëŸ¬ í™•ì¥íŒ)
#  - ì±„ìš© ê³µê³  URL â†’ íšŒì‚¬ ìš”ì•½(ì •ê·œí™”) â†’ ì´ë ¥ì„œ ì—…ë¡œë“œ/ì¸ë±ì‹± â†’ ìì†Œì„œ ìƒì„±
#  - Wanted / Saramin / JobKorea ì‚¬ì´íŠ¸ë³„ ë§ì¶¤ íŒŒì„œ(ì •ë°€ í¬ë¡¤ëŸ¬) ì¶”ê°€
#
# ë³€ê²½ ìš”ì•½
#   1) parse_portal_specific(url, soup, raw_text) ë””ìŠ¤íŒ¨ì²˜
#   2) parse_wanted / parse_saramin / parse_jobkorea ê·œì¹™ ì¶”ì¶œ
#   3) ê·œì¹™ ê²°ê³¼ ìš°ì„  ì‚¬ìš©, ë¶€ì¡± ì‹œ LLM ì •ì œë¡œ ë³´ì™„
###############################################################################################

# -*- coding: utf-8 -*-
import os, re, json, urllib.parse, random, time, io
from typing import Optional, Tuple, Dict, List

import requests
from bs4 import BeautifulSoup
import html2text
import streamlit as st
import pandas as pd
import numpy as np

# ================== ê¸°ë³¸ ì„¤ì • ==================
st.set_page_config(page_title="Job_Helper_Bot (ìì†Œì„œ ìƒì„±)", page_icon="ğŸ“‘", layout="wide")
st.title("Job_Helper_Bot : ì±„ìš© ê³µê³  URL â†’ íšŒì‚¬ ìš”ì•½ â†’ ì´ë ¥ì„œ ë“±ë¡ â†’ ìì†Œì„œ ìƒì„±")

# ================== OpenAI ==================
try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()

API_KEY = os.getenv("OPENAI_API_KEY") or (st.secrets.get("OPENAI_API_KEY", None) if hasattr(st,"secrets") else None)
if not API_KEY:
    API_KEY = st.text_input("OPENAI_API_KEY ì…ë ¥", type="password")
if not API_KEY:
    st.stop()
client = OpenAI(api_key=API_KEY)

with st.sidebar:
    st.subheader("ëª¨ë¸ ì„¤ì •")
    CHAT_MODEL = st.selectbox("ëŒ€í™”/ìƒì„± ëª¨ë¸", ["gpt-4o-mini","gpt-4o"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸(ë‚´ë¶€ìš©)", ["text-embedding-3-small","text-embedding-3-large"], index=0)

# ================== HTTP ìœ í‹¸ ==================
def normalize_url(u: str) -> Optional[str]:
    if not u: return None
    u = u.strip()
    if not re.match(r"^https?://", u): u = "https://" + u
    parts = urllib.parse.urlsplit(u)
    return urllib.parse.urlunsplit((parts.scheme, parts.netloc, parts.path, parts.query, ""))

def http_get(url: str, timeout: int = 12) -> Optional[requests.Response]:
    try:
        r = requests.get(
            url,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36",
                "Accept-Language": "ko, en;q=0.9",
            },
            timeout=timeout,
        )
        if r.status_code == 200 and "text/html" in r.headers.get("content-type",""):
            return r
    except Exception:
        pass
    return None

# ================== ì›ë¬¸ ìˆ˜ì§‘ (Jina â†’ Web â†’ BS4) ==================
def fetch_jina_text(url: str, timeout: int = 15) -> str:
    """Jina reader í”„ë¡ì‹œë¥¼ í†µí•´ ì •ì  í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  í™•ë³´ (ë™ì  ë¡œë”© íšŒí”¼ìš©)"""
    try:
        parts = urllib.parse.urlsplit(url)
        prox = f"https://r.jina.ai/http://{parts.netloc}{parts.path}"
        if parts.query: prox += f"?{parts.query}"
        r = http_get(prox, timeout=timeout)
        return r.text.strip() if r else ""
    except Exception:
        return ""

def html_to_text(html_str: str) -> str:
    conv = html2text.HTML2Text()
    conv.ignore_links = True
    conv.ignore_images = True
    conv.body_width = 0
    txt = conv.handle(html_str)
    txt = re.sub(r"\n{3,}", "\n\n", txt)
    return txt.strip()

def fetch_webbase_text(url: str) -> str:
    r = http_get(url, timeout=12)
    if not r: return ""
    return html_to_text(r.text)

def fetch_bs4_text(url: str) -> Tuple[str, Optional[BeautifulSoup]]:
    r = http_get(url, timeout=12)
    if not r: return "", None
    soup = BeautifulSoup(r.text, "lxml")
    # í° ë¸”ë¡ì„ ìš°ì„  í•©ì¹˜ëŠ” ì „ëµ
    blocks = []
    for sel in ["article","section","main","div","ul","ol"]:
        for el in soup.select(sel):
            txt = el.get_text(" ", strip=True)
            if txt and len(txt) > 300:
                txt = re.sub(r"\s+"," ", txt)
                blocks.append(txt)
    if not blocks:
        return soup.get_text(" ", strip=True)[:120000], soup
    seen, out = set(), []
    for b in blocks:
        if b not in seen:
            seen.add(b); out.append(b)
    return ("\n\n".join(out)[:120000], soup)

def fetch_all_text(url: str):
    url = normalize_url(url)
    if not url: return "", {"error":"invalid_url"}, None
    jina = fetch_jina_text(url)
    if jina:
        _, soup = fetch_bs4_text(url)
        return jina, {"source":"jina","len":len(jina),"url_final":url}, soup
    web = fetch_webbase_text(url)
    if web:
        _, soup = fetch_bs4_text(url)
        return web, {"source":"webbase","len":len(web),"url_final":url}, soup
    bs, soup = fetch_bs4_text(url)
    return bs, {"source":"bs4","len":len(bs),"url_final":url}, soup

# ================== ê³µí†µ ìœ í‹¸ (í´ë¦¬ë‹/ì„¹ì…˜ ìˆ˜ì§‘) ==================
PREF_KW = re.compile(r"(ìš°ëŒ€|ì„ í˜¸|preferred|plus|ê°€ì‚°ì |ìˆìœ¼ë©´\s*ì¢‹ìŒ)", re.I)
RESP_HDR = re.compile(r"(ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|Role|Responsibilities?)", re.I)
QUAL_HDR = re.compile(r"(ìê²©\s*ìš”ê±´|ì§€ì›\s*ìê²©|Requirements?|Qualifications?)", re.I)
PREF_HDR = re.compile(r"(ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€|Preferred|Nice\s*to\s*have|Plus)", re.I)

def _clean_line(s: str) -> str:
    s = re.sub(r"\s+"," ", s or "").strip(" -â€¢Â·â–¶â–ªï¸").strip()
    return s[:180]

def _push_unique(bucket: List[str], line: str, seen: set):
    line = _clean_line(line)
    if line and line not in seen:
        seen.add(line); bucket.append(line)

def extract_company_meta(soup: Optional[BeautifulSoup]) -> Dict[str,str]:
    """í˜ì´ì§€ ë©”íƒ€ì—ì„œ íšŒì‚¬ëª…/ì†Œê°œ/ì§ë¬´ëª… íŒíŠ¸ ì¶”ì¶œ"""
    meta = {"company_name":"","company_intro":"","job_title":""}
    if not soup: return meta
    cand = []
    og = soup.find("meta", {"property":"og:site_name"})
    if og and og.get("content"): cand.append(og["content"])
    app = soup.find("meta", {"name":"application-name"})
    if app and app.get("content"): cand.append(app["content"])
    if soup.title and soup.title.string: cand.append(soup.title.string)
    cand = [re.split(r"[\-\|\Â·\â€”]", c)[0].strip() for c in cand if c]
    cand = [c for c in cand if 2 <= len(c) <= 40]
    meta["company_name"] = cand[0] if cand else ""

    md = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
    if md and md.get("content"):
        meta["company_intro"] = re.sub(r"\s+"," ", md["content"]).strip()[:500]

    jt = ""
    ogt = soup.find("meta", {"property":"og:title"})
    if ogt and ogt.get("content"): jt = ogt["content"]
    if not jt:
        h1 = soup.find("h1")
        if h1 and h1.get_text(): jt = h1.get_text(strip=True)
    if not jt:
        h2 = soup.find("h2")
        if h2 and h2.get_text(): jt = h2.get_text(strip=True)
    meta["job_title"] = re.sub(r"\s+"," ", jt).strip()[:120]
    return meta

def collect_after_heading(soup: BeautifulSoup, head_regex: re.Pattern, limit: int = 12) -> List[str]:
    """
    h1~h4 ì¤‘ ì œëª©ì— ì •ê·œì‹ì´ ë§¤ì¹­ë˜ëŠ” ìš”ì†Œë¥¼ ì°¾ê³ ,
    ê·¸ 'ë‹¤ìŒ í˜•ì œë“¤'ê³¼ 'ë°”ë¡œ í•˜ìœ„ ul/ol/li/p'ì—ì„œ ë¬¸ì¥ì„ ëª¨ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    """
    out, seen = [], set()
    heads = []
    for tag in soup.find_all(re.compile("^h[1-4]$")):
        if head_regex.search(tag.get_text(" ", strip=True) or ""):
            heads.append(tag)

    for h in heads:
        # í˜•ì œ ìˆœíšŒ
        sib = h.find_next_sibling()
        while sib and sib.name not in {"h1","h2","h3","h4"} and len(out) < limit:
            # ëª©ë¡/ë¬¸ë‹¨ ìš°ì„ 
            if sib.name in {"ul","ol"}:
                for li in sib.find_all("li", recursive=True):
                    _push_unique(out, li.get_text(" ", strip=True), seen)
                    if len(out) >= limit: break
            elif sib.name in {"p","div","section"}:
                txt = sib.get_text(" ", strip=True)
                if len(txt) > 4:
                    # í° ë©ì–´ë¦¬ë©´ ë¬¸ì¥ ë¶„ë¦¬
                    lines = re.split(r"[â€¢\-\nÂ·â–ªï¸â–¶]+|\s{2,}", txt)
                    for l in lines:
                        _push_unique(out, l, seen)
                        if len(out) >= limit: break
            sib = sib.find_next_sibling()
        if len(out) >= limit: break

        # ë°”ë¡œ í•˜ìœ„ ëª©ë¡/ë¬¸ë‹¨
        for sel in ["ul","ol","p","div","section"]:
            for el in h.find_all(sel, recursive=False):
                text = el.get_text(" ", strip=True)
                if sel in {"ul","ol"}:
                    for li in el.find_all("li", recursive=True):
                        _push_unique(out, li.get_text(" ", strip=True), seen)
                        if len(out) >= limit: break
                else:
                    lines = re.split(r"[â€¢\-\nÂ·â–ªï¸â–¶]+|\s{2,}", text)
                    for l in lines:
                        _push_unique(out, l, seen)
                        if len(out) >= limit: break
            if len(out) >= limit: break

    return out[:limit]

# ================== ì‚¬ì´íŠ¸ë³„ ì •ë°€ íŒŒì„œ ==================
def parse_wanted(soup: BeautifulSoup) -> Dict[str, List[str]]:
    """
    Wanted (wanted.co.kr) ì¶”ì¶œ ê·œì¹™:
      - h2/h3 ì œëª©ì— 'ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´/ìš°ëŒ€ì‚¬í•­'ì´ ìì£¼ ë…¸ì¶œ
      - data-cy ì†ì„±ì´ë‚˜ ë™ì  í´ë˜ìŠ¤ëŠ” ë³€ë™ ê°€ëŠ¥ â†’ ì œëª©ê¸°ë°˜ ìˆ˜ì§‘
    """
    res  = collect_after_heading(soup, RESP_HDR, limit=16)
    qual = collect_after_heading(soup, QUAL_HDR, limit=16)
    pref = collect_after_heading(soup, PREF_HDR, limit=16)

    # ë³´ì •: ìê²©ìš”ê±´ ë‚´ ìš°ëŒ€ í‚¤ì›Œë“œê°€ ì„ì˜€ìœ¼ë©´ ë¶„ë¦¬
    remain = []
    for q in qual:
        if PREF_KW.search(q): pref.append(q)
        else: remain.append(q)
    qual = remain

    return {
        "responsibilities": res[:12],
        "qualifications":   qual[:12],
        "preferences":      pref[:12],
    }

def parse_saramin(soup: BeautifulSoup) -> Dict[str, List[str]]:
    """
    Saramin (saramin.co.kr) ì¶”ì¶œ ê·œì¹™:
      - ìƒì„¸ë³¸ë¬¸ì— í‘œ/ì •ì˜ëª©ë¡(dl/dt/dd) ë˜ëŠ” h2/h3 ì„¹ì…˜+ul/liê°€ í˜¼ìš©
      - í—¤ë”© í…ìŠ¤íŠ¸ ê¸°ë°˜ + ì •ì˜ëª©ë¡ì˜ ì œëª© ë§¤ì¹­ ì‹œ dd í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    """
    out = {"responsibilities": [], "qualifications": [], "preferences": []}
    # 1) í‘œì¤€ í—¤ë” ê¸°ë°˜
    out["responsibilities"] += collect_after_heading(soup, RESP_HDR, 16)
    out["qualifications"]   += collect_after_heading(soup, QUAL_HDR, 16)
    out["preferences"]      += collect_after_heading(soup, PREF_HDR, 16)

    # 2) dl/dt/dd êµ¬ì¡° ë³´ì¡°
    for dl in soup.find_all("dl"):
        for dt in dl.find_all("dt", recursive=False):
            title = (dt.get_text(" ", strip=True) or "")
            dd = dt.find_next_sibling("dd")
            if not dd: continue
            text = dd.get_text(" ", strip=True)
            if not text: continue
            lines = re.split(r"[â€¢\-\nÂ·â–ªï¸â–¶]+|\s{2,}", text)
            if RESP_HDR.search(title):
                for l in lines: _push_unique(out["responsibilities"], l, set(out["responsibilities"]))
            elif QUAL_HDR.search(title):
                for l in lines: _push_unique(out["qualifications"], l, set(out["qualifications"]))
            elif PREF_HDR.search(title) or PREF_KW.search(title):
                for l in lines: _push_unique(out["preferences"], l, set(out["preferences"]))

    # ìê²©ìš”ê±´ ë‚´ ìš°ëŒ€ í‚¤ì›Œë“œ ë³´ì •
    remain = []
    for q in out["qualifications"]:
        if PREF_KW.search(q): out["preferences"].append(q)
        else: remain.append(q)
    out["qualifications"] = remain

    # ìƒí•œ ì œí•œ/ì¤‘ë³µ ì œê±°
    for k in out:
        seen=set(); clean=[]
        for s in out[k]:
            s=_clean_line(s)
            if s and s not in seen:
                seen.add(s); clean.append(s)
        out[k] = clean[:12]
    return out

def parse_jobkorea(soup: BeautifulSoup) -> Dict[str, List[str]]:
    """
    JobKorea (jobkorea.co.kr) ì¶”ì¶œ ê·œì¹™:
      - ìƒì„¸í˜ì´ì§€ì— 'ìƒì„¸ìš”ê°•/ì§€ì›ìê²©/ìš°ëŒ€ì‚¬í•­' ì„¹ì…˜ì´ ë§ìŒ
      - h2/h3 í—¤ë” + ì¸ì ‘ ul/li ë˜ëŠ” div/p ìˆ˜ì§‘
    """
    res  = collect_after_heading(soup, re.compile(r"(ìƒì„¸\s*ìš”ê°•|ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|Responsibilities?)", re.I), 16)
    qual = collect_after_heading(soup, re.compile(r"(ì§€ì›\s*ìê²©|ìê²©\s*ìš”ê±´|Requirements?|Qualifications?)", re.I), 16)
    pref = collect_after_heading(soup, re.compile(r"(ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€|Preferred|Plus)", re.I), 16)

    remain=[]
    for q in qual:
        if PREF_KW.search(q): pref.append(q)
        else: remain.append(q)
    qual=remain

    return {
        "responsibilities": res[:12],
        "qualifications":   qual[:12],
        "preferences":      pref[:12],
    }

def parse_portal_specific(url: str, soup: Optional[BeautifulSoup], raw_text: str) -> Dict[str, List[str]]:
    """
    URL ë„ë©”ì¸ìœ¼ë¡œ íŒŒì„œ ë¶„ê¸°. soupì´ ì—†ì„ ë• raw_textë¡œ ìµœì†Œ ëŒ€ì‘(í—¤ë” í‚¤ì›Œë“œ ê¸°ë°˜).
    ë°˜í™˜: {"responsibilities":[...], "qualifications":[...], "preferences":[...]}
    """
    out = {"responsibilities":[], "qualifications":[], "preferences":[]}
    if not soup:
        # soup ì—†ìœ¼ë©´ raw_textë¥¼ ë¼ì¸ ìŠ¤ìº” (ìµœì†Œ ë³´ì¥)
        lines = [ _clean_line(x) for x in (raw_text or "").split("\n") if x.strip() ]
        bucket = None
        for l in lines:
            if RESP_HDR.search(l): bucket="responsibilities"; continue
            if QUAL_HDR.search(l): bucket="qualifications"; continue
            if PREF_HDR.search(l) or PREF_KW.search(l): bucket="preferences"; continue
            if bucket:
                _push_unique(out[bucket], l, set(out[bucket]))
        return out

    host = urllib.parse.urlsplit(normalize_url(url) or "").netloc.lower()
    if "wanted.co.kr" in host:
        out = parse_wanted(soup)
    elif "saramin.co.kr" in host:
        out = parse_saramin(soup)
    elif "jobkorea.co.kr" in host:
        out = parse_jobkorea(soup)
    else:
        # ê¸°íƒ€ í¬í„¸/ìì‚¬ ì±„ìš©: ê¸°ë³¸ í—¤ë” ê¸°ë°˜
        out["responsibilities"] = collect_after_heading(soup, RESP_HDR, 16)
        out["qualifications"]   = collect_after_heading(soup, QUAL_HDR, 16)
        out["preferences"]      = collect_after_heading(soup, PREF_HDR, 16)

    # ì•ˆì „ ë³´ì •: ìê²©ìš”ê±´ì— ìš°ëŒ€ í¬í•¨ëœ ê²½ìš°
    remain=[]
    for q in out.get("qualifications", []):
        if PREF_KW.search(q): out["preferences"].append(q)
        else: remain.append(q)
    out["qualifications"] = remain

    # ì¤‘ë³µ/ê¸¸ì´ ì œí•œ
    for k in out:
        seen=set(); clean=[]
        for s in out[k]:
            s=_clean_line(s)
            if s and s not in seen:
                seen.add(s); clean.append(s)
        out[k]=clean[:12]
    return out

# ================== LLM ì •ì œ (ì±„ìš© ê³µê³  â†’ êµ¬ì¡° JSON) ==================
PROMPT_SYSTEM_STRUCT = (
    "ë„ˆëŠ” ì±„ìš© ê³µê³ ë¥¼ ê¹”ë”í•˜ê²Œ êµ¬ì¡°í™”í•˜ëŠ” ë³´ì¡°ì›ì´ë‹¤. "
    "ì…ë ¥ í…ìŠ¤íŠ¸ëŠ” í¬í„¸ ê´‘ê³  ë¬¸êµ¬, UIì”ì¬, ë³µìˆ˜ ì§ë¬´ê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆë‹¤. "
    "í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì¤‘ë³µì—†ì´ ì •ì œí•˜ë¼."
)

def llm_structurize(raw_text: str, meta_hint: Dict[str,str], model: str) -> Dict:
    ctx = (raw_text or "").strip()
    if len(ctx) > 9000:
        ctx = ctx[:9000]

    user_msg = {
        "role": "user",
        "content": (
            "ë‹¤ìŒ ì±„ìš© ê³µê³  ì›ë¬¸ì„ êµ¬ì¡°í™”í•´ì¤˜.\n\n"
            f"[íŒíŠ¸] íšŒì‚¬ëª… í›„ë³´: {meta_hint.get('company_name','')}\n"
            f"[íŒíŠ¸] ì§ë¬´ëª… í›„ë³´: {meta_hint.get('job_title','')}\n"
            "--- ì›ë¬¸ ì‹œì‘ ---\n"
            f"{ctx}\n"
            "--- ì›ë¬¸ ë ---\n\n"
            "JSONìœ¼ë¡œë§Œ ë‹µí•˜ê³ , í‚¤ëŠ” ë°˜ë“œì‹œ ì•„ë˜ë§Œ í¬í•¨:\n"
            "{"
            "\"company_name\": str, "
            "\"company_intro\": str, "
            "\"job_title\": str, "
            "\"responsibilities\": [str], "
            "\"qualifications\": [str], "
            "\"preferences\": [str]"
            "}\n"
            "- 'ìš°ëŒ€ ì‚¬í•­(preferences)'ì€ ë¹„ì›Œë‘ì§€ ë§ê³ , ì›ë¬¸ì—ì„œ 'ìš°ëŒ€/ì„ í˜¸/preferred/plus/ê°€ì‚°ì ' ë“± í‘œì‹œê°€ ìˆëŠ” í•­ëª©ì„ ê·¸ëŒ€ë¡œ ë‹´ì•„ë¼.\n"
            "- ë¶ˆë¦¿/ë§ˆì»¤/ì´ëª¨ì§€ ì œê±°, ë¬¸ì¥ ê°„ê²°í™”, ì¤‘ë³µ ì œê±°."
        ),
    }

    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.2,
            response_format={"type": "json_object"},
            messages=[{"role":"system","content":PROMPT_SYSTEM_STRUCT}, user_msg],
        )
        data = json.loads(resp.choices[0].message.content)
    except Exception as e:
        data = {
            "company_name": meta_hint.get("company_name",""),
            "company_intro": meta_hint.get("company_intro","ì›ë¬¸ì´ ì •ì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."),
            "job_title": meta_hint.get("job_title",""),
            "responsibilities": [],
            "qualifications": [],
            "preferences": [],
            "error": str(e),
        }

    # í´ë¦°ì—…/ë³´ì •
    for k in ["responsibilities","qualifications","preferences"]:
        arr = data.get(k, [])
        if not isinstance(arr, list): arr = []
        clean_list=[]; seen=set()
        for it in arr:
            t = _clean_line(str(it))
            if t and t not in seen:
                seen.add(t); clean_list.append(t)
        data[k] = clean_list[:12]

    for k in ["company_name","company_intro","job_title"]:
        if k in data and isinstance(data[k], str):
            data[k] = re.sub(r"\s+"," ", data[k]).strip()

    # LLMì´ ìš°ëŒ€ì‚¬í•­ì„ ë†“ì¹œ ê²½ìš° ì¶”ê°€ ë³´ì •
    if len(data.get("preferences", [])) < 1:
        # ìê²© ìš”ê±´ ë‚´ 'ìš°ëŒ€' í‚¤ì›Œë“œ ì´ë™
        kw_pref = PREF_KW
        remain=[]; moved=[]
        for q in data.get("qualifications", []):
            if kw_pref.search(q):
                moved.append(q)
            else:
                remain.append(q)
        if moved:
            data["preferences"] = moved[:12]
            data["qualifications"] = remain[:12]
    return data

# ================== íŒŒì¼ ë¦¬ë” (PDF/TXT/MD/DOCX) ==================
try:
    import pypdf
except Exception:
    pypdf = None

def read_pdf(data: bytes) -> str:
    if pypdf is None:
        return ""
    try:
        reader = pypdf.PdfReader(io.BytesIO(data))
        return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
    except Exception:
        return ""

def read_docx(data: bytes) -> str:
    try:
        import docx2txt, tempfile
        with tempfile.NamedTemporaryFile(suffix=".docx", delete=True) as tmp:
            tmp.write(data); tmp.flush()
            text = docx2txt.process(tmp.name) or ""
            return text
    except Exception:
        return ""

def read_file_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt",".md")):
        for enc in ("utf-8","cp949","euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        return read_pdf(data)
    elif name.endswith(".docx"):
        return read_docx(data)
    return ""

# ================== ê°„ë‹¨ ì²­í¬/ì„ë² ë”© ==================
def chunk(text: str, size: int = 600, overlap: int = 120) -> List[str]:
    t = re.sub(r"\s+"," ", text).strip()
    if not t: return []
    out, start = [], 0
    while start < len(t):
        end = min(len(t), start+size)
        out.append(t[start:end])
        if end == len(t): break
        start = max(0, end-overlap)
    return out

def embed_texts(texts: List[str], model_name: str) -> np.ndarray:
    if not texts:
        return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=model_name, input=texts)
    vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
    return vecs

# ================== ì„¸ì…˜ ìƒíƒœ ==================
if "clean_struct" not in st.session_state:
    st.session_state.clean_struct = None
if "resume_raw" not in st.session_state:
    st.session_state.resume_raw = ""
if "resume_chunks" not in st.session_state:
    st.session_state.resume_chunks = []
if "resume_embeds" not in st.session_state:
    st.session_state.resume_embeds = None

# ================== 1) ì±„ìš© ê³µê³  URL â†’ ì •ì œ ==================
st.header("1) ì±„ìš© ê³µê³  URL â†’ ì •ì œ")
url = st.text_input("ì±„ìš© ê³µê³  ìƒì„¸ URL", placeholder="ì±„ìš© ê³µê³  ì‚¬ì´íŠ¸ì˜ URLì„ ì…ë ¥í•˜ì„¸ìš”")
if st.button("ì›ë¬¸ ìˆ˜ì§‘ â†’ ì •ì œ", type="primary"):
    if not url.strip():
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ì›ë¬¸ ìˆ˜ì§‘ ì¤‘..."):
            raw, meta, soup = fetch_all_text(url.strip())
            hint = extract_company_meta(soup)

        if not raw:
            st.error("ì›ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë¡œê·¸ì¸/ë™ì  ë Œë”ë§/ë´‡ ì°¨ë‹¨ ê°€ëŠ¥)")
        else:
            # 1) ì‚¬ì´íŠ¸ë³„ ì •ë°€ í¬ë¡¤ëŸ¬ ìš°ì„  ì‹œë„
            site_struct = parse_portal_specific(url.strip(), soup, raw)
            ok_cnt = sum(len(site_struct.get(k, [])) for k in ["responsibilities","qualifications","preferences"])
            # 2) ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ LLM ì •ì œ ë³´ì™„
            if ok_cnt < 3:  # í•­ëª©ì´ ë„ˆë¬´ ì ê±°ë‚˜ ê±°ì˜ ëª» ì°¾ì•˜ìœ¼ë©´
                with st.spinner("LLMìœ¼ë¡œ ì •ì œ ì¤‘..."):
                    clean = llm_structurize(raw, hint, CHAT_MODEL)
                # ì •ë°€+LLM ë³‘í•©(ì •ë°€ì´ ìˆëŠ” í•­ëª©ì€ ìš°ì„  ìœ ì§€)
                for k in ["responsibilities","qualifications","preferences"]:
                    if site_struct.get(k):
                        clean[k] = site_struct[k]
                # ë©”íƒ€ ë³´ê°•
                if site_struct and not clean.get("company_name"): clean["company_name"] = hint.get("company_name","")
                if site_struct and not clean.get("job_title"):    clean["job_title"]    = hint.get("job_title","")
            else:
                # ì •ë°€ ê²°ê³¼ë¡œ clean êµ¬ì„±
                clean = {
                    "company_name": hint.get("company_name",""),
                    "company_intro": hint.get("company_intro",""),
                    "job_title": hint.get("job_title",""),
                    "responsibilities": site_struct.get("responsibilities",[]),
                    "qualifications":   site_struct.get("qualifications",[]),
                    "preferences":      site_struct.get("preferences",[]),
                }

            st.session_state.clean_struct = clean
            st.success("ì •ì œ ì™„ë£Œ!")

# ================== 2) íšŒì‚¬ ìš”ì•½ (ì •ì œ ê²°ê³¼) ==================
st.header("2) íšŒì‚¬ ìš”ì•½")
clean = st.session_state.clean_struct
if clean:
    st.markdown(f"**íšŒì‚¬ëª…:** {clean.get('company_name','-')}")
    st.markdown(f"**ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ(ìš”ì•½):** {clean.get('company_intro','-')}")
    st.markdown(f"**ëª¨ì§‘ ë¶„ì•¼(ì§ë¬´ëª…):** {clean.get('job_title','-')}")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown("**ì£¼ìš” ì—…ë¬´**")
        for b in clean.get("responsibilities", []): st.markdown(f"- {b}")
    with c2:
        st.markdown("**ìê²© ìš”ê±´**")
        for b in clean.get("qualifications", []): st.markdown(f"- {b}")
    with c3:
        st.markdown("**ìš°ëŒ€ ì‚¬í•­**")
        prefs = clean.get("preferences", [])
        if prefs:
            for b in prefs: st.markdown(f"- {b}")
        else:
            st.caption("ìš°ëŒ€ ì‚¬í•­ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
else:
    st.info("ë¨¼ì € URLì„ ì •ì œí•´ ì£¼ì„¸ìš”.")

st.divider()

# ================== 3) ë‚´ ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ ì—…ë¡œë“œ (DOCX/TXT/MD/PDF) ==================
st.header("3) ë‚´ ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ ì—…ë¡œë“œ")
uploads = st.file_uploader(
    "ì´ë ¥ì„œ/í”„ë¡œì íŠ¸ íŒŒì¼ ì—…ë¡œë“œ (PDF/TXT/MD/DOCX, ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
    type=["pdf","txt","md","docx"], accept_multiple_files=True
)

# ë‚´ë¶€ìš© ê¸°ë³¸ íŒŒë¼ë¯¸í„° (UI ë¹„ë…¸ì¶œ)
_RESUME_CHUNK = 600
_RESUME_OVLP  = 120

if st.button("ì´ë ¥ì„œ ì¸ë±ì‹±(ìë™)", type="secondary"):
    if not uploads:
        st.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        all_text=[]
        for up in uploads:
            t = read_file_text(up)
            if t: all_text.append(t)
        resume_text = "\n\n".join(all_text)
        if not resume_text.strip():
            st.error("í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            chunks = chunk(resume_text, size=_RESUME_CHUNK, overlap=_RESUME_OVLP)
            with st.spinner("ì´ë ¥ì„œ ë²¡í„°í™” ì¤‘..."):
                embeds = embed_texts(chunks, EMBED_MODEL)
            st.session_state.resume_raw = resume_text
            st.session_state.resume_chunks = chunks
            st.session_state.resume_embeds = embeds
            st.success(f"ì¸ë±ì‹± ì™„ë£Œ (ì²­í¬ {len(chunks)}ê°œ)")

# ================== (Step4) ì´ë ¥ì„œ ê¸°ë°˜ ìì†Œì„œ ìƒì„± ==================
st.header("4) ì´ë ¥ì„œ ê¸°ë°˜ ìì†Œì„œ ìƒì„±")
topic = st.text_input("íšŒì‚¬ ìš”ì²­ ì£¼ì œ(ì„ íƒ)", placeholder="ì˜ˆ: ì„±ì¥ ê³¼ì • / ì§ë¬´ ì§€ì›ë™ê¸° / í˜‘ì—… ê²½í—˜ / ë¬¸ì œí•´ê²° ì‚¬ë¡€ ë“±")

def build_cover_letter(clean_struct: Dict, resume_text: str, topic_hint: str, model: str) -> str:
    company = json.dumps(clean_struct or {}, ensure_ascii=False)
    resume_snippet = resume_text.strip()
    if len(resume_snippet) > 9000:
        resume_snippet = resume_snippet[:9000]

    system = (
        "ë„ˆëŠ” í•œêµ­ì–´ ìê¸°ì†Œê°œì„œ ì „ë¬¸ê°€ë‹¤. ì±„ìš© ê³µê³ ì˜ íšŒì‚¬/ì§ë¬´ ìš”ê±´ê³¼ í›„ë³´ìì˜ ì´ë ¥ì„œë¥¼ ì°¸ê³ í•´ "
        "íšŒì‚¬ íŠ¹í™” ìì†Œì„œë¥¼ ì‘ì„±í•œë‹¤. ê³¼ì¥/í—ˆìœ„ëŠ” ê¸ˆì§€í•˜ê³ , ìˆ˜ì¹˜/ì§€í‘œ/ê¸°ê°„/ì„íŒ©íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì²´í™”í•œë‹¤."
    )
    if topic_hint and topic_hint.strip():
        req = f"íšŒì‚¬ ì¸¡ ìš”ì²­ ì£¼ì œëŠ” '{topic_hint.strip()}' ì´ë‹¤. ì´ ì£¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•˜ë¼."
    else:
        req = "íŠ¹ì • ì£¼ì œ ìš”ì²­ì´ ì—†ìœ¼ë¯€ë¡œ, ì±„ìš© ê³µê³ ì˜ ìš”ê±´ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì§€ì›ë™ê¸°ì™€ ì§ë¬´ì í•©ì„±ì„ ê°•ì¡°í•˜ë¼."

    user = (
        f"[íšŒì‚¬/ì§ë¬´ ìš”ì•½(JSON)]\n{company}\n\n"
        f"[í›„ë³´ì ì´ë ¥ì„œ(ìš”ì•½ ê°€ëŠ¥)]\n{resume_snippet}\n\n"
        f"[ì‘ì„± ì§€ì‹œ]\n- {req}\n"
        "- ë¶„ëŸ‰: 600~1000ì\n"
        "- êµ¬ì„±: 1) ì§€ì› ë™ê¸° 2) ì§ë¬´ ê´€ë ¨ í•µì‹¬ ì—­ëŸ‰Â·ê²½í—˜ 3) ì„±ê³¼/ì§€í‘œ 4) ì…ì‚¬ í›„ ê¸°ì—¬ ë°©ì•ˆ 5) ë§ˆë¬´ë¦¬\n"
        "- ìì—°ìŠ¤ëŸ½ê³  ì§„ì •ì„± ìˆëŠ” 1ì¸ì¹­ ì„œìˆ . ë¬¸ì¥ê³¼ ë¬¸ë‹¨ ê°€ë…ì„±ì„ ìœ ì§€.\n"
        "- ë¶ˆí•„ìš”í•œ ë¯¸ì‚¬ì—¬êµ¬/ì¤‘ë³µ/ê´‘ê³  ë¬¸êµ¬ ì‚­ì œ."
    )
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.4,
            messages=[{"role":"system","content":system},{"role":"user","content":user}]
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"(ìì†Œì„œ ìƒì„± ì‹¤íŒ¨: {e})"

if st.button("ìì†Œì„œ ìƒì„±", type="primary"):
    if not st.session_state.clean_struct:
        st.warning("ë¨¼ì € íšŒì‚¬ URLì„ ì •ì œí•˜ì„¸ìš”.")
    elif not st.session_state.resume_raw.strip():
        st.warning("ë¨¼ì € ì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  'ì´ë ¥ì„œ ì¸ë±ì‹±(ìë™)'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ìì†Œì„œ ìƒì„± ì¤‘..."):
            cover = build_cover_letter(st.session_state.clean_struct, st.session_state.resume_raw, topic, CHAT_MODEL)
        st.subheader("ìì†Œì„œ (ìƒì„± ê²°ê³¼)")
        st.write(cover)
        st.download_button(
            "ìì†Œì„œ TXT ë‹¤ìš´ë¡œë“œ",
            data=cover.encode("utf-8"),
            file_name="cover_letter.txt",
            mime="text/plain"
        )
