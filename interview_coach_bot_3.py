# -*- coding: utf-8 -*-
# interview_coach_bot.py
# v3.1 â€” ì±„ìš© í¬í„¸ ìƒì„¸ í˜ì´ì§€ì˜ "ëª¨ë“  í…ìŠ¤íŠ¸"ë¥¼ â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´ì— ê·¸ëŒ€ë¡œ ì¶œë ¥
#        (WebBaseLoader ìš°ì„ , ì‹¤íŒ¨ì‹œ BeautifulSoup get_text() í´ë°±)
#        ë‚˜ë¨¸ì§€ ê¸°ëŠ¥(ì§ˆë¬¸ ìƒì„±, ì±„ì /ë ˆì´ë‹¤/CSV)ì€ ê·¸ëŒ€ë¡œ ìœ ì§€

import os, io, re, json, textwrap, urllib.parse, difflib, random, time
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import pandas as pd
import streamlit as st

# ---------- Optional deps ----------
try:
    import pypdf
except Exception:
    pypdf = None

try:
    import plotly.graph_objects as go
    PLOTLY_OK = True
except Exception:
    PLOTLY_OK = False

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# --- WebBaseLoader (ì›ë¬¸ ì „ì²´ ë¡œë“œìš©)
try:
    from langchain_community.document_loaders import WebBaseLoader
    WEBBASE_OK = True
except Exception:
    WEBBASE_OK = False

import requests
from bs4 import BeautifulSoup

# ---------- Page config ----------
st.set_page_config(page_title="íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜", page_icon="ğŸ¯", layout="wide")

# =========================================================
# Secrets / API keys
# =========================================================
def _secrets_file_exists() -> bool:
    candidates = [
        os.path.join(os.path.expanduser("~"), ".streamlit", "secrets.toml"),
        os.path.join(os.getcwd(), ".streamlit", "secrets.toml"),
    ]
    return any(os.path.exists(p) for p in candidates)

def load_api_key_from_env_or_secrets() -> Optional[str]:
    key = os.getenv("OPENAI_API_KEY")
    if key: return key
    try:
        if _secrets_file_exists() or hasattr(st, "secrets"):
            return st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        pass
    return None

def load_naver_keys():
    cid = os.getenv("NAVER_CLIENT_ID")
    csec = os.getenv("NAVER_CLIENT_SECRET")
    try:
        if hasattr(st, "secrets"):
            cid = cid or st.secrets.get("NAVER_CLIENT_ID", None)
            csec = csec or st.secrets.get("NAVER_CLIENT_SECRET", None)
    except Exception:
        pass
    return cid, csec

NAVER_ID, NAVER_SECRET = load_naver_keys()
UA = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36"}

# =========================================================
# Utils
# =========================================================
def _clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", t or "").strip()

def _snippetize(text: str, maxlen: int = 240) -> str:
    t = _clean_text(text)
    return t if len(t) <= maxlen else t[: maxlen - 1] + "â€¦"

def _domain(u: str|None) -> str|None:
    if not u: return None
    try:
        if not u.startswith("http"): u = "https://" + u
        return urllib.parse.urlparse(u).netloc.lower().replace("www.","")
    except Exception:
        return None

def read_file_to_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt", ".md")):
        for enc in ("utf-8", "cp949", "euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        if pypdf is None:
            st.warning("pypdfê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— pypdf ì¶”ê°€.")
            return ""
        try:
            reader = pypdf.PdfReader(io.BytesIO(data))
            return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
        except Exception as e:
            st.warning(f"PDF íŒŒì‹± ì‹¤íŒ¨({uploaded.name}): {e}")
            return ""
    return ""

def chunk_text(text: str, size: int = 900, overlap: int = 150):
    text = re.sub(r"\s+", " ", text).strip()
    if not text: return []
    out, start = [], 0
    while start < len(text):
        end = min(len(text), start + size)
        out.append(text[start:end])
        if end == len(text): break
        start = max(0, end - overlap)
    return out

# =========================================================
# NAVER Open API (ë‰´ìŠ¤/ì›¹ê²€ìƒ‰)
# =========================================================
def _naver_api_get(api: str, params: dict, cid: str, csec: str):
    url = f"https://openapi.naver.com/v1/search/{api}.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec, **UA}
    r = requests.get(url, headers=headers, params=params, timeout=8)
    if r.status_code != 200:
        return None
    return r.json()

@st.cache_data(show_spinner=False, ttl=1800)
def naver_search_news(query: str, display: int = 8, sort: str = "date") -> list[dict]:
    cid, csec = load_naver_keys()
    if not (cid and csec): return []
    js = _naver_api_get("news", {"query": query, "display": display, "sort": sort}, cid, csec)
    if not js: return []
    out = []
    for it in js.get("items", []):
        title = _clean_text(re.sub(r"</?b>|&quot;|&apos;|&amp;|&lt;|&gt;", "", it.get("title","")))
        out.append({"title": title, "link": it.get("link"), "pubDate": it.get("pubDate")})
    return out

@st.cache_data(show_spinner=False, ttl=3600)
def naver_search_web(query: str, display: int = 10, sort: str = "date") -> list[str]:
    cid, csec = load_naver_keys()
    if not (cid and csec): return []
    js = _naver_api_get("webkr", {"query": query, "display": display, "sort": sort}, cid, csec)
    if not js: return []
    links = []
    for it in js.get("items", []):
        link = it.get("link")
        if link and link not in links:
            links.append(link)
    return links

@st.cache_data(ttl=3600)
def fetch_news(company_name: str, max_items: int = 6) -> list[dict]:
    news = naver_search_news(company_name, display=max_items, sort="date")
    if news:
        return news
    # Google RSS fallback
    q = urllib.parse.quote(company_name)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    items = []
    try:
        r = requests.get(url, timeout=8, headers=UA)
        if r.status_code != 200: return []
        soup = BeautifulSoup(r.text, "xml")
        for it in soup.find_all("item")[:max_items]:
            title = _clean_text(it.title.get_text()) if it.title else ""
            link  = it.link.get_text() if it.link else ""
            pub   = it.pubDate.get_text() if it.pubDate else ""
            items.append({"title": title, "link": link, "pubDate": pub})
    except Exception:
        return []
    return items

# =========================================================
# ì±„ìš© ë§í¬ íƒìƒ‰ + ìƒì„¸ ê³µê³ 
# =========================================================
CAREER_HINTS = ["careers","recruit","jobs","career","ì±„ìš©","ì¸ì¬ì˜ì…","recruitment","join"]
SEARCH_ENGINES = ["https://duckduckgo.com/html/?q={query}"]
JOB_SITES = ["wanted.co.kr","saramin.co.kr","jobkorea.co.kr","rocketpunch.com","linkedin.com","indeed.com"]

def _first_detail_from_list(url: str, role_hint: str="") -> Optional[str]:
    try:
        r = requests.get(url, timeout=8, headers=UA)
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""): return None
        soup = BeautifulSoup(r.text, "html.parser")
        dom = _domain(url) or ""
        if "wanted.co.kr" in dom:
            for a in soup.select("a[href*='/wd/']"):
                href = urllib.parse.urljoin(url, a.get("href"))
                title = (a.get_text() or "").strip()
                if role_hint and (role_hint not in title):
                    continue
                return href
        if "saramin.co.kr" in dom:
            for a in soup.select("a[href*='view?idx=']"):
                return urllib.parse.urljoin(url, a.get("href"))
        if "jobkorea.co.kr" in dom:
            for a in soup.select("a[href*='/Recruit/GI_Read/']"):
                return urllib.parse.urljoin(url, a.get("href"))
        # ì¼ë°˜ íŒ¨í„´
        for a in soup.find_all("a", href=True):
            href = urllib.parse.urljoin(url, a.get("href"))
            if re.search(r"/(wd|jobs|job|view|read|detail|posting)/", href, re.I):
                return href
    except Exception:
        return None
    return None

@st.cache_data(show_spinner=False, ttl=3600)
def discover_job_from_homepage(homepage: str, limit: int = 5) -> list[str]:
    if not homepage: return []
    try:
        if not homepage.startswith("http"): homepage = "https://" + homepage
        r = requests.get(homepage, timeout=8, headers=UA)
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
            return []
        soup = BeautifulSoup(r.text, "html.parser")
        links=[]
        for a in soup.find_all("a", href=True):
            href = a["href"]
            text = (a.get_text() or "").lower()
            if any(k in href.lower() or k in text for k in CAREER_HINTS):
                links.append(urllib.parse.urljoin(homepage, href))
        out=[]; seen=set()
        for lk in links:
            d = _domain(lk)
            if lk not in seen and d:
                seen.add(lk); out.append(lk)
                if len(out) >= limit: break
        return out[:limit]
    except Exception:
        return []

@st.cache_data(show_spinner=False, ttl=3600)
def discover_job_posting_urls(company: str, role: str, homepage: str|None, limit: int=5) -> list[str]:
    urls=[]
    if homepage:
        urls += discover_job_from_homepage(homepage, limit=limit)
    resolved=[]
    for u in urls:
        if re.search(r"/(wd|view|read|detail|posting)/", u, re.I):
            resolved.append(u)
        else:
            detail = _first_detail_from_list(u, role_hint=role or "")
            if detail: resolved.append(detail)
    urls = resolved[:]
    if urls: return urls[:limit]

    # í¬í„¸ ê²€ìƒ‰
    if NAVER_ID and NAVER_SECRET:
        for dom in JOB_SITES:
            if len(urls)>=limit: break
            q = f"{company} {role} site:{dom}" if role else f"{company} ì±„ìš© site:{dom}"
            links = naver_search_web(q, display=7, sort="date")
            for lk in links:
                if _domain(lk) and dom in _domain(lk) and lk not in urls:
                    if not re.search(r"/(wd|view|read|detail|posting)/", lk, re.I):
                        lk2 = _first_detail_from_list(lk, role_hint=role or "")
                        if lk2: lk = lk2
                    urls.append(lk)
                if len(urls)>=limit: break
    return urls[:limit]

# =========================================================
# ì›ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë” (í•µì‹¬)
# =========================================================
# í•„ìš”í•œ import
import requests, re, urllib.parse
from bs4 import BeautifulSoup

# WebBaseLoader (langchain-community) ì‹œë„
try:
    from langchain_community.document_loaders import WebBaseLoader
    WEBBASE_OK = True
except Exception:
    WEBBASE_OK = False

# Playwright URL Loader (ì„ íƒ) - ì„¤ì¹˜ í•„ìš”
USE_PLAYWRIGHT = False
try:
    from langchain.document_loaders import PlaywrightURLLoader 
    #from langchain.document_loaders import PlaywrightURLLoader
    PLAYWRIGHT_OK = True
except Exception:
    PLAYWRIGHT_OK = False

UA = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
}

def webbase_dump_all_text(url: str) -> str:
    if not WEBBASE_OK or not url:
        return ""
    try:
        loader = WebBaseLoader(url)
        docs = loader.load()
        return "\n\n".join([getattr(d, "page_content", "") for d in docs if getattr(d, "page_content", "")])
    except Exception:
        return ""

def bs4_dump_all_text(url: str) -> str:
    try:
        r = requests.get(url, timeout=12, headers=UA)
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
            return ""
        soup = BeautifulSoup(r.text, "html.parser")
        for tag in soup(["script","style","noscript"]):
            tag.decompose()
        text = soup.get_text(separator="\n", strip=True)
        return re.sub(r"\n{3,}", "\n\n", text)
    except Exception:
        return ""

def jina_reader_text(url: str) -> str:
    try:
        # r.jina.ai proxy (í”„ë¦¬ë Œë”) â€” public ì„œë¹„ìŠ¤, ë„ë©”ì¸/ë¦¬ë¯¸íŠ¸ ì£¼ì˜
        proxied = "https://r.jina.ai/http/" + url
        r = requests.get(proxied, timeout=18, headers=UA)
        if r.status_code != 200:
            return ""
        txt = r.text.strip()
        # ì¼ë¶€ í˜ì´ì§€ëŠ” ì•„ì£¼ ì§§ì€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ë¯€ë¡œ ê¸¸ì´ ì œí•œ ì™„í™”
        return txt if len(txt) > 10 else txt
    except Exception:
        return ""

def playwright_dump_all_text(url: str) -> str:
    # Playwright í™˜ê²½ì´ ì¤€ë¹„ë˜ì–´ ìˆì–´ì•¼ í•¨. Streamlit CloudëŠ” ì§€ì› ì—¬ë¶€ í™•ì¸ í•„ìš”.
    if not PLAYWRIGHT_OK:
        return ""
    try:
        loader = PlaywrightURLLoader(urls=[url], browser_type="chromium", headless=True, wait_until="networkidle")
        docs = loader.load()
        return "\n\n".join([getattr(d, "page_content", "") for d in docs if getattr(d, "page_content", "")])
    except Exception:
        return ""

def get_full_page_text(url: str) -> str:
    # ì •ê·œí™”(ê°„ë‹¨)
    if not url: return ""
    if url.startswith("//"): url = "https:" + url
    # 1) WebBaseLoader
    txt = webbase_dump_all_text(url)
    if txt and len(txt.strip()) > 50:
        return txt
    # 2) BeautifulSoup
    txt = bs4_dump_all_text(url)
    if txt and len(txt.strip()) > 50:
        return txt
    # 3) Jina í”„ë¦¬ë Œë”
    txt = jina_reader_text(url)
    if txt and len(txt.strip()) > 10:
        return txt
    # 4) Playwright (ê°€ì¥ ê°•ë ¥í•˜ì§€ë§Œ ë¦¬ì†ŒìŠ¤/ì„¤ì¹˜ í•„ìš”)
    txt = playwright_dump_all_text(url)
    return txt or ""


# =========================================================
# OpenAI
# =========================================================
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    API_KEY = load_api_key_from_env_or_secrets()
    if not API_KEY:
        st.info("í™˜ê²½ë³€ìˆ˜/Secretsì—ì„œ í‚¤ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ì— ì…ë ¥ í›„ ì—”í„°.")
        API_KEY = st.text_input("OPENAI_API_KEY", type="password")
    MODEL = st.selectbox("ì±— ëª¨ë¸", ["gpt-4o-mini","gpt-4o","gpt-4.1-mini"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small","text-embedding-3-large"], index=0)

    with st.expander("ë””ë²„ê·¸"):
        try:
            import openai as _openai_pkg; _openai_ver = getattr(_openai_pkg, "__version__", None)
        except Exception: _openai_ver=None
        try:
            import httpx as _httpx_pkg; _httpx_ver = getattr(_httpx_pkg, "__version__", None)
        except Exception: _httpx_ver=None
        st.write({
            "api_key_provided": bool(API_KEY),
            "naver_keys": bool(NAVER_ID and NAVER_SECRET),
            "openai_version": _openai_ver,
            "httpx_version": _httpx_ver,
            "webbaseloader_available": WEBBASE_OK
        })

if not OpenAI or not API_KEY:
    st.error("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    st.stop()
client = OpenAI(api_key=API_KEY, timeout=30.0)

# =========================================================
# â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥
# =========================================================
st.subheader("â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥")
c1, c2 = st.columns(2)
with c1:
    company_name_input = st.text_input("íšŒì‚¬ ì´ë¦„", placeholder="ì˜ˆ: ë„¤ì´ë²„ / Kakao / ì‚¼ì„±SDS")
with c2:
    role_title = st.text_input("ì§€ì› ì§ë¬´ëª…", placeholder="ë°ì´í„° ì• ë„ë¦¬ìŠ¤íŠ¸ / ML ì—”ì§€ë‹ˆì–´ ...")
job_url_input  = st.text_input("ì±„ìš© ê³µê³  URL(ì„ íƒ) â€” ì—†ë‹¤ë©´ ìë™ íƒìƒ‰")
homepage_input = st.text_input("ê³µì‹ í™ˆí˜ì´ì§€ URL(ì„ íƒ)", placeholder="https://...")

for k,v in [("company", None), ("answer_text",""), ("history",[]), ("current_question","")]:
    if k not in st.session_state: st.session_state[k]=v

def build_company_obj(name: str, homepage: str|None, role: str|None, job_url: str|None) -> dict:
    # ë‰´ìŠ¤ëŠ” ìœ ì§€(ë‹¤ë¥¸ ì„¹ì…˜ ìš©)
    news = fetch_news(name, max_items=6)
    # ê³µê³  URL ê²°ì •
    if job_url and job_url.strip():
        urls = [job_url.strip()]
    else:
        urls = discover_job_posting_urls(name, role or "", homepage, limit=6)
    chosen_url = urls[0] if urls else None
    # í•µì‹¬: ì›ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸
    full_text = get_full_page_text(chosen_url) if chosen_url else ""
    return {
        "company_name": name.strip() or "(íšŒì‚¬ëª… ë¯¸ì„¤ì •)",
        "homepage": homepage or None,
        "role": role or "",
        "job_url": chosen_url,
        "news": news,
        # â†“â†“â†“ í•µì‹¬: í˜ì´ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸(ê·¸ëŒ€ë¡œ) ì €ì¥
        "job_raw_text": full_text
    }

if st.button("íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°", type="primary"):
    if not company_name_input.strip():
        st.warning("íšŒì‚¬ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        st.session_state.history = []
        st.session_state.current_question = ""
        st.session_state.answer_text = ""
        with st.spinner("íšŒì‚¬/ì§ë¬´/ê³µê³ /ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì›ë¬¸ ë¡œë“œ ì¤‘..."):
            st.session_state.company = build_company_obj(company_name_input, homepage_input or None, role_title or None, job_url_input or None)
        st.success("íšŒì‚¬ ì •ë³´ ê°±ì‹  ë° ì‹¤í–‰ê²°ê³¼ ì´ˆê¸°í™” ì™„ë£Œ!")

company = st.session_state.get("company")

# =========================================================
# â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´  â†’  â€œí˜ì´ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸â€ ê·¸ëŒ€ë¡œ ì¶œë ¥
# =========================================================
st.subheader("â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´ (í˜ì´ì§€ ì›ë¬¸ ì „ì²´)")

if company:
    cols = st.columns(2)
    with cols[0]:
        if company.get("job_url"): st.link_button("ì±„ìš© ê³µê³  ì—´ê¸°", company["job_url"])
    with cols[1]:
        if company.get("homepage"): st.link_button("í™ˆí˜ì´ì§€ ì—´ê¸°", company["homepage"])
    if company.get("news"):
        st.markdown("**ìµœê·¼ ë‰´ìŠ¤:**")
        for n in company["news"][:3]:
            st.markdown(f"- [{_clean_text(n['title'])}]({n['link']})")
    st.markdown("---")

    raw_text = company.get("job_raw_text","").strip()
    if not raw_text:
        st.warning("ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ì¸/ë´‡ì°¨ë‹¨/ë™ì  ë Œë”ë§ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê±°ë‚˜, ê³µê³  URLì„ ì§ì ‘ ì…ë ¥í•´ ë³´ì„¸ìš”.")
    # ìš”ì²­ì‚¬í•­: ë‘ ì„¹ì…˜ ëª¨ë‘ â€˜ëª¨ë“  í…ìŠ¤íŠ¸â€™ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥
    c1, c2 = st.columns(2)
    with c1:
        st.markdown("#### íšŒì‚¬ ìš”ì•½ (ì›ë¬¸ ì „ì²´)")
        st.text_area("íšŒì‚¬ ìš”ì•½ ì›ë¬¸", value=raw_text, height=500)
        st.download_button("íšŒì‚¬ ìš”ì•½ ì›ë¬¸ ë‹¤ìš´ë¡œë“œ", data=raw_text.encode("utf-8"), file_name="company_summary_raw.txt", mime="text/plain")
    with c2:
        st.markdown("#### ì±„ìš© ìš”ê±´ (ì›ë¬¸ ì „ì²´)")
        st.text_area("ì±„ìš© ìš”ê±´ ì›ë¬¸", value=raw_text, height=500)
        st.download_button("ì±„ìš© ìš”ê±´ ì›ë¬¸ ë‹¤ìš´ë¡œë“œ", data=raw_text.encode("utf-8"), file_name="job_requirements_raw.txt", mime="text/plain")

else:
    st.info("ìœ„ ì…ë ¥ì„ ì™„ë£Œí•˜ê³  â€˜íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°â€™ë¥¼ ëˆŒëŸ¬ í‘œì‹œí•˜ì„¸ìš”.")

# =========================================================
# â‘¢ ì§ˆë¬¸ ìƒì„±  (ê¸°ì¡´ ìœ ì§€)
# =========================================================
st.subheader("â‘¢ ì§ˆë¬¸ ìƒì„±")

TYPE_INSTRUCTIONS = {
    "í–‰ë™(STAR)": "S(ìƒí™©)-T(ê³¼ì œ)-A(í–‰ë™)-R(ì„±ê³¼) ì‹¤ë¬´ì‚¬ë¡€ ìœ ë„",
    "ê¸°ìˆ  ì‹¬ì¸µ": "ì„¤ê³„/íŠ¸ë ˆì´ë“œì˜¤í”„/ì„±ëŠ¥-ë¹„ìš©/í’ˆì§ˆ ì§€í‘œ ì‹¬ì¸µ",
    "í•µì‹¬ê°€ì¹˜ ì í•©ì„±": "í•µì‹¬ê°€ì¹˜Â·íƒœë„ ê²€ì¦ ìƒí™©í˜•",
    "ì—­ì§ˆë¬¸": "ì§€ì›ìê°€ íšŒì‚¬ë¥¼ í‰ê°€í•˜ëŠ” ì—­ì§ˆë¬¸"
}
q_type = st.selectbox("ì§ˆë¬¸ ìœ í˜•", list(TYPE_INSTRUCTIONS.keys()))
level  = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"])
hint   = st.text_input("ì§ˆë¬¸ ìƒì„± íŒíŠ¸(ì„ íƒ)", placeholder="ì˜ˆ: ì „í™˜ í¼ë„ / ëª¨ë¸ ì„±ëŠ¥-ë¹„ìš© / ë°ì´í„° í’ˆì§ˆ")

def _similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()
def pick_diverse(cands: list[str], hist: list[str], gamma: float = 0.35) -> str:
    if not cands: return ""
    if not hist:  return random.choice(cands)
    best=None; best_score=1e9
    for q in cands:
        sims=[_similarity(q,h) for h in hist] or [0.0]
        score=(sum(sims)/len(sims)) + gamma*np.std(sims)
        if score < best_score:
            best_score=score; best=q
    return best

if "history" not in st.session_state: st.session_state.history=[]
if "current_question" not in st.session_state: st.session_state.current_question=""

if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", use_container_width=True, type="primary"):
    st.session_state.answer_text = ""
    try:
        news_titles = ", ".join([_snippetize(n["title"], 70) for n in (company or {}).get("news", [])[:3]])
        ctx = textwrap.dedent(f"""
        [íšŒì‚¬ëª…] {(company or {}).get('company_name','')}
        [ëª¨ì§‘ ë¶„ì•¼] {(company or {}).get('role','')}
        [ìµœê·¼ ì´ìŠˆ/ë‰´ìŠ¤] {news_titles}
        """).strip()
        seed = int(time.time()*1000) % 2_147_483_647
        sys = f"""ë„ˆëŠ” '{(company or {}).get('company_name','íšŒì‚¬')}'ì˜ ë©´ì ‘ê´€ì´ë‹¤.
ì»¨í…ìŠ¤íŠ¸/ì´ìŠˆë¥¼ ë°˜ì˜í•˜ì—¬ **{q_type}** ìœ í˜•({TYPE_INSTRUCTIONS[q_type]})ì˜ ì§ˆë¬¸ **6ê°œ**ë¥¼ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ë¼.
ì„œë¡œ í˜•íƒœÂ·ê´€ì Â·í‚¤ì›Œë“œê°€ ë‹¬ë¼ì•¼ í•˜ë©° ë‚œì´ë„ëŠ” {level}. í¬ë§·: 1) ... 2) ... 3) ..."""
        user = f"[ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n\n[íŒíŠ¸]\n{hint}\n[ëœë¤ì‹œë“œ] {seed}"
        resp = client.chat.completions.create(
            model=MODEL, temperature=0.9,
            messages=[{"role":"system","content":sys},{"role":"user","content":user}]
        )
        raw = resp.choices[0].message.content.strip()
        cands = [re.sub(r'^\s*\d+\)\s*','',line).strip() for line in raw.splitlines() if re.match(r'^\s*\d+\)', line)]
        if not cands:
            cands = [l.strip("- ").strip() for l in raw.splitlines() if len(l.strip())>0][:6]
        hist_qs = [h["question"] for h in st.session_state.history][-10:]
        st.session_state.current_question = pick_diverse(cands, hist_qs) or (cands[0] if cands else "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
    except Exception as e:
        st.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")

st.text_area("ì§ˆë¬¸", height=110, value=st.session_state.get("current_question",""))

# =========================================================
# â‘£ ë‚˜ì˜ ë‹µë³€ / ì±„ì  & ì½”ì¹­ (ê¸°ì¡´ ì´ì =ê¸°ì¤€ í•©ì‚° 0~100 ìœ ì§€)
# =========================================================
st.subheader("â‘£ ë‚˜ì˜ ë‹µë³€ / ì½”ì¹­")
ans = st.text_area("ì—¬ê¸°ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš” (STAR ê¶Œì¥: ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)", height=200, key="answer_text")

CRITERIA = ["ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜"]

def evaluate_answer(question: str, answer: str) -> dict:
    schema = {
      "type":"object",
      "properties":{
        "factors":{
          "type":"object",
          "properties":{k:{"type":"object","properties":{
              "score":{"type":"integer","minimum":0,"maximum":20},
              "comment":{"type":"string"},
              "deduct":{"type":"string"},
              "improve":{"type":"string"}
          }, "required":["score"]} for k in CRITERIA},
          "additionalProperties": False
        },
        "revised":{"type":"string"},
        "strengths":{"type":"array","items":{"type":"string"}},
        "risks":{"type":"array","items":{"type":"string"}},
        "improvements":{"type":"array","items":{"type":"string"}}
      },
      "required":["factors","revised"]
    }

    sys = ("ë„ˆëŠ” í†±í‹°ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. ì•„ë˜ ìŠ¤í‚¤ë§ˆì— ë§ì¶˜ **í•œêµ­ì–´ JSONë§Œ** ì¶œë ¥í•˜ë¼. "
           "ê° ê¸°ì¤€(score 0~20)ì„ ì±„ì í•˜ê³  comment/ê°ì (deduct)/ê°œì„ (improve)ì„ ê°„ë‹¨íˆ ì±„ì›Œë¼. "
           f"ìŠ¤í‚¤ë§ˆ: {json.dumps(schema, ensure_ascii=False)}")
    user = f"[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n[í›„ë³´ì ë‹µë³€]\n{answer}"
    resp = client.chat.completions.create(
        model=MODEL, temperature=0.3,
        messages=[{"role":"system","content":sys},{"role":"user","content":user}],
        response_format={"type":"json_object"}
    )
    data = json.loads(resp.choices[0].message.content)

    factors = data.get("factors", {})
    sum_score = sum(int(factors[k]["score"]) for k in CRITERIA if k in factors and isinstance(factors[k].get("score"), int))
    data["sum_score"] = max(0, min(100, sum_score))
    return data

if st.button("ì±„ì  & ì½”ì¹­", type="primary", use_container_width=True):
    if not st.session_state.get("current_question"):
        st.warning("ë¨¼ì € 'ìƒˆ ì§ˆë¬¸ ë°›ê¸°'ë¡œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì±„ì /ì½”ì¹­ ì¤‘..."):
            data = evaluate_answer(st.session_state["current_question"], st.session_state.answer_text)
            row = {k: (data["factors"].get(k,{}).get("score") if data.get("factors") else None) for k in CRITERIA}
            st.session_state.history.append({
                "ts": pd.Timestamp.now(),
                "question": st.session_state["current_question"],
                "answer": st.session_state.answer_text,
                "sum_score": data.get("sum_score"),
                "factors": row,
                "comments": {k: data["factors"].get(k,{}).get("comment") for k in CRITERIA} if data.get("factors") else {},
                "deducts":  {k: data["factors"].get(k,{}).get("deduct") for k in CRITERIA} if data.get("factors") else {},
                "improves": {k: data["factors"].get(k,{}).get("improve") for k in CRITERIA} if data.get("factors") else {},
                "strengths": data.get("strengths",[]),
                "risks": data.get("risks",[]),
                "improvements": data.get("improvements",[]),
                "revised": data.get("revised",""),
                "raw": data
            })

# =========================================================
# â‘¤ ê²°ê³¼/ë ˆì´ë”/CSV â€” ì´ì  ì¼ì›í™”, ìµœì‹  vs í‰ê· , ëˆ„ì í•© í‘œ
# =========================================================
st.divider()
st.subheader("í”¼ë“œë°± ê²°ê³¼")
if st.session_state.history:
    last = st.session_state.history[-1]
    c1,c2 = st.columns([1,3])
    with c1: st.metric("ì´ì (/100)", last.get("sum_score","â€”"))
    with c2:
        st.markdown(f"**ì´ì : {last.get('sum_score','â€”')}/100**")
        st.markdown("**2. ê¸°ì¤€ë³„ ê·¼ê±°(ì ìˆ˜/ê°ì /ê°œì„ ):**")
        rows=[]
        for k in CRITERIA:
            sc = last["factors"].get(k)
            comment = (last.get("comments") or {}).get(k,"")
            deduct  = (last.get("deducts") or {}).get(k,"")
            improve = (last.get("improves") or {}).get(k,"")
            rows.append((f"{k}({sc if sc is not None else '-'}/20)", f"ê°•ì : {comment or '-'} / ê°ì : {deduct or '-'} / ê°œì„ : {improve or '-'}"))
        st.dataframe(pd.DataFrame(rows, columns=["ê¸°ì¤€(ì ìˆ˜)","ì½”ë©˜íŠ¸"]), use_container_width=True, hide_index=True)

        if last.get("strengths"):
            st.markdown("**3. ê°•ì :**\n" + "\n".join([f"- {x}" for x in last["strengths"]]))
        if last.get("risks"):
            st.markdown("**4. ë¦¬ìŠ¤í¬:**\n" + "\n".join([f"- {x}" for x in last["risks"]]))
        if last.get("improvements"):
            st.markdown("**5. ê°œì„  í¬ì¸íŠ¸:**\n" + "\n".join([f"- {x}" for x in last["improvements"]]))
        if last.get("revised"):
            st.markdown("**6. ìˆ˜ì •ë³¸ ë‹µë³€:**")
            st.markdown(last["revised"])
else:
    st.caption("ì•„ì§ ì±„ì  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("ì—­ëŸ‰ ë ˆì´ë” (ì„¸ì…˜ ëˆ„ì , NAëŠ” 0ìœ¼ë¡œ í‘œì‹œ)")

def history_df(hist):
    if not hist: return None
    rows=[]
    for h in hist:
        rows.append([h["factors"].get(k) for k in CRITERIA])
    df = pd.DataFrame(rows, columns=CRITERIA)
    return df

cdf = history_df(st.session_state.history)
if cdf is not None and not cdf.empty:
    latest = cdf.iloc[-1].fillna(0.0).astype(float).tolist()
    avg    = cdf.astype(float).mean(skipna=True).fillna(0.0).tolist()

    if PLOTLY_OK:
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(
            r=latest+[latest[0]], theta=CRITERIA+[CRITERIA[0]],
            fill='toself', name="ìµœì‹ ", opacity=0.7))
        fig.add_trace(go.Scatterpolar(
            r=avg+[avg[0]], theta=CRITERIA+[CRITERIA[0]],
            fill='toself', name="ì„¸ì…˜ í‰ê· ", opacity=0.4))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,20])), showlegend=True, height=440)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.bar_chart(pd.DataFrame({"latest": latest, "avg": avg}, index=CRITERIA))

    # ìµœì‹  ì ìˆ˜ í–‰ + í•©ê³„/ì‹œë„
    table = pd.DataFrame([cdf.iloc[-1].tolist()], columns=CRITERIA)
    table["í•©ê³„(0~100)"] = table[CRITERIA].sum(axis=1, numeric_only=True)
    attempts = len(cdf)
    st.dataframe(table, use_container_width=True)
    st.caption(f"ì‹œë„ íšŸìˆ˜: {attempts}íšŒ")
else:
    st.caption("ì•„ì§ ì—­ëŸ‰ ì ìˆ˜ê°€ íŒŒì‹±ëœ ì½”ì¹­ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV)")
def build_report(hist):
    rows=[]
    for h in hist:
        row={"timestamp":h.get("ts"),"question":h.get("question"),"answer":h.get("answer"),
             "sum_score":h.get("sum_score")}
        for k in CRITERIA:
            row[f"score_{k}"] = (h.get("factors") or {}).get(k)
            row[f"comment_{k}"] = (h.get("comments") or {}).get(k)
            row[f"deduct_{k}"]  = (h.get("deducts") or {}).get(k)
            row[f"improve_{k}"] = (h.get("improves") or {}).get(k)
        row["revised"] = h.get("revised","")
        rows.append(row)
    cols = ["timestamp","question","answer","sum_score"] + \
           [f"score_{k}" for k in CRITERIA] + \
           [f"comment_{k}" for k in CRITERIA] + \
           [f"deduct_{k}" for k in CRITERIA] + \
           [f"improve_{k}" for k in CRITERIA] + ["revised"]
    return pd.DataFrame(rows)[cols] if rows else pd.DataFrame(columns=cols)

rep = build_report(st.session_state.history)
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=rep.to_csv(index=False).encode("utf-8-sig"),
                   file_name="interview_session_report.csv", mime="text/csv")

st.caption("â€» â‘¡ ì„¹ì…˜ì€ ìš”ì²­ì— ë”°ë¼ ê³µê³  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ **ê·¸ëŒ€ë¡œ** ë…¸ì¶œí•©ë‹ˆë‹¤(WebBaseLoader ìš°ì„ , ì‹¤íŒ¨ ì‹œ BS4).")
