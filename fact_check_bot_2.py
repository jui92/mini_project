import os, time, contextlib, tempfile, requests, feedparser, arxiv
from bs4 import BeautifulSoup
from newspaper import Article
import streamlit as st
from openai import OpenAI

from langchain_core.prompts import ChatPromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.docstore.document import Document
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# ======================
# 0) API Key ì•ˆì „ ë¡œë“œ
# ======================
def load_openai_key():
    k = os.getenv("OPENAI_API_KEY")
    if k: return k
    try:
        return st.secrets["OPENAI_API_KEY"]
    except Exception:
        return None

OPENAI_API_KEY = load_openai_key()
if not OPENAI_API_KEY:
    st.error("âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” .streamlit/secrets.tomlì— ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)


# ======================
# 1) ê³µí†µ ìœ í‹¸
# ======================
st.set_page_config(page_title="ë‰´ìŠ¤Â·ë…¼ë¬¸ ê¸°ë°˜ íŒ©íŠ¸ì²´í¬ ì±—ë´‡ (GPT-4o)", page_icon="ğŸ§¾", layout="wide")
st.title("ğŸ§¾ ë‰´ìŠ¤Â·ë…¼ë¬¸ ê¸°ë°˜ íŒ©íŠ¸ì²´í¬ ì±—ë´‡ (GPT-4o)")

@contextlib.contextmanager
def timer(store, key):
    t0 = time.perf_counter(); yield
    store[key] = (time.perf_counter() - t0) * 1000

def split_text(text, chunk_size=2000, overlap=200):
    return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap).split_text(text)

def chat_gpt4o(messages, temperature=0.2):
    resp = client.chat.completions.create(model="gpt-4o", messages=messages, temperature=temperature)
    return resp.choices[0].message.content


# ======================
# 2) ë°ì´í„° ìˆ˜ì§‘
# ======================
def fetch_article_text(url):
    # ìš°ì„  newspaper3kë¡œ ì‹œë„
    try:
        art = Article(url, language='ko'); art.download(); art.parse()
        if len(art.text.strip()) > 400: return art.text
    except Exception: pass
    # fallback
    r = requests.get(url, timeout=15); r.raise_for_status()
    html = BeautifulSoup(r.text, "html.parser")
    return "\n".join(p.get_text(" ", strip=True) for p in html.find_all("p"))

def load_pdf_text(file_bytes):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(file_bytes); path = tmp.name
    pages = PyPDFLoader(path).load_and_split()
    return "\n".join(p.page_content for p in pages)

def fetch_news_rss(keyword, max_items=10):
    rss = f"https://news.google.com/rss/search?q={requests.utils.quote(keyword)}"
    feed = feedparser.parse(rss)
    rows=[]
    for e in feed.entries[:max_items]:
        summary = BeautifulSoup(e.summary, "html.parser").get_text(" ", strip=True)
        rows.append({"title": e.title, "summary": summary, "link": e.link})
    return rows

def fetch_arxiv(keyword, max_items=10):
    search = arxiv.Search(query=keyword, max_results=max_items, sort_by=arxiv.SortCriterion.Relevance)
    return [{"title":r.title,"summary":r.summary,"link":r.entry_id} for r in search.results()]


# ======================
# 3) ìš”ì•½ (Map-Reduce)
# ======================
def summarize_long_text(text, metrics):
    chunks = split_text(text)
    partials=[]
    for ch in chunks:
        partials.append(chat_gpt4o([
            {"role":"system","content":"ë„ˆëŠ” ìš”ì•½ê°€ë‹¤. í•µì‹¬ ì‚¬ì‹¤ë§Œ 3~5ë¬¸ì¥ í•œêµ­ì–´ ìš”ì•½. ê³¼ì¥Â·ì¶”ì¸¡ ê¸ˆì§€."},
            {"role":"user","content":ch}
        ]))
    merged="\n\n".join(partials)
    final = chat_gpt4o([
        {"role":"system","content":"ë„Œ í¸ì§‘ìë‹¤. ì¤‘ë³µ ì œê±°, ìˆ˜ì¹˜Â·ì¸ìš© ìœ ì§€, 5~7ë¬¸ì¥ìœ¼ë¡œ ìµœì¢… í•œêµ­ì–´ ìš”ì•½."},
        {"role":"user","content":merged}
    ])
    return final


# ======================
# 4) ë²¡í„°ìŠ¤í† ì–´ (RAG)
# ======================
EMBED = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
SPLITTER = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)

@st.cache_resource(show_spinner=False)
def build_vectorstore(texts):
    docs=[]
    for t in texts:
        for ch in SPLITTER.split_text(t):
            docs.append(Document(page_content=ch))
    return Chroma.from_documents(docs, EMBED)

RAG_PROMPT = ChatPromptTemplate.from_messages([
    ("system","ë„ˆëŠ” ê·¼ê±° ê¸°ë°˜ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ ë‚´ì—ì„œë§Œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ë¼. "
              "ì—†ìœ¼ë©´ 'ë¬¸ì„œì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤'ë¼ ë§í•˜ë¼. ë‹µë³€ ëì— [ì¶œì²˜ N] í˜•íƒœë¡œ í‘œì‹œ."),
    ("human","ì§ˆë¬¸: {question}\n\nì»¨í…ìŠ¤íŠ¸:\n{context}")
])

def join_ctx(ds):
    return "\n\n".join(f"[ì¶œì²˜ {i+1}] {d.page_content[:1200]}" for i,d in enumerate(ds))

def rag_answer(question, retriever):
    hits = retriever.get_relevant_documents(question)
    ctx = join_ctx(hits)
    msg = RAG_PROMPT.format_messages(question=question, context=ctx)
    res = client.chat.completions.create(model="gpt-4o",
                                         messages=[m.to_dict() for m in msg],
                                         temperature=0.2)
    return res.choices[0].message.content, hits


# ======================
# 5) Streamlit UI
# ======================
with st.sidebar:
    st.markdown("### â± ì„±ëŠ¥")
    perf={}

st.subheader("ì…ë ¥ ë°©ì‹ ì„ íƒ")
mode = st.radio("ëª¨ë“œ", ["ìë™ ìˆ˜ì§‘(ì¶”ì²œ)", "ì§ì ‘ ì…ë ¥(ìˆ˜ë™)"], horizontal=True)

if mode=="ìë™ ìˆ˜ì§‘(ì¶”ì²œ)":
    kw = st.text_input("í‚¤ì›Œë“œ (ì˜ˆ: 'AI regulation', 'ì¸ê³µì§€ëŠ¥ ê·œì œ')")
    n_news = st.slider("ë‰´ìŠ¤ ê°œìˆ˜",1,20,8)
    n_paper= st.slider("ë…¼ë¬¸ ê°œìˆ˜",1,20,8)

    if st.button("ë‰´ìŠ¤Â·ë…¼ë¬¸ ê²€ìƒ‰", type="primary"):
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            news = fetch_news_rss(kw,n_news)
            papers= fetch_arxiv(kw,n_paper)
        st.session_state["news"]=news; st.session_state["papers"]=papers
        st.success(f"ë‰´ìŠ¤ {len(news)}ê°œ, ë…¼ë¬¸ {len(papers)}ê°œ ìˆ˜ì§‘!")

    selected=[]
    if st.session_state.get("news"):
        st.write("**ë‰´ìŠ¤ ì„ íƒ**")
        for i,r in enumerate(st.session_state["news"]):
            if st.checkbox(f"ë‰´ìŠ¤ {i+1}. {r['title']}", key=f"news_{i}"):
                try: selected.append(fetch_article_text(r["link"]))
                except: selected.append(r["summary"])
    if st.session_state.get("papers"):
        st.write("**ë…¼ë¬¸ ì„ íƒ**")
        for i,r in enumerate(st.session_state["papers"]):
            if st.checkbox(f"ë…¼ë¬¸ {i+1}. {r['title']}", key=f"paper_{i}"):
                selected.append(r["summary"])

    if st.button("ìš”ì•½ ìƒì„±", type="primary"):
        if not selected:
            st.warning("ìµœì†Œ 1ê°œ ì„ íƒí•˜ì„¸ìš”.")
        else:
            with st.spinner("ìš”ì•½ ì¤‘..."):
                bundle="\n\n---\n\n".join(selected)
                summary=summarize_long_text(bundle,metrics=perf)
                st.session_state.summary=summary
                st.session_state.texts=selected
                st.session_state.history=[]
                st.success("ìš”ì•½ ì™„ë£Œ!")

else:
    url=st.text_input("ë‰´ìŠ¤/ë¸”ë¡œê·¸ URL")
    pdf=st.file_uploader("PDF ì—…ë¡œë“œ", type=["pdf"])
    free=st.text_area("í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥",height=150)
    if st.button("ìš”ì•½ ìƒì„±", type="primary"):
        raw=""
        if url: raw=fetch_article_text(url)
        elif pdf is not None: raw=load_pdf_text(pdf.getvalue())
        elif free.strip(): raw=free
        else: st.warning("ì…ë ¥ í•„ìš”."); st.stop()
        summary=summarize_long_text(raw,metrics=perf)
        st.session_state.summary=summary
        st.session_state.texts=[raw]
        st.session_state.history=[]
        st.success("ìš”ì•½ ì™„ë£Œ!")

# ---------- ìš”ì•½ ë³´ê¸° ----------
if st.session_state.get("summary"):
    with st.expander("ğŸ“Œ ìš”ì•½ ê²°ê³¼", expanded=True):
        st.write(st.session_state.summary)

# ---------- ì§ˆì˜ì‘ë‹µ ----------
st.subheader("ì§ˆë¬¸Â·ì‘ë‹µ")
if "messages" not in st.session_state:
    st.session_state["messages"]=[{"role":"assistant","content":"ìš”ì•½ì´ ì¤€ë¹„ë˜ë©´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"}]
for m in st.session_state.messages:
    st.chat_message(m["role"]).write(m["content"])

user_q=st.chat_input("ìš”ì•½ì´ë‚˜ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”")
if user_q:
    st.session_state.messages.append({"role":"user","content":user_q})
    st.chat_message("user").write(user_q)
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            # 1) ìš”ì•½ ê¸°ë°˜ 1ì°¨ ë‹µ
            ans = chat_gpt4o([
                {"role":"system","content":"ìš”ì•½ ë‚´ìš©ì— ê·¼ê±°í•´ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ë¼. "
                                           "ì—†ìœ¼ë©´ 'ìš”ì•½ì— ê·¼ê±°ê°€ ì—†ìŠµë‹ˆë‹¤'ë¼ ë§í•˜ë¼."},
                {"role":"system","content":f"ìš”ì•½:\n{st.session_state.summary}"},
                {"role":"user","content":user_q}
            ])
            # 2) í•„ìš”ì‹œ RAG ë°±ì—…
            if ("ê·¼ê±°ê°€ ì—†ìŠµë‹ˆë‹¤" in ans) or (len(ans.strip())<10):
                if "vs" not in st.session_state:
                    st.session_state["vs"]=build_vectorstore(st.session_state.get("texts",[]))
                retriever=st.session_state["vs"].as_retriever(search_kwargs={"k":5})
                ans,_=rag_answer(user_q,retriever)
            st.session_state.messages.append({"role":"assistant","content":ans})
            st.session_state.history=st.session_state.get("history",[])+[
                {"role":"user","content":user_q},
                {"role":"assistant","content":ans}
            ]
            st.write(ans)

with st.sidebar:
    if perf:
        st.markdown("### â± ì²˜ë¦¬ì‹œê°„(ms)")
        for k,v in perf.items():
            st.write(f"- {k}: {v:,.0f} ms")
