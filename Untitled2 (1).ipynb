{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQmmkl0caMzQ"
      },
      "source": [
        "# 뉴스 카테고리 다중분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKlKlpMvaEeX"
      },
      "source": [
        "### 데이터 로드 및 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwanWjgyaBxl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHpta1NYaGT1"
      },
      "source": [
        "#### 훈련 데이터와 테스트 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3uN8N1AaBtc"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)\n",
        "\n",
        "print('훈련 샘플의 수: {}'.format(len(x_train)))\n",
        "print('테스트 샘플의 수: {}'.format(len(x_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b9xyhm9aJKU"
      },
      "source": [
        "#### 데이터 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZCufOJZaBq-"
      },
      "outputs": [],
      "source": [
        "print(x_train[0])\n",
        "print(x_test[0])\n",
        "print(y_train[0])\n",
        "print(y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkqrQFauaBoi"
      },
      "outputs": [],
      "source": [
        "num_classes = max(y_train) + 1\n",
        "print('클래스의 수 : {}'.format(num_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib-m7QBfaYjl"
      },
      "source": [
        "#### 데이터 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re9pkWwDaBmJ"
      },
      "outputs": [],
      "source": [
        "print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))\n",
        "print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train))/len(x_train)))\n",
        "\n",
        "plt.hist([len(s) for s in x_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bT4T0bfaBjt"
      },
      "outputs": [],
      "source": [
        "fig, axe = plt.subplots(ncols=1)\n",
        "fig.set_size_inches(11,5)\n",
        "sns.countplot(x=y_train)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86oqQg0qaBhT"
      },
      "outputs": [],
      "source": [
        "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "print(\"각 클래스 빈도수:\")\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmaJTIEnaeem"
      },
      "source": [
        "#### 원본 뉴스 데이터로 복원\n",
        "실습을 위해 정수 시퀀스로 변환된 데이터를 다시 텍스트로 복원   \n",
        "단어를 key값으로, 고유한 정수를 value로 가지는 dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubQMKz9saBe2"
      },
      "outputs": [],
      "source": [
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "print(word_index['the'])\n",
        "print(word_index['it'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mayXYD-XaBch"
      },
      "outputs": [],
      "source": [
        "index_to_word = {index+3 : word for word, index in word_index.items()}\n",
        "print(index_to_word[4])\n",
        "print(index_to_word[16])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u_qc9-JaBaS"
      },
      "outputs": [],
      "source": [
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "  index_to_word[index]=token\n",
        "print(' '.join([index_to_word[index] for index in x_train[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h86p69fZaBYB"
      },
      "outputs": [],
      "source": [
        "seq = [4, 587, 23, 133, 6, 30, 515]\n",
        "tokens = [index_to_word.get(t, \"<unk>\") for t in seq]\n",
        "text = \" \".join(tokens)\n",
        "print(tokens)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqwOkIK0aBUX"
      },
      "outputs": [],
      "source": [
        "decoded = []\n",
        "for i in range(len(x_train)):\n",
        "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
        "    decoded.append(t)\n",
        "\n",
        "x_train = decoded\n",
        "print(len(x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0Y43nl2aBRn"
      },
      "outputs": [],
      "source": [
        "decoded = []\n",
        "for i in range(len(x_test)):\n",
        "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
        "    decoded.append(t)\n",
        "\n",
        "x_test = decoded\n",
        "print(len(x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yTy9PzFamKu"
      },
      "source": [
        "#### 벡터화 하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMICiondaBPH"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "dtmvector = CountVectorizer()\n",
        "x_train_dtm = dtmvector.fit_transform(x_train)\n",
        "print(x_train_dtm.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ih7e2QRaBMP"
      },
      "outputs": [],
      "source": [
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
        "print(tfidfv.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83TQ-sv2aqu9"
      },
      "source": [
        "### 모델 학습 : 나이브 베이즈 분류기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmWeG5L8aBI3"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btXJLTGtaBGg"
      },
      "outputs": [],
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(tfidfv, y_train)\n",
        "\n",
        "x_test_dtm = dtmvector.transform(x_test)\n",
        "tfidfv_test = tfidf_transformer.transform(x_test_dtm)\n",
        "\n",
        "predicted = model.predict(tfidfv_test)\n",
        "print(\"정확도:\", accuracy_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7mgRAJ6aBEL"
      },
      "outputs": [],
      "source": [
        "print(x_test[3])\n",
        "print(y_test[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDA67GNWax11"
      },
      "outputs": [],
      "source": [
        "probability_3 = model.predict_proba(tfidfv_test[3])[0]\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (11,5)\n",
        "plt.bar(model.classes_, probability_3)\n",
        "plt.xlim(-1, 21)\n",
        "plt.xticks(model.classes_)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Probability\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvKMb2lfaxzg"
      },
      "outputs": [],
      "source": [
        "model.predict(tfidfv_test[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPW--n9Xa2dt"
      },
      "source": [
        "#### F1-Score, Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qEx-6d6axxu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, model.predict(tfidfv_test), zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg1i0PTIaxu7"
      },
      "outputs": [],
      "source": [
        "def graph_confusion_matrix(model, x_test, y_test):#, classes_name):\n",
        "  df_cm = pd.DataFrame(confusion_matrix(y_test, model.predict(x_test)))#, index=classes_name, columns=classes_name)\n",
        "  fig = plt.figure(figsize=(12,12))\n",
        "  heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "  heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=12)\n",
        "  heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=12)\n",
        "  plt.ylabel('label')\n",
        "  plt.xlabel('predicted value')\n",
        "\n",
        "graph_confusion_matrix(model, tfidfv_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndnEh3IOa669"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpVC0S7ha8Rm"
      },
      "source": [
        "### 다양한 머신러닝 모델 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYu844qga9iv"
      },
      "source": [
        "#### Complement Naive Bayes Classifier(CNB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb68Zv4jaBAV"
      },
      "outputs": [],
      "source": [
        "cb = ComplementNB()\n",
        "cb.fit(tfidfv, y_train)\n",
        "\n",
        "predicted = cb.predict(tfidfv_test)\n",
        "print(\"정확도:\", accuracy_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw-5P3xtbAlm"
      },
      "source": [
        "#### 로지스틱 회귀(Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD5RfAS3aA97"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(C=10000, penalty='l2', max_iter=3000)\n",
        "lr.fit(tfidfv, y_train)\n",
        "\n",
        "predicted = lr.predict(tfidfv_test)\n",
        "print(\"정확도:\", accuracy_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uesym_HXbDa-"
      },
      "source": [
        "#### 선형 서포트 벡터 머신(Linear Support Vector Machine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hcdZACaaA7m"
      },
      "outputs": [],
      "source": [
        "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False)\n",
        "lsvc.fit(tfidfv, y_train)\n",
        "\n",
        "predicted = lsvc.predict(tfidfv_test)\n",
        "print(\"정확도:\", accuracy_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZxTc2pkbF1e"
      },
      "source": [
        "#### 결정 트리(Decision Tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGFRYfmFaA4y"
      },
      "outputs": [],
      "source": [
        "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
        "tree.fit(tfidfv, y_train)\n",
        "\n",
        "predicted = tree.predict(tfidfv_test)\n",
        "print(\"정확도:\", accuracy_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDkFTdfLbH2F"
      },
      "source": [
        "#### 랜덤 포레스트(Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfZRTQTMaA1v"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
        "forest.fit(tfidfv, y_train)\n",
        "\n",
        "predicted = forest.predict(tfidfv_test)\n",
        "print(\"정확도:\", accuracy_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J1gHeNnbL_F"
      },
      "source": [
        "#### 그래디언트 부스팅 트리(GradientBoostingClassifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wig9GnXaAzH"
      },
      "outputs": [],
      "source": [
        "grbt = GradientBoostingClassifier(random_state=0)\n",
        "grbt.fit(tfidfv, y_train)\n",
        "\n",
        "predicted = grbt.predict(tfidfv_test)\n",
        "print(\"정확도:\", accuracy_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D4WueNPbOFV"
      },
      "source": [
        "#### 보팅(Voting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhdJnd_2bPVn"
      },
      "outputs": [],
      "source": [
        "X_train_dense = tfidfv.toarray().astype(np.float32, copy=False)\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[(\"lr\", LogisticRegression(penalty=\"l2\", solver=\"liblinear\", random_state=0)),\n",
        "                (\"cnb\", ComplementNB()),\n",
        "                (\"gb\", GradientBoostingClassifier(random_state=0))],voting=\"soft\")\n",
        "voting_classifier.fit(X_train_dense, y_train)\n",
        "\n",
        "predicted = voting_classifier.predict(tfidfv_test)\n",
        "print(\"정확도:\", accuracy_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOoRDgLGbRyV"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY9tBqUREUiT"
      },
      "source": [
        "## Vocabulary Size X 다양한 머신러닝 모델\n",
        "- Vocabulary Size : 5000, 3000, None\n",
        "- 비교 모델\n",
        "  - MultinomialNB, ComplementNB\n",
        "  - LogisticRegression, LinearSVC\n",
        "  - DecisionTreeClassifier, RandomForestClassifier\n",
        "  - GradientBoostingClassifier\n",
        "  - Voting(soft/hard)  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WFxA6XIdEBgi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings, os, random\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WSGL1pYKEBeU"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# 인덱스 → 단어 복원\n",
        "word_index = reuters.get_word_index()\n",
        "index_to_word = {idx + 3: w for w, idx in word_index.items()}\n",
        "index_to_word[0], index_to_word[1], index_to_word[2] = \"<pad>\", \"<sos>\", \"<unk>\"\n",
        "SPECIALS = {0, 1, 2}\n",
        "\n",
        "# 정수 시퀀스를 공백으로 join한 텍스트로 변환\n",
        "def decode_sequences(seqs, drop_special=True):\n",
        "    out = []\n",
        "    for s in seqs:\n",
        "        toks = []\n",
        "        for t in s:\n",
        "            if drop_special and t in SPECIALS:\n",
        "                continue\n",
        "            toks.append(index_to_word.get(t, \"<unk>\"))\n",
        "        out.append(\" \".join(toks))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SyF1MloHoya"
      },
      "source": [
        "- 학습 시간 문제로 코드 수정\n",
        "  - GradientBoostingClassifier 제외\n",
        "  - Voting - hard 제외, soft만 유지  \n",
        "  - LogisticRegression - max_iter 축소, tol 추가\n",
        "  - RandomForest - n_estimators 축소"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4D7Xskc6EBb6"
      },
      "outputs": [],
      "source": [
        "# 모델 파이프라인 정의\n",
        "# - LR, NB, SVM, DT, RF 는 희소 입력 OK → identity\n",
        "# - GB 는 희소 입력 X → densify로 dense 변환 후 적용\n",
        "\n",
        "def build_models(seed=SEED):\n",
        "    identity = FunctionTransformer(lambda X: X, accept_sparse=True)\n",
        "    # densify = FunctionTransformer(lambda X: X.toarray(), accept_sparse=True)\n",
        "\n",
        "    models = {\"NB_Multinomial\": make_pipeline(identity, MultinomialNB()),\n",
        "              \"CNB\": make_pipeline(identity, ComplementNB()),\n",
        "              \"LogReg_L2\": make_pipeline(identity, LogisticRegression(penalty=\"l2\", solver=\"saga\",\n",
        "                                                                      multi_class=\"multinomial\",\n",
        "                                                                      max_iter=1000, tol=1e-3,\n",
        "                                                                      random_state=seed, n_jobs=-1)),\n",
        "              \"LinearSVM\": make_pipeline(identity, LinearSVC(random_state=seed)),\n",
        "              \"DecisionTree\": make_pipeline(identity, DecisionTreeClassifier(random_state=seed)),\n",
        "              \"RandomForest\": make_pipeline(identity, RandomForestClassifier(n_estimators=300,\n",
        "                                                                             random_state=seed,\n",
        "                                                                             n_jobs=-1))}\n",
        "            #   # GradientBoosting은 희소 입력 미지원 → dense 변환\n",
        "            #   \"GradBoost\": make_pipeline(densify, GradientBoostingClassifier(random_state=seed))}\n",
        "\n",
        "    # Voting(soft): 확률 가능한 모델만 (LR, CNB, GB)\n",
        "    voting_soft = VotingClassifier(estimators=[(\"lr\", models[\"LogReg_L2\"]),\n",
        "                                               (\"cnb\", models[\"CNB\"]),\n",
        "                                               (\"nb\", models[\"NB_Multinomial\"])],voting=\"soft\")\n",
        "                                               #(\"gb\", models[\"GradBoost\"])],voting=\"soft\")\n",
        "    # # Voting(hard): 7개 전부\n",
        "    # voting_hard = VotingClassifier(estimators=[(\"nb\", models[\"NB_Multinomial\"]),\n",
        "    #                                            (\"cnb\", models[\"CNB\"]),\n",
        "    #                                            (\"lr\", models[\"LogReg_L2\"]),\n",
        "    #                                            (\"svm\", models[\"LinearSVM\"]),\n",
        "    #                                            (\"dt\", models[\"DecisionTree\"]),\n",
        "    #                                            (\"rf\", models[\"RandomForest\"]),\n",
        "    #                                            (\"gb\", models[\"GradBoost\"])],voting=\"hard\")\n",
        "    return models, voting_soft #,voting_hard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r7fHFBWHEBZy"
      },
      "outputs": [],
      "source": [
        "# 학습/평가 함수\n",
        "def eval_model(name, clf, Xtr, ytr, Xte, yte):\n",
        "    clf.fit(Xtr, ytr)\n",
        "    y_pred = clf.predict(Xte)\n",
        "    return {\"model\": name,\n",
        "            \"acc\": accuracy_score(yte, y_pred),\n",
        "            \"balanced_acc\": balanced_accuracy_score(yte, y_pred),\n",
        "            \"macro_f1\": f1_score(yte, y_pred, average=\"macro\"),\n",
        "            \"y_pred\": y_pred}\n",
        "\n",
        "def confusion_summary(y_true, y_pred, topk=10, all_labels=None):\n",
        "    if all_labels is None:\n",
        "        all_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
        "    cm_sum = cm.sum(axis=1, keepdims=True)\n",
        "    cm_norm = np.divide(cm, cm_sum, out=np.zeros_like(cm, dtype=float), where=cm_sum!=0)\n",
        "    cm_off = cm_norm.copy()\n",
        "    np.fill_diagonal(cm_off, 0.0)\n",
        "    pairs = np.dstack(np.unravel_index(np.argsort(cm_off.ravel())[::-1], cm_off.shape))[0][:topk]\n",
        "    return cm, cm_norm, pairs, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3wdt7nZ5EBXq"
      },
      "outputs": [],
      "source": [
        "# 실행 함수(num_words, vec_mode만 바꿔 호출)\n",
        "def run_once(num_words, vec_mode=\"tfidf\", topk_pairs=10, seed=SEED):\n",
        "    # 데이터 로드\n",
        "    (xtr_ids, ytr), (xte_ids, yte) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
        "\n",
        "    # 텍스트 복원\n",
        "    xtr_text = decode_sequences(xtr_ids, drop_special=True)\n",
        "    xte_text = decode_sequences(xte_ids, drop_special=True)\n",
        "\n",
        "    # 벡터화\n",
        "    if vec_mode == \"tfidf\":\n",
        "        vectorizer = TfidfVectorizer(ngram_range=(1,1), min_df=2, max_df=0.95, sublinear_tf=True)\n",
        "    else:\n",
        "        vectorizer = CountVectorizer(ngram_range=(1,1), min_df=2, max_df=0.95)\n",
        "\n",
        "    Xtr = vectorizer.fit_transform(xtr_text)\n",
        "    Xte = vectorizer.transform(xte_text)\n",
        "\n",
        "    # 모델 구성\n",
        "    # models, voting_soft, voting_hard = build_models(seed=seed)\n",
        "    models, voting_soft = build_models(seed=seed)\n",
        "\n",
        "    # 학습 & 평가\n",
        "    results = []\n",
        "    for name, clf in models.items():\n",
        "        results.append(eval_model(name, clf, Xtr, ytr, Xte, yte))\n",
        "    results.append(eval_model(\"Voting_soft(LR+CNB+NB)\", voting_soft, Xtr, ytr, Xte, yte))\n",
        "    # results.append(eval_model(\"Voting_hard(All)\", voting_hard, Xtr, ytr, Xte, yte))\n",
        "\n",
        "    # 요약 표 출력\n",
        "    df_res = (pd.DataFrame(results).sort_values(\"macro_f1\", ascending=False).reset_index(drop=True))\n",
        "\n",
        "    print(f\"[num_words={num_words} | vec={vec_mode}] 결과 (Macro-F1 기준 내림차순)\")\n",
        "    print(df_res[[\"model\", \"acc\", \"balanced_acc\", \"macro_f1\"]].to_string(index=False))\n",
        "\n",
        "    # 혼동행렬 요약 (베스트 1개)\n",
        "    best = max(results, key=lambda d: d[\"macro_f1\"])\n",
        "    cm, cm_norm, pairs, labels = confusion_summary(yte, best[\"y_pred\"], topk=topk_pairs)\n",
        "\n",
        "    print(f\"\\n[Best by macro-F1] {best['model']}의 혼동행렬 요약\")\n",
        "    print(f\"- Confusion matrix shape: {cm.shape} (rows=true, cols=predicted)\")\n",
        "    print(f\"\\nTop-{topk_pairs} confused pairs (true -> pred: rate, count):\")\n",
        "\n",
        "    for i, j in pairs:\n",
        "        count = cm[i, j]\n",
        "        if count > 0:\n",
        "            print(f\"  {labels[i]} -> {labels[j]}: rate={cm_norm[i, j]:.3f}, count={count}\")\n",
        "\n",
        "    return df_res, results, (cm, cm_norm, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7CLgCSzEBVq",
        "outputId": "c825524a-a78d-4571-a612-abef4eac805e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[num_words=5000 | vec=tfidf] 결과 (Macro-F1 기준 내림차순)\n",
            "                 model      acc  balanced_acc  macro_f1\n",
            "             LinearSVM 0.832146      0.634498  0.675920\n",
            "          RandomForest 0.773375      0.418554  0.473705\n",
            "          DecisionTree 0.703473      0.445195  0.447408\n",
            "                   CNB 0.763580      0.404732  0.441942\n",
            "             LogReg_L2 0.798753      0.394181  0.432609\n",
            "Voting_soft(LR+CNB+GB) 0.732413      0.217712  0.247759\n",
            "        NB_Multinomial 0.678094      0.122976  0.113883\n",
            "\n",
            "[Best by macro-F1] LinearSVM의 혼동행렬 요약\n",
            "- Confusion matrix shape: (46, 46) (rows=true, cols=predicted)\n",
            "\n",
            "Top-10 confused pairs (true -> pred: rate, count):\n",
            "  5 -> 1: rate=1.000, count=5\n",
            "  40 -> 19: rate=0.700, count=7\n",
            "  42 -> 25: rate=0.667, count=2\n",
            "  37 -> 4: rate=0.500, count=1\n",
            "  41 -> 4: rate=0.500, count=4\n",
            "  14 -> 11: rate=0.500, count=1\n",
            "  36 -> 11: rate=0.455, count=5\n",
            "  15 -> 28: rate=0.444, count=4\n",
            "  38 -> 1: rate=0.333, count=1\n",
            "  35 -> 11: rate=0.333, count=2\n"
          ]
        }
      ],
      "source": [
        "# num_words=5000\n",
        "df_5000, results_5000, cm_pack_5000 = run_once(num_words=5000, vec_mode=\"tfidf\", topk_pairs=10, seed=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ7NCnIYEBTS",
        "outputId": "d2d64f15-c131-4db9-ad8e-3abc4340f80c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[num_words=3000 | vec=tfidf] 결과 (Macro-F1 기준 내림차순)\n",
            "                 model      acc  balanced_acc  macro_f1\n",
            "             LinearSVM 0.834372      0.637980  0.682639\n",
            "          RandomForest 0.782725      0.442724  0.491461\n",
            "             LogReg_L2 0.802760      0.423390  0.467768\n",
            "          DecisionTree 0.695459      0.452602  0.448998\n",
            "                   CNB 0.756901      0.404111  0.438829\n",
            "Voting_soft(LR+CNB+GB) 0.750223      0.268574  0.298092\n",
            "        NB_Multinomial 0.695013      0.157038  0.163511\n",
            "\n",
            "[Best by macro-F1] LinearSVM의 혼동행렬 요약\n",
            "- Confusion matrix shape: (46, 46) (rows=true, cols=predicted)\n",
            "\n",
            "Top-10 confused pairs (true -> pred: rate, count):\n",
            "  5 -> 1: rate=1.000, count=5\n",
            "  40 -> 19: rate=0.700, count=7\n",
            "  41 -> 4: rate=0.500, count=4\n",
            "  14 -> 11: rate=0.500, count=1\n",
            "  37 -> 4: rate=0.500, count=1\n",
            "  36 -> 11: rate=0.455, count=5\n",
            "  15 -> 28: rate=0.333, count=3\n",
            "  38 -> 1: rate=0.333, count=1\n",
            "  38 -> 16: rate=0.333, count=1\n",
            "  35 -> 11: rate=0.333, count=2\n"
          ]
        }
      ],
      "source": [
        "# num_words=3000\n",
        "df_3000, results_3000, cm_pack_3000 = run_once(num_words=3000, vec_mode=\"tfidf\", topk_pairs=10, seed=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iskw8mcNEBQ1",
        "outputId": "b3b36163-7152-4aff-864e-3b78f42200a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[num_words=None | vec=tfidf] 결과 (Macro-F1 기준 내림차순)\n",
            "                 model      acc  balanced_acc  macro_f1\n",
            "             LinearSVM 0.831256      0.631417  0.667421\n",
            "          DecisionTree 0.707035      0.471876  0.461522\n",
            "                   CNB 0.758682      0.408033  0.454552\n",
            "          RandomForest 0.761799      0.390794  0.447817\n",
            "             LogReg_L2 0.788958      0.340861  0.373389\n",
            "Voting_soft(LR+CNB+GB) 0.684773      0.137222  0.155720\n",
            "        NB_Multinomial 0.632235      0.086281  0.084486\n",
            "\n",
            "[Best by macro-F1] LinearSVM의 혼동행렬 요약\n",
            "- Confusion matrix shape: (46, 46) (rows=true, cols=predicted)\n",
            "\n",
            "Top-10 confused pairs (true -> pred: rate, count):\n",
            "  5 -> 1: rate=1.000, count=5\n",
            "  42 -> 25: rate=0.667, count=2\n",
            "  40 -> 19: rate=0.600, count=6\n",
            "  37 -> 4: rate=0.500, count=1\n",
            "  41 -> 4: rate=0.500, count=4\n",
            "  14 -> 11: rate=0.500, count=1\n",
            "  36 -> 11: rate=0.455, count=5\n",
            "  15 -> 28: rate=0.444, count=4\n",
            "  17 -> 16: rate=0.417, count=5\n",
            "  35 -> 11: rate=0.333, count=2\n"
          ]
        }
      ],
      "source": [
        "# num_words=None\n",
        "df_None, results_None, cm_pack_None = run_once(num_words=None, vec_mode=\"tfidf\", topk_pairs=10, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi5LwMk_JMqM"
      },
      "source": [
        "- 모델 비교\n",
        "  - LinearSVM\n",
        "    - 고차원 희소 벡터(BoW/TF-IDF)에 강한 선형 분류기\n",
        "    - accuracy(0.83±) 대비 balanced_acc(0.63±)·macro-F1(0.67~0.68) 간 격차는 클래스 불균형이 원인으로 추정   \n",
        "     다수 클래스는 잘 맞추지만 소수 클래스 리콜은 떨어진다는 신호\n",
        "  - LogisticRegression\n",
        "    - 정확도는 준수(0.79~0.80)하지만 macro-F1은 SVM보다 낮음\n",
        "      - 동일한 선형 계열이라도 정규화/최적화 차이와 다중클래스 로스(softmax)의 특성상 소수 클래스에서 결정경계가 조금 더 보수적으로 형성되기 쉬움\n",
        "      - 기본 임계값(0.5 argmax)로는 소수 클래스 리콜을 끌어올리기 어려움 → class_weight='balanced', C 튜닝으로 개선 여지\n",
        "  - Naive Bayes (Multinomial/Complement)\n",
        "    - 빠르고 투박: 다수 클래스에 유리, 소수 클래스 리콜이 낮아 macro-F1이 낮음\n",
        "    - 결과에서 볼 수 있듯이 ComplementNB가 MultinomialNB보다 일반적으로 불균형에 더 강함\n",
        "  - RandomForest / DecisionTree\n",
        "    - 희소·고차원에서 트리 분기는 정보이득이 분산되어 성능/균형 정확도 낮음(macro-F1 0.44~0.49)\n",
        "    - RF가 DT보다 낫지만 여전히 선형 모델 대비 불리\n",
        "  - Voting_soft\n",
        "    - LR+CNB+NB라서 약한 NB 2개 + LR 1개 조합이 되어 LR 단독보다 못함\n",
        "    - 확률교정/가중치 부재: LR만 강하고 NB는 약하니 평균하면 신호가 희석\n",
        "\n",
        "- 단어 수(num_words)에 따른 경향\n",
        "  - 3000 > 5000 ≥ None (macro-F1 기준)\n",
        "    - 3000: 희소성과 노이즈의 균형이 가장 좋음 → 최고 macro-F1(0.6826)\n",
        "    - 5000: 약간의 노이즈 증가로 소수 클래스 리콜이 아주 약간 희생\n",
        "    - None(전체): 희귀 토큰 과다 → 분산 증가, 소수 클래스에서 오히려 성능 하락\n",
        "\n",
        "- 혼동행렬 Top-10\n",
        "  - 공통적으로 5→1, 40→19, 41→4, 36→11, 15→28 등이 자주 등장\n",
        "    - 테스트 표본 수가 적거나, 어휘가 유사해 결정경계가 잘 안 갈라질 가능성이 높음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USkYzfDhD3Tm"
      },
      "source": [
        "## Vocabulary Size X 딥러닝 모델\n",
        "- Vocabulary Size : 3000, 5000, 10000\n",
        "- 비교 모델 : cnn, bilstm\n",
        "- 최대 시퀀스 길이 : 200, 300, 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tqueAn-2D-3e"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "EPOCHS = 12\n",
        "BATCH_SIZE = 128\n",
        "EMBED_DIM = 128\n",
        "\n",
        "NUM_WORDS_LIST = [3000, 5000, 10000]\n",
        "USE_MODELS = [\"cnn\", \"bilstm\"]\n",
        "MAXLEN_LIST = [200, 300, 500]\n",
        "PRINT_CONFUSION_PER_RUN = False  # Top-10 confused pairs 출력 여부\n",
        "\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ib-OHl_lD-51"
      },
      "outputs": [],
      "source": [
        "# DL models\n",
        "def build_cnn(vocab_size, maxlen, embed_dim=128, filters=128, ksz=5, dp=0.3, classes=46):\n",
        "    inp = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "    x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(inp)\n",
        "    x = layers.Conv1D(filters, ksz, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(dp)(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    out = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_bilstm(vocab_size, maxlen, embed_dim=128, units=64, dp=0.3, classes=46):\n",
        "    inp = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "    x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(inp)\n",
        "    x = layers.Bidirectional(layers.LSTM(units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(dp)(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    out = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iR2kU22GPnLo"
      },
      "outputs": [],
      "source": [
        "# Index -> word decode\n",
        "word_index = reuters.get_word_index()\n",
        "index_to_word = {idx + 3: w for w, idx in word_index.items()}\n",
        "index_to_word[0], index_to_word[1], index_to_word[2] = \"<pad>\", \"<sos>\", \"<unk>\"\n",
        "SPECIALS = {0, 1, 2}\n",
        "\n",
        "def decode_sequences(seqs, drop_special=True):\n",
        "    out = []\n",
        "    for s in seqs:\n",
        "        toks = [index_to_word.get(t, \"<unk>\") for t in s if (not drop_special or t not in SPECIALS)]\n",
        "        out.append(\" \".join(toks))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5Z9rREB3PwSY"
      },
      "outputs": [],
      "source": [
        "# Confusion summary (Top-K) + formatter\n",
        "def confusion_summary(y_true, y_pred, topk=10, all_labels=None):\n",
        "    if all_labels is None:\n",
        "        all_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
        "    cm_sum = cm.sum(axis=1, keepdims=True)\n",
        "    cm_norm = np.divide(cm, cm_sum, out=np.zeros_like(cm, dtype=float), where=cm_sum!=0)\n",
        "    cm_off = cm_norm.copy()\n",
        "    np.fill_diagonal(cm_off, 0.0)\n",
        "    pairs = np.dstack(np.unravel_index(np.argsort(cm_off.ravel())[::-1], cm_off.shape))[0][:topk]\n",
        "    return cm, cm_norm, pairs, all_labels\n",
        "\n",
        "def format_pairs(cm, cm_norm, pairs, labels):\n",
        "    lines = []\n",
        "    for i, j in pairs:\n",
        "        count = cm[i, j]\n",
        "        if count > 0:\n",
        "            lines.append(f\"{labels[i]} -> {labels[j]}: rate={cm_norm[i, j]:.3f}, count={count}\")\n",
        "    if not lines:\n",
        "        lines = [\"(no off-diagonal confusions)\"]\n",
        "    return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "56Qk-LA-D-8H"
      },
      "outputs": [],
      "source": [
        "# 실행 함수\n",
        "def run_one(num_words, use_model, maxlen):\n",
        "    # Load data\n",
        "    (xtr_ids, ytr), (xte_ids, yte) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
        "\n",
        "    # Pad for DL models\n",
        "    Xtr_seq = pad_sequences(xtr_ids, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
        "    Xte_seq = pad_sequences(xte_ids, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    # vocab_size (Embedding 용)\n",
        "    max_index = 0\n",
        "    for s in list(xtr_ids) + list(xte_ids):\n",
        "        if len(s): max_index = max(max_index, max(s))\n",
        "    vocab_size = max_index + 1\n",
        "    n_classes = int(max(max(ytr), max(yte)) + 1)\n",
        "\n",
        "    # DL 모델 선택/학습\n",
        "    if use_model == \"cnn\":\n",
        "        dl_model = build_cnn(vocab_size, maxlen, EMBED_DIM, classes=n_classes)\n",
        "    elif use_model == \"bilstm\":\n",
        "        dl_model = build_bilstm(vocab_size, maxlen, EMBED_DIM, classes=n_classes)\n",
        "    else:\n",
        "        raise ValueError(\"USE_MODEL must be 'cnn' or 'bilstm'.\")\n",
        "\n",
        "    es = callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=2, restore_best_weights=True)\n",
        "    rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-5)\n",
        "\n",
        "    dl_model.fit(Xtr_seq, ytr, validation_split=0.1, epochs=EPOCHS,\n",
        "                 batch_size=BATCH_SIZE, callbacks=[es, rlr], verbose=0)\n",
        "\n",
        "    y_prob_dl = dl_model.predict(Xte_seq, verbose=0)\n",
        "    y_pred_dl = np.argmax(y_prob_dl, axis=1)\n",
        "    acc_dl = accuracy_score(yte, y_pred_dl)\n",
        "    bacc_dl = balanced_accuracy_score(yte, y_pred_dl)\n",
        "    f1m_dl = f1_score(yte, y_pred_dl, average=\"macro\")\n",
        "\n",
        "    # Classical baseline: TF-IDF + Logistic Regression (동일 num_words의 텍스트)\n",
        "    xtr_text = decode_sequences(xtr_ids, drop_special=True)\n",
        "    xte_text = decode_sequences(xte_ids, drop_special=True)\n",
        "\n",
        "    tfidf = TfidfVectorizer(ngram_range=(1,1), min_df=2, max_df=0.95, sublinear_tf=True)\n",
        "    Xtr_tfidf = tfidf.fit_transform(xtr_text)\n",
        "    Xte_tfidf = tfidf.transform(xte_text)\n",
        "\n",
        "    lr = LogisticRegression(penalty=\"l2\", solver=\"saga\", max_iter=2000, random_state=SEED)\n",
        "    lr.fit(Xtr_tfidf, ytr)\n",
        "    y_pred_lr = lr.predict(Xte_tfidf)\n",
        "\n",
        "    acc_lr = accuracy_score(yte, y_pred_lr)\n",
        "    bacc_lr = balanced_accuracy_score(yte, y_pred_lr)\n",
        "    f1m_lr = f1_score(yte, y_pred_lr, average=\"macro\")\n",
        "\n",
        "    # Confusion Top-10 (DL / LR)\n",
        "    cm_dl, cmn_dl, pairs_dl, labels_dl = confusion_summary(yte, y_pred_dl, topk=10)\n",
        "    cm_lr, cmn_lr, pairs_lr, labels_lr = confusion_summary(yte, y_pred_lr, topk=10)\n",
        "    lines_dl = format_pairs(cm_dl, cmn_dl, pairs_dl, labels_dl)\n",
        "    lines_lr = format_pairs(cm_lr, cmn_lr, pairs_lr, labels_lr)\n",
        "\n",
        "    # Compact per-run print\n",
        "    print(f\"\\n=== num_words={num_words}, use_model={use_model}, maxlen={maxlen} ===\")\n",
        "    print(f\"[DL-{use_model}] acc={acc_dl:.6f}  balanced_acc={bacc_dl:.6f}  macro_f1={f1m_dl:.6f}\")\n",
        "    if PRINT_CONFUSION_PER_RUN:\n",
        "        print(\"Top-10 confused pairs (DL):\")\n",
        "        for ln in lines_dl: print(ln)\n",
        "\n",
        "    print(f\"[TFIDF+LR] acc={acc_lr:.6f}  balanced_acc={bacc_lr:.6f}  macro_f1={f1m_lr:.6f}\")\n",
        "    if PRINT_CONFUSION_PER_RUN:\n",
        "        print(\"Top-10 confused pairs (LR):\")\n",
        "        for ln in lines_lr: print(ln)\n",
        "\n",
        "    return {\"num_words\": num_words, \"use_model\": use_model, \"maxlen\": maxlen,\n",
        "            \"dl_acc\": acc_dl, \"dl_bal_acc\": bacc_dl, \"dl_macro_f1\": f1m_dl,\n",
        "            \"lr_acc\": acc_lr, \"lr_bal_acc\": bacc_lr, \"lr_macro_f1\": f1m_lr,\n",
        "            \"delta_macro_f1\": f1m_dl - f1m_lr,\n",
        "            \"_y_true\": yte, \"_y_pred_dl\": y_pred_dl, \"_y_pred_lr\": y_pred_lr}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pEixObTD--g",
        "outputId": "47311b75-cc9d-4e85-aced-f6419060162a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== num_words=3000, use_model=cnn, maxlen=200 ===\n",
            "[DL-cnn] acc=0.799644  balanced_acc=0.462937  macro_f1=0.481896\n",
            "[TFIDF+LR] acc=0.801870  balanced_acc=0.416325  macro_f1=0.455547\n",
            "\n",
            "=== num_words=3000, use_model=cnn, maxlen=300 ===\n",
            "[DL-cnn] acc=0.805432  balanced_acc=0.488752  macro_f1=0.523166\n",
            "[TFIDF+LR] acc=0.801870  balanced_acc=0.416325  macro_f1=0.455547\n",
            "\n",
            "=== num_words=3000, use_model=cnn, maxlen=500 ===\n",
            "[DL-cnn] acc=0.803206  balanced_acc=0.477127  macro_f1=0.507487\n",
            "[TFIDF+LR] acc=0.801870  balanced_acc=0.416325  macro_f1=0.455547\n",
            "\n",
            "=== num_words=3000, use_model=bilstm, maxlen=200 ===\n",
            "[DL-bilstm] acc=0.754230  balanced_acc=0.247190  macro_f1=0.222489\n",
            "[TFIDF+LR] acc=0.801870  balanced_acc=0.416325  macro_f1=0.455547\n",
            "\n",
            "=== num_words=3000, use_model=bilstm, maxlen=300 ===\n",
            "[DL-bilstm] acc=0.775601  balanced_acc=0.300459  macro_f1=0.280814\n",
            "[TFIDF+LR] acc=0.801870  balanced_acc=0.416325  macro_f1=0.455547\n",
            "\n",
            "=== num_words=3000, use_model=bilstm, maxlen=500 ===\n",
            "[DL-bilstm] acc=0.763134  balanced_acc=0.288500  macro_f1=0.264864\n",
            "[TFIDF+LR] acc=0.801870  balanced_acc=0.416325  macro_f1=0.455547\n",
            "\n",
            "=== num_words=5000, use_model=cnn, maxlen=200 ===\n",
            "[DL-cnn] acc=0.807658  balanced_acc=0.492885  macro_f1=0.512139\n",
            "[TFIDF+LR] acc=0.797863  balanced_acc=0.390956  macro_f1=0.428748\n",
            "\n",
            "=== num_words=5000, use_model=cnn, maxlen=300 ===\n",
            "[DL-cnn] acc=0.805432  balanced_acc=0.502969  macro_f1=0.526109\n",
            "[TFIDF+LR] acc=0.797863  balanced_acc=0.390956  macro_f1=0.428748\n",
            "\n",
            "=== num_words=5000, use_model=cnn, maxlen=500 ===\n",
            "[DL-cnn] acc=0.802760  balanced_acc=0.503527  macro_f1=0.540072\n",
            "[TFIDF+LR] acc=0.797863  balanced_acc=0.390956  macro_f1=0.428748\n",
            "\n",
            "=== num_words=5000, use_model=bilstm, maxlen=200 ===\n",
            "[DL-bilstm] acc=0.768477  balanced_acc=0.301531  macro_f1=0.284541\n",
            "[TFIDF+LR] acc=0.797863  balanced_acc=0.390956  macro_f1=0.428748\n",
            "\n",
            "=== num_words=5000, use_model=bilstm, maxlen=300 ===\n",
            "[DL-bilstm] acc=0.756011  balanced_acc=0.252705  macro_f1=0.229769\n",
            "[TFIDF+LR] acc=0.797863  balanced_acc=0.390956  macro_f1=0.428748\n",
            "\n",
            "=== num_words=5000, use_model=bilstm, maxlen=500 ===\n",
            "[DL-bilstm] acc=0.767142  balanced_acc=0.267667  macro_f1=0.251896\n",
            "[TFIDF+LR] acc=0.797863  balanced_acc=0.390956  macro_f1=0.428748\n",
            "\n",
            "=== num_words=10000, use_model=cnn, maxlen=200 ===\n",
            "[DL-cnn] acc=0.802760  balanced_acc=0.467776  macro_f1=0.497142\n",
            "[TFIDF+LR] acc=0.792075  balanced_acc=0.354759  macro_f1=0.394817\n",
            "\n",
            "=== num_words=10000, use_model=cnn, maxlen=300 ===\n",
            "[DL-cnn] acc=0.799644  balanced_acc=0.485879  macro_f1=0.518660\n",
            "[TFIDF+LR] acc=0.792075  balanced_acc=0.354759  macro_f1=0.394817\n",
            "\n",
            "=== num_words=10000, use_model=cnn, maxlen=500 ===\n",
            "[DL-cnn] acc=0.789849  balanced_acc=0.412407  macro_f1=0.441444\n",
            "[TFIDF+LR] acc=0.792075  balanced_acc=0.354759  macro_f1=0.394817\n",
            "\n",
            "=== num_words=10000, use_model=bilstm, maxlen=200 ===\n",
            "[DL-bilstm] acc=0.772930  balanced_acc=0.319841  macro_f1=0.306255\n",
            "[TFIDF+LR] acc=0.792075  balanced_acc=0.354759  macro_f1=0.394817\n",
            "\n",
            "=== num_words=10000, use_model=bilstm, maxlen=300 ===\n",
            "[DL-bilstm] acc=0.772484  balanced_acc=0.328163  macro_f1=0.319491\n",
            "[TFIDF+LR] acc=0.792075  balanced_acc=0.354759  macro_f1=0.394817\n",
            "\n",
            "=== num_words=10000, use_model=bilstm, maxlen=500 ===\n",
            "[DL-bilstm] acc=0.755565  balanced_acc=0.289932  macro_f1=0.282403\n",
            "[TFIDF+LR] acc=0.792075  balanced_acc=0.354759  macro_f1=0.394817\n",
            "\n",
            "===== Summary (DL vs TFIDF+LR) — Macro-F1 기준 내림차순 (DL 우선) =====\n",
            " num_words use_model  maxlen   dl_acc  dl_bal_acc  dl_macro_f1   lr_acc  lr_bal_acc  lr_macro_f1  delta_macro_f1\n",
            "      5000       cnn     500 0.802760    0.503527     0.540072 0.797863    0.390956     0.428748        0.111323\n",
            "      5000       cnn     300 0.805432    0.502969     0.526109 0.797863    0.390956     0.428748        0.097360\n",
            "      3000       cnn     300 0.805432    0.488752     0.523166 0.801870    0.416325     0.455547        0.067619\n",
            "     10000       cnn     300 0.799644    0.485879     0.518660 0.792075    0.354759     0.394817        0.123842\n",
            "      5000       cnn     200 0.807658    0.492885     0.512139 0.797863    0.390956     0.428748        0.083391\n",
            "      3000       cnn     500 0.803206    0.477127     0.507487 0.801870    0.416325     0.455547        0.051941\n",
            "     10000       cnn     200 0.802760    0.467776     0.497142 0.792075    0.354759     0.394817        0.102325\n",
            "      3000       cnn     200 0.799644    0.462937     0.481896 0.801870    0.416325     0.455547        0.026350\n",
            "     10000       cnn     500 0.789849    0.412407     0.441444 0.792075    0.354759     0.394817        0.046626\n",
            "     10000    bilstm     300 0.772484    0.328163     0.319491 0.792075    0.354759     0.394817       -0.075326\n",
            "     10000    bilstm     200 0.772930    0.319841     0.306255 0.792075    0.354759     0.394817       -0.088562\n",
            "      5000    bilstm     200 0.768477    0.301531     0.284541 0.797863    0.390956     0.428748       -0.144207\n",
            "     10000    bilstm     500 0.755565    0.289932     0.282403 0.792075    0.354759     0.394817       -0.112414\n",
            "      3000    bilstm     300 0.775601    0.300459     0.280814 0.801870    0.416325     0.455547       -0.174733\n",
            "      3000    bilstm     500 0.763134    0.288500     0.264864 0.801870    0.416325     0.455547       -0.190683\n",
            "      5000    bilstm     500 0.767142    0.267667     0.251896 0.797863    0.390956     0.428748       -0.176853\n",
            "      5000    bilstm     300 0.756011    0.252705     0.229769 0.797863    0.390956     0.428748       -0.198979\n",
            "      3000    bilstm     200 0.754230    0.247190     0.222489 0.801870    0.416325     0.455547       -0.233058\n"
          ]
        }
      ],
      "source": [
        "rows = []\n",
        "for nw in NUM_WORDS_LIST:\n",
        "    for um in USE_MODELS:\n",
        "        for ml in MAXLEN_LIST:\n",
        "            rows.append(run_one(nw, um, ml))\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values([\"dl_macro_f1\",\"lr_macro_f1\"], ascending=False).reset_index(drop=True)\n",
        "\n",
        "cols = [\"num_words\",\"use_model\",\"maxlen\", \"dl_acc\",\"dl_bal_acc\",\"dl_macro_f1\",\n",
        "        \"lr_acc\",\"lr_bal_acc\",\"lr_macro_f1\", \"delta_macro_f1\"]\n",
        "\n",
        "print(\"\\n===== Summary (DL vs TFIDF+LR) — Macro-F1 기준 내림차순 (DL 우선) =====\")\n",
        "print(df[cols].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZMtnfOwCi-K"
      },
      "source": [
        "- CNN vs BiLSTM\n",
        "  - CNN 계열은 전체적으로 TFIDF+LR 대비 Macro-F1이 높고 안정적\n",
        "    - CNN은 지역적인 패턴(ngram-like)을 잘 잡아주고 학습이 안정적\n",
        "  - BiLSTM 계열은 모든 실험에서 TFIDF+LR보다 오히려 성능이 낮음\n",
        "    - 특히 Macro-F1이 0.22~0.32 수준으로 크게 떨어짐\n",
        "    - 학습 데이터 크기, 모델 구조, 하이퍼파라미터가 충분하지 않아 과적합 or 학습 불안정 가능성\n",
        "    - 데이터 양 부족 + 긴 시퀀스에서 gradient 문제로 학습이 불안정했을 가능성\n",
        "\n",
        "- 단어 수(CNN 기준)\n",
        "  - 3000 → 5000 단어로 늘렸을 때 Macro-F1이 개선됨\n",
        "  - 10000 단어에서는 오히려 성능 하락 → 희소성 증가 + 데이터 부족 → 학습 불안정\n",
        "\n",
        "- 문장 길이\n",
        "  - CNN : 200 → 300 → 500 길이 확장 시 Macro-F1이 조금씩 증가하다가 500에서 최고치\n",
        "  - BiLSTM : 길이에 따른 명확한 개선 없음, 오히려 길어질수록 성능이 더 떨어짐"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
