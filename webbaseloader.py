import os
import streamlit as st
import tempfile
import re # ê¸€ì ìˆ˜ ì¹´ìš´íŠ¸ë¥¼ ìœ„í•´ ì¶”ê°€

# LangChain ëª¨ë“ˆ ì„í¬íŠ¸
from langchain_community.document_loaders import Docx2txtLoader, WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
# LangChain_openaiëŠ” í™˜ê²½ ë³€ìˆ˜ 'OPENAI_API_KEY'ë¥¼ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.

st.set_page_config(page_title="ìì†Œì„œ ì‘ì„± ì±—ë´‡", layout="centered")
# ê¸€ì ìˆ˜ (ê³µë°± ì œì™¸) ì¹´ìš´íŠ¸ í•¨ìˆ˜
def count_korean_chars(text):
    """ê³µë°±ì„ ì œì™¸í•œ ìˆœìˆ˜ í•œê¸€, ì˜ë¬¸, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ë“±ì˜ ê¸€ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if not text:
        return 0
    # ëª¨ë“  ê³µë°± ë¬¸ì (ìŠ¤í˜ì´ìŠ¤, íƒ­, ê°œí–‰) ì œê±°
    cleaned_text = re.sub(r'\s+', '', text) 
    return len(cleaned_text)

# íŒŒì¼ ë¡œë“œ (Word) ì²˜ë¦¬ í•¨ìˆ˜
@st.cache_data(show_spinner=False)
def load_file_docs(_file):
    try:
        # Word íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(mode="wb", delete=False) as tmp_file:
            tmp_file.write(_file.getvalue())
            tmp_file_path = tmp_file.name
            
            # Word íŒŒì¼ ë¡œë“œ
            loader = Docx2txtLoader(file_path=tmp_file_path)
            pages = loader.load()
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(tmp_file_path)
        return pages
    except Exception as e:
        st.error(f"Word íŒŒì¼ '{_file.name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# URL ë¡œë”© ë° ì²˜ë¦¬ í•¨ìˆ˜
@st.cache_data(show_spinner=False)
def load_url_docs(_url):
    try:
        # WebBaseLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹í˜ì´ì§€ ì½˜í…ì¸  ë¡œë“œ
        loader = WebBaseLoader(_url)
        pages = loader.load()
        return pages
    except Exception as e:
        # ì›¹í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ ì‹œ êµ¬ì²´ì ì¸ ë©”ì‹œì§€ ì¶œë ¥
        st.error(f"URL: {_url} ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ì ‘ê·¼í•  ìˆ˜ ì—†ê±°ë‚˜ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource(show_spinner="í…ìŠ¤íŠ¸ ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
def create_vector_store(_docs):
    # ë‹¨ì¼í™”ëœ ë¬¸ì„œ ë¶„í• (Text Splitting)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(_docs)
    
    st.info(f"ë¬¸ì„œê°€ ì´ {len(split_docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤. ìª¼ê°œì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    
    # Chroma DBì— ì €ì¥.
    vectorstore = Chroma.from_documents(split_docs, OpenAIEmbeddings(model='text-embedding-3-small'))
    return vectorstore

# ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ì¶œì²˜ ì •ë³´ ê°•í™”)
def format_docs(docs):
    formatted_content = []
    for i, doc in enumerate(docs):
        source = doc.metadata.get('source', 'ì—…ë¡œë“œ íŒŒì¼')
        
        # ë¬¸ì„œ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ì—¬ LLMì´ ì°¸ê³ í•  ìˆ˜ ìˆë„ë¡ í•¨
        if source.startswith(('http://', 'https://')):
            source_display = f"Source URL: {source}"
        else:
            source_display = "Source File"

        formatted_content.append(f"**[ì°¸ê³  ë¬¸ì„œ {i+1}]** ({source_display})\n{doc.page_content}")
        
    # LLMì´ ì°¸ê³ í•  ìˆ˜ ìˆë„ë¡ ì¶œì²˜ê°€ í¬í•¨ëœ ë¬¸ìì—´ì„ ë°˜í™˜
    return "\n\n".join(formatted_content)

# RAG ì²´ì¸ êµ¬ì¶•
@st.cache_resource(show_spinner="RAG ì²´ì¸ êµ¬ì„± ì¤‘...")
def chaining(_vectorstore):
    retriever = _vectorstore.as_retriever(search_kwargs={"k": 5}) # ê²€ìƒ‰ ë¬¸ì„œ ê°œìˆ˜ 5ê°œë¡œ ì„¤ì •

    qa_system_prompt = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì œê³µí•œ [ê²€ìƒ‰ëœ ë¬¸ì„œ] (ê²½ë ¥ê¸°ìˆ ì„œ, ì„±ê²©, ì¥ë‹¨ì , ì§ë¬´ê¸°ìˆ ì„œ, ì¸ì¬ìƒ, ì¡°ì§ë¬¸í™”, ì‚°ì—…ì •ë³´, ê¸°ì—…ì •ë³´, ë‰´ìŠ¤ ë“±) ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬, \
ì‚¬ìš©ìì˜ ìš”ì²­(ìê¸°ì†Œê°œì„œ ë¬¸í•­)ì— ëŒ€í•œ **ìê¸°ì†Œê°œì„œ ì´ˆì•ˆì„ ì§ì ‘ ì‘ì„±**í•´ì£¼ëŠ” ì „ë¬¸ AI ì‘ê°€ì…ë‹ˆë‹¤.

## ì‘ì„± ì§€ì¹¨
1. **ë°˜ë“œì‹œ** ì œê³µëœ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì˜ **ê²½í—˜, ì—­ëŸ‰, ì‚¬ì‹¤**ë§Œì„ í™œìš©í•˜ì—¬ ë‹µë³€ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
2. ë‹µë³€ì€ ì‚¬ìš©ìì˜ ìš”ì²­(ì§ˆë¬¸)ì— ëŒ€í•œ ìê¸°ì†Œê°œì„œ ë¬¸í•­ì˜ **ë³¸ë¬¸ ì´ˆì•ˆ** í˜•íƒœë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
3. ë¬¸ì²´ëŠ” **ë…¼ë¦¬ì ì´ê³ , ì „ë¬¸ì ì´ë©°, êµ¬ì²´ì ì¸ ì„œìˆ **ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
4. ë‹µë³€ì˜ ë¶„ëŸ‰ì€ ìµœì†Œ 400ì (ê³µë°± ì œì™¸) ì´ìƒì´ ë˜ë„ë¡ ì‘ì„±í•˜ë˜, í•µì‹¬ ë‚´ìš©ì´ ì˜ ë“œëŸ¬ë‚˜ë„ë¡ í•©ë‹ˆë‹¤.
5. [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ë‹µë³€ì„ ìœ„í•œ ì •ë³´(ê²½í—˜, ì‚¬ë¡€)ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´, 'ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ í•´ë‹¹ ë¬¸í•­ì˜ ì´ˆì•ˆì„ ì‘ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë ¨ ê²½í—˜ì´ë‚˜ ì‚¬ë¡€ë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ê±°ë‚˜ ì§ˆë¬¸ì„ ë°”ê¿”ì£¼ì„¸ìš”.'ë¼ê³  ì‘ë‹µí•©ë‹ˆë‹¤.
6. ê¸°ì—…, ì§ë¬´ì™€ ê´€ë ¨ëœ ê²½í—˜, ì§€ì‹, ì—­ëŸ‰ì„ ì˜ ì„¤ëª…í•˜ë„ë¡ ì‘ì„±.
7. ì…ì‚¬í•˜ë ¤ëŠ” ì˜ì§€ê°€ ëª…í™•í•˜ê²Œ ë³´ì´ë„ë¡ ì‘ì„±.

## í•„ìˆ˜ ì¶œë ¥ í˜•ì‹ (ì‹ ê·œ ì§€ì¹¨)
- ì‚¬ìš©ìì˜ ìš”ì²­ì— í¬í•¨ëœ **íšŒì‚¬ ëª…**ê³¼ **ì§ë¬´ ëª…**ì„ ë‹µë³€ ìµœìƒë‹¨ì— ëª…í™•í•˜ê²Œ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
- [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— í¬í•¨ëœ Source URL ì¤‘ ë‹µë³€ êµ¬ì„±ì— ê°€ì¥ í•µì‹¬ì ìœ¼ë¡œ ì‚¬ìš©ëœ **í•˜ë‚˜ì˜ URL**ì„ ì°¾ì•„ì„œ 'ì°¸ê³  URL'ë¡œ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. (URLì´ ëª¨ë‘ íŒŒì¼ ì¶œì²˜ì´ê±°ë‚˜ ë‹µë³€ì— URL ì¶œì²˜ê°€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ë‹¤ë©´ 'ì œì‹œëœ URL ì—†ìŒ'ìœ¼ë¡œ ëª…ì‹œ)

[ì˜ˆì‹œ ì¶œë ¥ í˜•ì‹]
íšŒì‚¬ ëª…: [ì‚¬ìš©ìê°€ ì œì‹œí•œ íšŒì‚¬ ëª…]
ì§ë¬´ ëª…: [ì‚¬ìš©ìê°€ ì œì‹œí•œ ì§ë¬´ ëª…]
ì°¸ê³  URL: [ê°€ì¥ ì¤‘ìš”í•œ í•˜ë‚˜ì˜ URL, ë˜ëŠ” 'ì œì‹œëœ URL ì—†ìŒ']
---
[ì—¬ê¸°ì— 400ì ì´ìƒì˜ ìê¸°ì†Œê°œì„œ ì´ˆì•ˆ ë³¸ë¬¸ ì‘ì„±]

\n\n[ê²€ìƒ‰ëœ ë¬¸ì„œ]\n{context}
    """

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("user", "{input}"),
        ]
    )

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)
    
    # 1. Context retrieval chain (ê²€ìƒ‰ ë° ë¬¸ì„œ í˜•ì‹ ì§€ì •)
    retrieval_chain = retriever | format_docs

    # 2. Answer generation chain (ë‹µë³€ ìƒì„±). contextì™€ inputì„ í•„ìš”ë¡œ í•¨.
    answer_chain = (
        RunnableParallel(
            context=retrieval_chain,      # ê²€ìƒ‰ëœ contextë¥¼ í”„ë¡¬í”„íŠ¸ì— ì œê³µ
            input=RunnablePassthrough()   # ì‚¬ìš©ì ì…ë ¥(ì§ˆë¬¸)ì„ í”„ë¡¬í”„íŠ¸ì— ì œê³µ
        )
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    # 3. Final output chain: ìµœì¢…ì ìœ¼ë¡œ contextì™€ answerë¥¼ ëª¨ë‘ ë°˜í™˜í•˜ëŠ” ì²´ì¸ êµ¬ì„±
    # (RunnableParallelì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì¶œë ¥ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° ëª…í™•í™”)
    rag_chain = RunnableParallel(
        context=retrieval_chain, # ê²€ìƒ‰ëœ context ê²°ê³¼ë¥¼ ìµœì¢… ì¶œë ¥ì— í¬í•¨
        answer=answer_chain      # ìƒì„±ëœ ë‹µë³€ ê²°ê³¼ë¥¼ ìµœì¢… ì¶œë ¥ì— í¬í•¨
    )
    return rag_chain # {"context": str, "answer": str} ë°˜í™˜

# Streamlit UI
st.set_page_config(page_title="ìì†Œì„œ ì‘ì„± ì±—ë´‡", layout="centered")
st.title("ìì†Œì„œ ì‘ì„± ì±—ë´‡ ğŸ’¬âœ¨")
st.markdown("ìì†Œì„œ ì´ˆì•ˆ ì‘ì„±ì— í•„ìš”í•œ **ì—¬ëŸ¬ ê°œì˜ ì´ë ¥ì„œ íŒŒì¼(Word)** ë˜ëŠ” **ê¸°ì—… ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” URL**ì„ ì—…ë¡œë“œ/ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ------------------------------------
# 1. ë°ì´í„° ì†ŒìŠ¤ ì…ë ¥ ì„¹ì…˜ (ë‹¤ì¤‘ ì²˜ë¦¬)
# ------------------------------------
st.subheader("1. ë°ì´í„° ì†ŒìŠ¤ ì…ë ¥")

# ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ
uploaded_files = st.file_uploader(
    "1-1. ì´ë ¥ì„œ(Word íŒŒì¼)ë¥¼ ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œí•˜ì„¸ìš”.", 
    type=[".docx"], 
    accept_multiple_files=True
)

st.markdown("---")

# ë‹¤ì¤‘ URL ì…ë ¥
url_input_area = st.text_area(
    "1-2. ê¸°ì—… í™ˆí˜ì´ì§€ URLì„ ì—¬ëŸ¬ ì¤„ë¡œ ì…ë ¥í•˜ì„¸ìš”. (ê° ì¤„ë§ˆë‹¤ 'http://' ë˜ëŠ” 'https://'ë¡œ ì‹œì‘)",
    height=150
)

# ------------------------------------
# 2. ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
# ------------------------------------
vectorstore = None
all_docs = []

if uploaded_files or url_input_area:
    with st.spinner("ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤..."):
        
        # A. íŒŒì¼ ë¡œë“œ
        if uploaded_files:
            file_progress = st.progress(0, text="Word íŒŒì¼ ë¡œë”© ì¤‘...")
            for i, file in enumerate(uploaded_files):
                docs_from_file = load_file_docs(file)
                all_docs.extend(docs_from_file)
                file_progress.progress((i + 1) / len(uploaded_files), text=f"Word íŒŒì¼ ë¡œë”© ì¤‘... ({file.name})")
            file_progress.empty()
            if uploaded_files:
                st.success(f"ì´ {len(uploaded_files)}ê°œì˜ Word íŒŒì¼ ë¡œë”© ì™„ë£Œ. ğŸ“")

        # B. URL ë¡œë“œ
        if url_input_area:
            urls = [u.strip() for u in url_input_area.split('\n') if u.strip()]
            valid_urls = [u for u in urls if u.startswith(("http://", "https://"))]
            invalid_urls = [u for u in urls if not u.startswith(("http://", "https://"))]
            
            if valid_urls:
                url_progress = st.progress(0, text="URL ì›¹í˜ì´ì§€ ë¡œë”© ì¤‘...")
                for i, url in enumerate(valid_urls):
                    docs_from_url = load_url_docs(url)
                    all_docs.extend(docs_from_url)
                    url_progress.progress((i + 1) / len(valid_urls), text=f"URL ë¡œë”© ì¤‘... ({url})")
                url_progress.empty()
                st.success(f"ì´ {len(valid_urls)}ê°œì˜ URL ì›¹í˜ì´ì§€ ë¡œë”© ì™„ë£Œ. ğŸŒ")

            if invalid_urls:
                st.error(f"ë‹¤ìŒ {len(invalid_urls)}ê°œ URLì€ í˜•ì‹ì´ ìœ íš¨í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤: {', '.join(invalid_urls[:3])}...") 
                st.info("URLì€ 'http://' ë˜ëŠ” 'https://'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")

        # C. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        if all_docs:
            vectorstore = create_vector_store(all_docs)
            st.success("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ. ì´ì œ ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("ë¡œë”©ëœ ë¬¸ì„œ ë‚´ìš©ì´ ë¹„ì–´ ìˆê±°ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ------------------------------------
# 3. ì±—ë´‡ ì„¹ì…˜
# ------------------------------------
st.subheader("3. ì±—ë´‡ê³¼ ëŒ€í™” ì‹œì‘")

if vectorstore:
    # vectorstoreë¥¼ ì¸ìˆ˜ë¡œ ì „ë‹¬í•˜ì—¬ ì²´ì¸ êµ¬ì„±
    rag_chain = chaining(vectorstore)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ì´ˆê¸° ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ (ì‚¬ìš©ì ì•ˆë‚´ ê°•í™”)
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "ë°ì´í„° ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. **íšŒì‚¬ ëª…, ì§ë¬´ ëª…, ê·¸ë¦¬ê³  ìê¸°ì†Œê°œì„œ í•­ëª©**ì„ ëª…í™•í•˜ê²Œ í¬í•¨í•˜ì—¬ ì‘ì„±ì„ ìš”ì²­í•´ ì£¼ì„¸ìš”.\n\n**ì˜ˆì‹œ ìš”ì²­:**\n`íšŒì‚¬ ëª…: êµ¬ê¸€ ì½”ë¦¬ì•„, ì§ë¬´ ëª…: ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´, ìì†Œì„œ í•­ëª©: ë‹¹ì‹ ì´ êµ¬ê¸€ì— ê¸°ì—¬í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ì—­ëŸ‰ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?`"}
        ]

    # ì´ì „ ëŒ€í™” í‘œì‹œ
    for msg in st.session_state.messages:
        st.chat_message(msg['role']).write(msg['content'])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
        st.chat_message("user").write(prompt_message)
        st.session_state.messages.append({"role": "user", "content": prompt_message})

        # LLM ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”."):
                try:
                    # RAG ì²´ì¸ í˜¸ì¶œ ë° ë”•ì…”ë„ˆë¦¬ ì‘ë‹µ ìˆ˜ì‹  ({"context": str, "answer": str})
                    response_dict = rag_chain.invoke(prompt_message)
                    response_text = response_dict["answer"]
                    retrieved_context = response_dict["context"]
                except Exception as e:
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
                    response_text = f"ì£„ì†¡í•©ë‹ˆë‹¤. RAG ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                    retrieved_context = "ê²€ìƒ‰ ì‹¤íŒ¨"
                    st.error(response_text)
                
                # 1. LLM ì‘ë‹µ ì¶œë ¥
                st.write(response_text)
                st.session_state.messages.append({"role": "assistant", "content": response_text})

                # 2. ê¸€ì ìˆ˜ ì¹´ìš´íŠ¸ í”¼ë“œë°±
                char_count = count_korean_chars(response_text)
                
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì •ì˜ëœ ìµœì†Œ ê¸€ì ìˆ˜
                MIN_CHARS = 400 
                
                st.markdown(f"**ğŸ“ ê¸€ì ìˆ˜:** ê³µë°± ì œì™¸ **{char_count}ì** (ìµœì†Œ {MIN_CHARS}ì ê¶Œì¥)")

                if char_count < MIN_CHARS:
                    st.warning(f"âš ï¸ **{MIN_CHARS - char_count}ì ë¶€ì¡±!** ê¸€ì ìˆ˜ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±ì‹œí‚¤ê¸° ìœ„í•´ ë‚´ìš©ì„ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ë³´ê°•í•´ë³´ì„¸ìš”.")
                else:
                    st.success("âœ… ì¶©ë¶„í•œ ë¶„ëŸ‰ì…ë‹ˆë‹¤.")

                # 3. RAG íˆ¬ëª…ì„± í™•ë³´
                # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ Expanderë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.
                with st.expander("ğŸ” LLMì´ ì°¸ê³ í•œ ê²€ìƒ‰ëœ ë¬¸ì„œ (Retrieval Context) ë³´ê¸°"):
                    st.code(retrieved_context, language='markdown')
else:
    st.info("ë¨¼ì € 1ë‹¨ê³„ì—ì„œ ì´ë ¥ì„œ(Word) íŒŒì¼ì´ë‚˜ ê¸°ì—… URLì„ ì…ë ¥í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
