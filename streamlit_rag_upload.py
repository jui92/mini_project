import os, hashlib, tempfile
import streamlit as st

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# (ì„ íƒ) Chroma ìºì‹œ ì´ˆê¸°í™”ê°€ ê¼­ í•„ìš”í•  ë•Œë§Œ ì‚¬ìš©
# import chromadb
# chromadb.api.client.SharedSystemClient.clear_system_cache()

# sqlite ëŒ€ì²´ (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
import sys
__import__("pysqlite3"); sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

# --- OpenAI í‚¤ ì•ˆì „ ì„¤ì • ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” .streamlit/secrets.tomlì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# íŒŒì¼ ë°”ì´íŠ¸ í•´ì‹œ (ìºì‹œ/ì»¬ë ‰ì…˜ ë¶„ë¦¬ì— ì‚¬ìš©)
def file_hash(uploaded_file) -> str:
    m = hashlib.sha256()
    m.update(uploaded_file.getvalue())
    return m.hexdigest()[:16]

@st.cache_data(show_spinner=False)
def load_pdf_bytes(_bytes: bytes):
    with tempfile.NamedTemporaryFile(mode="wb", delete=False) as tmp:
        tmp.write(_bytes)
        path = tmp.name
    loader = PyPDFLoader(file_path=path)
    return loader.load_and_split()

def make_vectorstore_key(fhash: str, embed_model: str) -> str:
    return f"chroma_{embed_model.replace('-', '_')}_{fhash}"

# persistë¥¼ ì“°ê³  ì‹¶ì§€ ì•Šë‹¤ë©´ persist_directory=None (ë©”ëª¨ë¦¬)ë¡œ ë‘ì„¸ìš”.
def create_vector_store(pages, collection_name: str):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    docs = splitter.split_documents(pages)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")  # 1536ì°¨ì›
    vs = Chroma.from_documents(
        docs, embeddings,
        collection_name=collection_name,
        persist_directory=None  # ë””ìŠ¤í¬ ë³´ì¡´ ì›í•˜ë©´ "./.chroma/<collection_name>" ë“±ìœ¼ë¡œ
    )
    return vs

@st.cache_resource(show_spinner=False)
def chaining(_pages, collection_name: str):
    vectorstore = create_vector_store(_pages, collection_name)
    retriever = vectorstore.as_retriever()

    qa_system_prompt = (
        "ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ QA ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•´ í•œêµ­ì–´ë¡œ ê³µì†í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”. "
        "ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. ì´ëª¨ì§€ëŠ” ë”± 1ê°œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\n\n{context}"
    )

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        ("human", "{input}")
    ])

    llm = ChatOpenAI(model="gpt-4o", temperature=0.2)
    rag_chain = (
        {"context": retriever | (lambda d: "\n\n".join(doc.page_content for doc in d)),
         "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# ---- UI ----
st.header("ChatPDF ğŸ’¬ ğŸ“š")
uploaded = st.file_uploader("PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

if uploaded is not None:
    fhash = file_hash(uploaded)
    pages = load_pdf_bytes(uploaded.getvalue())
    coll = make_vectorstore_key(fhash, "text-embedding-3-small")
    rag_chain = chaining(pages, coll)

    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
        st.chat_message("user").write(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = rag_chain.invoke(prompt)
                st.session_state.messages.append({"role": "assistant", "content": response})
                st.write(response)
