# -*- coding: utf-8 -*-
# ==========================================================
# íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜ (KR)
# - íšŒì‚¬ ì†Œê°œë§Œ LLM ìš”ì•½ / ì±„ìš© ìš”ê±´(ì—…ë¬´Â·ìê²©Â·ìš°ëŒ€)ì€ 'ì›ë¬¸ ê·¸ëŒ€ë¡œ'
# - ë™ì  ì±„ìš© ê³µê³  ëŒ€ì‘: r.jina.ai ìŠ¤ëƒ…ìƒ· + ì‚¬ì´íŠ¸ë³„ ì²˜ë¦¬ + ì„¹ì…˜ í—¤ë” ìš°ì„  ë¶„ë¦¬ + ë¶ˆë¦¿ ë¶„ë¥˜ê¸°
# - ìë™ ìƒì„¸ ê³µê³  URL ìš°ì„  ì„ íƒ(ëª©ë¡ í˜ì´ì§€ ì œê±°)
# - ì§ˆë¬¸ ë‹¤ì–‘í™” / RAG(ì„ íƒ)
# - ì±„ì : ì ìš© ê°€ëŠ¥í•œ ì¶•ì— í•œí•´ 0~20 ì±„ì , ì´ì  = (ì ìš©ì¶• í‰ê·  Ã— 5) â†’ 100ì 
# - ì ìˆ˜ ì¼ê´€í™”: ì¢Œ/ìš°/CSV/ë ˆì´ë” ëª¨ë‘ ë™ì¼ ì ìˆ˜ ì‚¬ìš©
# - ë ˆì´ë” í‘œì— 'í•©ê³„' ì¶”ê°€(ë¹„ì ìš©ì€ ì œì™¸í•˜ê³  í•©ì‚°)
# ==========================================================

import os, io, re, json, textwrap, urllib.parse, difflib, random, functools
from typing import List, Dict, Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st

# ---------- optional dependencies ----------
try:
    import pypdf
except Exception:
    pypdf = None

try:
    import plotly.graph_objects as go
    PLOTLY_OK = True
except Exception:
    PLOTLY_OK = False

try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# ---------- page ----------
st.set_page_config(page_title="íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜", page_icon="ğŸ¯", layout="wide")

# ---------- keys ----------
def _secrets_file_exists() -> bool:
    return any(os.path.exists(p) for p in [
        os.path.join(os.path.expanduser("~"), ".streamlit", "secrets.toml"),
        os.path.join(os.getcwd(), ".streamlit", "secrets.toml"),
    ])

def load_api_key_from_env_or_secrets() -> Optional[str]:
    k = os.getenv("OPENAI_API_KEY")
    if k: return k
    try:
        if _secrets_file_exists() or hasattr(st, "secrets"):
            return st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        pass
    return None

def load_naver_keys():
    cid = os.getenv("NAVER_CLIENT_ID")
    csec = os.getenv("NAVER_CLIENT_SECRET")
    try:
        if hasattr(st, "secrets"):
            cid = cid or st.secrets.get("NAVER_CLIENT_ID", None)
            csec = csec or st.secrets.get("NAVER_CLIENT_SECRET", None)
    except Exception:
        pass
    return cid, csec

NAVER_ID, NAVER_SECRET = load_naver_keys()

# ---------- utils ----------
def _clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", (t or "")).strip()

def _snippetize(text: str, n: int = 220) -> str:
    t = _clean_text(text)
    return t if len(t) <= n else t[: n-1] + "â€¦"

def chunk_text(text: str, size: int = 900, overlap: int = 150):
    text = re.sub(r"\s+", " ", text or "").strip()
    if not text: return []
    out, i = [], 0
    while i < len(text):
        j = min(len(text), i + size)
        out.append(text[i:j])
        if j == len(text): break
        i = max(0, j - overlap)
    return out

def read_file_to_text(up) -> str:
    name = up.name.lower()
    data = up.read()
    if name.endswith((".txt",".md")):
        for enc in ("utf-8","cp949","euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    if name.endswith(".pdf"):
        if pypdf is None: return ""
        try:
            reader = pypdf.PdfReader(io.BytesIO(data))
            return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
        except Exception:
            return ""
    return ""

def _domain(u: str|None) -> str|None:
    if not u: return None
    try:
        if not u.startswith(("http://","https://")):
            u = "https://" + u
        return urllib.parse.urlparse(u).netloc.lower().replace("www.","")
    except Exception:
        return None

# ---------- http cache ----------
@functools.lru_cache(maxsize=256)
def _cached_get(url: str, timeout: int = 8) -> Optional[str]:
    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code==200 and "text/html" in r.headers.get("content-type",""):
            return r.text
    except Exception:
        pass
    return None

# ---------- NAVER open API ----------
def _naver_api_get(api: str, params: dict, cid: str, csec: str):
    url = f"https://openapi.naver.com/v1/search/{api}.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec, "User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(url, headers=headers, params=params, timeout=8)
        if r.status_code != 200: return None
        return r.json()
    except Exception:
        return None

def naver_search_news(q: str, display: int = 6) -> list[dict]:
    cid, csec = load_naver_keys()
    if not (cid and csec): return []
    js = _naver_api_get("news", {"query": q, "display": display, "sort": "date"}, cid, csec)
    if not js: return []
    out=[]
    for it in js.get("items", []):
        title = _clean_text(re.sub(r"</?b>|&quot;|&apos;|&amp;|&lt;|&gt;", "", it.get("title","")))
        out.append({"title": title, "link": it.get("link"), "pubDate": it.get("pubDate")})
    return out

def naver_search_web(q: str, display: int = 5) -> list[str]:
    cid, csec = load_naver_keys()
    if not (cid and csec): return []
    js = _naver_api_get("webkr", {"query": q, "display": display, "sort": "date"}, cid, csec)
    if not js: return []
    links=[]
    for it in js.get("items", []):
        link = it.get("link")
        if link and link not in links: links.append(link)
    return links

# ---------- site snippets ----------
VAL_KEYWORDS = ["í•µì‹¬ê°€ì¹˜","ê°€ì¹˜","ë¯¸ì…˜","ë¹„ì „","ë¬¸í™”","ì›ì¹™","ì² í•™","ê³ ê°","ë°ì´í„°","í˜ì‹ ",
                "values","mission","vision","culture","principles","philosophy","customer","data","innovation"]

def fetch_site_snippets(home: str|None) -> dict:
    if not home: return {"values":[], "recent":[], "about":None}
    if not home.startswith(("http://","https://")): home = "https://" + home
    values, recent, about = [], [], None
    for path in ["","/about","/company","/about-us"]:
        html = _cached_get(home.rstrip("/") + path, timeout=6)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        if about is None:
            hero = soup.find(["p","div"], class_=re.compile(r"(lead|hero|intro)", re.I))
            if hero: about = _snippetize(hero.get_text(" "))
        for tag in soup.find_all(["h1","h2","h3","p","li"]):
            txt = _clean_text(tag.get_text(" "))
            if 8 <= len(txt) <= 220:
                if any(k in txt.lower() for k in [k.lower() for k in VAL_KEYWORDS]): values.append(txt)
                if any(k in txt for k in ["í”„ë¡œì íŠ¸","ê°œë°œ","ì¶œì‹œ","ì„±ê³¼","launched","release","delivered"]): recent.append(txt)
    def dedup(x): 
        s=set(); out=[]
        for t in x:
            if t not in s: s.add(t); out.append(t)
        return out
    values = dedup(values)[:6]; recent = dedup(recent)[:6]
    return {"values": values, "recent": recent, "about": about}

# ---------- job discover ----------
CAREER_HINTS = ["careers","career","jobs","job","recruit","recruiting","join","hire","hiring","ì±„ìš©","ì¸ì¬","ì…ì‚¬ì§€ì›","ì±„ìš©ê³µê³ ","ì¸ì¬ì˜ì…","ì»¤ë¦¬ì–´"]
JOB_SITES   = ["wanted.co.kr","saramin.co.kr","jobkorea.co.kr","rocketpunch.com","indeed.com","linkedin.com"]

DETAIL_HINTS = re.compile(r"(job|position|posting|recruit|notice|opening).*(\d{3,}|detail)|/positions/|/jobs/|/recruit/|/careers/\w+", re.I)

def discover_job_from_homepage(home: str, limit: int = 3) -> list[str]:
    if not home: return []
    if not home.startswith(("http://","https://")): home = "https://" + home
    html = _cached_get(home, timeout=8)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    links=[]
    for path in ["careers","recruit","jobs","career","ì±„ìš©","ì¸ì¬ì˜ì…","recruitment","join"]:
        links.append(urllib.parse.urljoin(home.rstrip("/") + "/", path))
    for a in soup.find_all("a", href=True):
        href=a["href"]; text=(a.get_text() or "").lower()
        if any(k in href.lower() or k in text for k in CAREER_HINTS):
            links.append(urllib.parse.urljoin(home, href))
    out=[]; seen=set()
    for lk in links:
        if lk not in seen: seen.add(lk); out.append(lk)
        if len(out)>=limit: break
    return out

def discover_job_urls(name: str, role: str, home: str|None, limit: int = 3) -> list[str]:
    urls=[]
    if home: urls += discover_job_from_homepage(home, limit=limit)
    if NAVER_ID and NAVER_SECRET:
        for dom in JOB_SITES:
            if len(urls)>=limit: break
            q = f"{name} {role} site:{dom}" if role else f"{name} ì±„ìš© site:{dom}"
            links = naver_search_web(q, display=7)
            for lk in links:
                urls.append(lk)
    # ìƒì„¸ ê³µê³  URL ìš°ì„  ì •ë ¬ + ì¤‘ë³µ ì œê±°
    uniq = []
    for u in urls:
        if u not in uniq: uniq.append(u)
    uniq = sorted(uniq, key=lambda u: 0 if DETAIL_HINTS.search(u or "") else 1)
    return uniq[:limit]

# ---------- dynamic snapshot (Jina Reader) ----------
def fetch_text_snapshot(url: str, timeout: int = 12) -> str:
    """ë Œë”ëœ í˜ì´ì§€ í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ·(r.jina.ai). lstrip ë²„ê·¸ ì—†ì´ ì•ˆì „í•˜ê²Œ."""
    try:
        if not url.startswith(("http://","https://")):
            url = "https://" + url
        parsed = urlparse(url)
        snap_url = f"https://r.jina.ai/http/{parsed.geturl()}"
        r = requests.get(snap_url, timeout=timeout, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code == 200 and r.text:
            return re.sub(r"\s+", " ", r.text).strip()
    except Exception:
        pass
    return ""

# ---------- job posting parser (ê°•í™”) ----------
RESP_KEYS = ["ì£¼ìš” ì—…ë¬´","ë‹´ë‹¹ ì—…ë¬´","ì—…ë¬´","Responsibilities","What you will do","Role","Your role","What you'll do"]
QUAL_KEYS = ["ìê²© ìš”ê±´","ì§€ì› ìê²©","Requirements","Qualifications","Must have","Required","Basic qualifications"]
PREF_KEYS = ["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","Preferred","Nice to have","Plus","Preferred qualifications"]

RESP_HINTS = [
    "ì—…ë¬´","ë‹´ë‹¹","ì±…ì„","ì—­í• ","Role","Responsibilities","Work you'll do","What you will do","What you'll do",
    "You will","Key responsibilities","ë¯¸ì…˜","Mission"
]
QUAL_HINTS = [
    "ìê²©","ìš”ê±´","í•„ìˆ˜","í•„ìˆ˜ì¡°ê±´","í•„ìˆ˜ì—­ëŸ‰","Requirements","Qualifications","Must have","Required",
    "Basic qualifications","ì¡°ê±´","ê²½ë ¥","í•™ë ¥","í•„ìš” ìŠ¤í‚¬","í•„ìˆ˜ ê¸°ìˆ ","í•„ìˆ˜ ê²½í—˜","ìš°ë¦¬ê°€ ì°¾ëŠ” ì¸ì¬"
]
PREF_HINTS = [
    "ìš°ëŒ€","ê°€ì‚°ì ","Preferred","Nice to have","Plus","ìš°ëŒ€ì‚¬í•­","ìˆìœ¼ë©´ ì¢‹ì€","ìš°ëŒ€ ì—­ëŸ‰","ê°€ì ","ì„ í˜¸",
    "Preferred qualifications","Bonus points"
]

def _extract_json_ld_job(soup: BeautifulSoup) -> Optional[dict]:
    for s in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(s.string or "")
            seq = data if isinstance(data, list) else [data]
            for obj in seq:
                typ = obj.get("@type") if isinstance(obj, dict) else None
                if (isinstance(typ, list) and "JobPosting" in typ) or typ == "JobPosting":
                    return obj
        except Exception:
            continue
    return None

def _split_bullets(txt: str) -> list[str]:
    arr = re.split(r"[\n\r]+|[â€¢Â·â–ªï¸â–¶ï¸\-]\s+|â€¢\s*", txt or "")
    return [a.strip(" -â€¢Â·â–ªï¸â–¶ï¸") for a in arr if len(a.strip())>2]

def classify_bullets(lines: list[str]) -> tuple[list[str], list[str], list[str]]:
    resp, qual, pref = [], [], []
    for s in lines:
        t = s.strip(" -â€¢Â·â–ªï¸â–¶ï¸").strip()
        if len(t) < 3: 
            continue
        low = t.lower()
        # ìš°ëŒ€ â†’ ìê²© â†’ ì—…ë¬´ ìˆœìœ¼ë¡œ ìš°ì„  ë¶„ë¥˜
        if any(k.lower() in low for k in PREF_HINTS):
            pref.append(t);  continue
        if any(k.lower() in low for k in QUAL_HINTS):
            qual.append(t);  continue
        if any(k.lower() in low for k in RESP_HINTS):
            resp.append(t);  continue
        # ì‹ í˜¸ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±
        if re.search(r"(ê²½ë ¥\s*\d+|~\s*\d+\s*ë…„|ì‹ ì…|ì¸í„´|ì •ê·œì§|í•™ë ¥|ì „ê³µ|ìê²©ì¦|ì˜ì–´|í† ìµ|OPIc|JLPT|ì •ë³´ì²˜ë¦¬ê¸°ì‚¬|Certification|Bachelor|Master|PhD)", t, re.I):
            qual.append(t); continue
        if re.search(r"^(ì„¤ê³„|êµ¬í˜„|ê°œë°œ|ìš´ì˜|ë¶„ì„|ì‘ì„±|ê°œì„ |ê´€ë¦¬|Design|Implement|Build|Operate|Analyze|Lead)\b", t, re.I):
            resp.append(t); continue
        # ê¸°ë³¸ê°’: ì—…ë¬´
        resp.append(t)
    def clean(xs):
        out, seen = [], set()
        for x in xs:
            x = _snippetize(x, 200)
            if x and x not in seen:
                seen.add(x); out.append(x)
        return out[:25]
    return clean(resp), clean(qual), clean(pref)

def _find_section_bullets(soup: BeautifulSoup, keys: list[str]) -> list[str]:
    for h in soup.find_all(re.compile("^h[1-4]$")):
        head = _clean_text(h.get_text())
        if any(k.lower() in head.lower() for k in keys):
            bul=[]
            nxt=h.find_next_sibling()
            stop=set(["h1","h2","h3","h4"])
            while nxt and nxt.name not in stop:
                if nxt.name in {"ul","ol"}:
                    for li in nxt.find_all("li"):
                        t=_clean_text(li.get_text(" "))
                        if len(t)>2: bul.append(t)
                elif nxt.name in {"p","div"}:
                    bul += _split_bullets(nxt.get_text(" "))
                nxt=nxt.find_next_sibling()
            if bul: return bul
    body_text = soup.get_text("\n")
    if any(k.lower() in body_text.lower() for k in keys):
        return _split_bullets(body_text)
    return []

def split_by_sections(text: str) -> dict:
    """ìŠ¤ëƒ…ìƒ· í…ìŠ¤íŠ¸ì—ì„œ ì„¹ì…˜ í—¤ë”ë¡œ í° ë©ì–´ë¦¬ë¥¼ ë¨¼ì € ë¶„ë¦¬"""
    pats = {
        "resp": r"(ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|Responsibilities?|Role|What you will do|What you'll do)",
        "qual": r"(ìê²©\s*ìš”ê±´|ì§€ì›\s*ìê²©|Requirements?|Qualifications?|Must\s+have|Required|Basic\s+qualifications?)",
        "pref": r"(ìš°ëŒ€\s*ì‚¬í•­?|Preferred|Nice\s+to\s+have|Plus|Preferred\s+qualifications?)"
    }
    sec = {"resp":"", "qual":"", "pref":""}
    try:
        spans=[]
        for k,pat in pats.items():
            for m in re.finditer(pat, text, re.I):
                spans.append((m.start(), k))
        spans.sort()
        if not spans:
            return sec
        for i,(pos,k) in enumerate(spans):
            end = spans[i+1][0] if i+1<len(spans) else len(text)
            sec[k] += text[pos:end]
    except Exception:
        pass
    return sec

def parse_job_posting(url: str) -> dict:
    out = {"title": None, "responsibilities": [], "qualifications": [], "preferred": [], "company_intro": None}
    try:
        r = requests.get(url, timeout=12, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code == 200 and "text/html" in r.headers.get("content-type",""):
            soup = BeautifulSoup(r.text, "html.parser")
            if soup.title and soup.title.string:
                out["title"] = _clean_text(soup.title.string)
            meta_desc = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
            if meta_desc and meta_desc.get("content"):
                out["company_intro"] = _snippetize(meta_desc["content"], 220)

            # 1) JSON-LD
            jp = _extract_json_ld_job(soup)
            if jp:
                out["title"] = jp.get("title") or out["title"]
                desc = _clean_text(jp.get("description",""))
                if desc:
                    lines = _split_bullets(desc)
                    r1,q1,p1 = classify_bullets(lines)
                    out["responsibilities"] += r1; out["qualifications"] += q1; out["preferred"] += p1

            host = (_domain(url) or "")
            got = lambda: (out["responsibilities"] or out["qualifications"] or out["preferred"])

            # 2) ì‚¬ì´íŠ¸ë³„ ì „ìš©
            if "wanted.co.kr" in host and not got():
                nd = soup.find("script", id="__NEXT_DATA__")
                if nd and nd.string:
                    try:
                        data = json.loads(nd.string)
                        text_fields=[]
                        def walk(x):
                            if isinstance(x, dict):
                                for k,v in x.items():
                                    if isinstance(v, (dict, list)):
                                        walk(v)
                                    elif isinstance(v, str) and len(v) > 20:
                                        if any(tk in k.lower() for tk in ["description","requirement","qualification","preference","responsibil"]):
                                            text_fields.append(v)
                            elif isinstance(x, list):
                                for it in x: walk(it)
                        walk(data)
                        lines=[]
                        for t in text_fields: lines += _split_bullets(t)
                        r2,q2,p2 = classify_bullets(lines)
                        out["responsibilities"] += r2; out["qualifications"] += q2; out["preferred"] += p2
                    except Exception:
                        pass

            if "saramin.co.kr" in host and not got():
                body = soup.select_one("#job_summary, .user_content, .wrap_jview, .content")
                if body:
                    lines = _split_bullets(body.get_text("\n"))
                    r3,q3,p3 = classify_bullets(lines)
                    out["responsibilities"] += r3; out["qualifications"] += q3; out["preferred"] += p3

            if "jobkorea.co.kr" in host and not got():
                body = soup.select_one("#tab02, .detailArea, .recruitMent, .smartApply, .viewContents")
                if body:
                    lines = _split_bullets(body.get_text("\n"))
                    r4,q4,p4 = classify_bullets(lines)
                    out["responsibilities"] += r4; out["qualifications"] += q4; out["preferred"] += p4

            if "rocketpunch.com" in host and not got():
                body = soup.select_one(".job-detail, .description, .job-detail__full")
                if body:
                    lines = _split_bullets(body.get_text("\n"))
                    r5,q5,p5 = classify_bullets(lines)
                    out["responsibilities"] += r5; out["qualifications"] += q5; out["preferred"] += p5

            # 3) ì¼ë°˜ ì„¹ì…˜ í—¤ë”
            if not got():
                body_text = soup.get_text("\n")
                # í—¤ë” ë¶„ë¦¬ ë¨¼ì €
                sec = split_by_sections(body_text)
                if sec.get("resp"): out["responsibilities"] += _split_bullets(sec["resp"])
                if sec.get("qual"): out["qualifications"]   += _split_bullets(sec["qual"])
                if sec.get("pref"): out["preferred"]        += _split_bullets(sec["pref"])
                # ê·¸ë˜ë„ ë¶€ì¡±í•˜ë©´ ì „ì²´ë¥¼ ë¶„ë¥˜ê¸°ë¡œ
                if not got():
                    lines = _split_bullets(body_text)
                    r6,q6,p6 = classify_bullets(lines)
                    out["responsibilities"] += r6; out["qualifications"] += q6; out["preferred"] += p6

        # 4) ìŠ¤ëƒ…ìƒ· í´ë°±(ì„¹ì…˜ í—¤ë” â†’ ë¶ˆë¦¿ ë¶„ë¥˜)
        if not (out["responsibilities"] or out["qualifications"] or out["preferred"]):
            snap = fetch_text_snapshot(url)
            if snap:
                sec = split_by_sections(snap)
                if sec.get("resp"): out["responsibilities"] += _split_bullets(sec["resp"])
                if sec.get("qual"): out["qualifications"]   += _split_bullets(sec["qual"])
                if sec.get("pref"): out["preferred"]        += _split_bullets(sec["pref"])
                if not (out["responsibilities"] or out["qualifications"] or out["preferred"]):
                    r7,q7,p7 = classify_bullets(_split_bullets(snap))
                    out["responsibilities"] += r7; out["qualifications"] += q7; out["preferred"] += p7

        # 5) <li> í´ë°±(ìµœí›„)
        if not (out["responsibilities"] or out["qualifications"] or out["preferred"]):
            try:
                soup2 = BeautifulSoup(r.text, "html.parser")
                lis = [ _clean_text(li.get_text(" ")) for li in soup2.find_all("li") ]
                r8,q8,p8 = classify_bullets(lis)
                out["responsibilities"] += r8; out["qualifications"] += q8; out["preferred"] += p8
            except Exception:
                pass

        def dedup(xs):
            seen, out2 = set(), []
            for x in xs:
                x = _snippetize(x, 200)
                if x and x not in seen:
                    seen.add(x); out2.append(x)
            return out2[:25]
        out["responsibilities"] = dedup(out["responsibilities"])
        out["qualifications"]   = dedup(out["qualifications"])
        out["preferred"]        = dedup(out["preferred"])

    except Exception:
        pass

    return out

# ---------- OpenAI ----------
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    API_KEY = load_api_key_from_env_or_secrets()
    if not API_KEY:
        st.info("í™˜ê²½ë³€ìˆ˜/Secretsì—ì„œ í‚¤ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ì— ì…ë ¥ í›„ ì—”í„°.")
        API_KEY = st.text_input("OPENAI_API_KEY", type="password")
    MODEL = st.selectbox("ì±— ëª¨ë¸", ["gpt-4o-mini","gpt-4o","gpt-4.1-mini"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small","text-embedding-3-large"], index=0)
    with st.expander("ë””ë²„ê·¸: í‚¤/ë²„ì „"):
        try:
            import openai as _o; ov = getattr(_o, "__version__", "?")
        except Exception:
            ov = "?"
        st.write({"api_key": bool(API_KEY), "naver_keys": bool(NAVER_ID and NAVER_SECRET), "openai": ov})

if not API_KEY:
    st.error("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."); st.stop()
client = OpenAI(api_key=API_KEY, timeout=30.0)

# ==========================================================
# ì…ë ¥
# ==========================================================
st.subheader("â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥")
company_name = st.text_input("íšŒì‚¬ ì´ë¦„", placeholder="ì˜ˆ: ë„¤ì´ë²„ / Kakao / ì‚¼ì„±SDS")
homepage     = st.text_input("ê³µì‹ í™ˆí˜ì´ì§€ URL(ì„ íƒ)", placeholder="https://...")
role_title   = st.text_input("ì§€ì› ì§ë¬´ëª…", placeholder="ë°ì´í„° ì• ë„ë¦¬ìŠ¤íŠ¸ / ML ì—”ì§€ë‹ˆì–´ ...")
job_url_in   = st.text_input("ì±„ìš© ê³µê³  URL(ì„ íƒ) â€” ì—†ë‹¤ë©´ ìë™ íƒìƒ‰")

# ì„¸ì…˜ ìƒíƒœ
defaults = {
    "company_state": {},
    "history": [],
    "current_question": "",
    "answer_text": "",
    "rag_store": {"chunks": [], "embeds": None},
}
for k,v in defaults.items():
    if k not in st.session_state: st.session_state[k]=v

def build_company(name: str, home: str|None, role: str|None, job_url: str|None) -> dict:
    site = fetch_site_snippets(home) if home else {"values":[], "recent":[], "about":None}
    urls = [job_url] if job_url else discover_job_urls(name, role or "", home, limit=5)
    jp = parse_job_posting(urls[0]) if urls else {"title":None,"responsibilities":[],"qualifications":[],"preferred":[],"company_intro":None}
    news = naver_search_news(name, display=6) or []
    return {
        "company_name": name.strip() or "(íšŒì‚¬ëª… ë¯¸ì„¤ì •)",
        "homepage": home or None,
        "values": site.get("values", []),
        "recent": site.get("recent", []),
        "company_intro_site": site.get("about"),
        "role": role or "",
        "job_url": urls[0] if urls else (job_url or None),
        # ì›ë¬¸ ê·¸ëŒ€ë¡œ
        "role_responsibilities": jp.get("responsibilities", []),
        "role_qualifications":   jp.get("qualifications", []),
        "role_preferred":        jp.get("preferred", []),
        "news": news
    }

def summarize_intro_only(c: dict) -> str:
    ctx = textwrap.dedent(f"""
    [í™ˆí˜ì´ì§€ ì†Œê°œ(ë°œì·Œ)] {c.get('company_intro_site') or ''}
    [ìµœê·¼ ë‰´ìŠ¤] {', '.join([_snippetize(n['title'],70) for n in c.get('news',[])[:3]])}
    """).strip()
    sys = "ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'íšŒì‚¬ ì†Œê°œ'ë§Œ 2~3ë¬¸ì¥ í•œêµ­ì–´ ìš”ì•½ìœ¼ë¡œ ì‘ì„±í•˜ë¼. ê´‘ê³ ì„± ë¬¸êµ¬ëŠ” ë°°ì œí•˜ê³  ì‚¬ì‹¤ ìœ„ì£¼ë¡œ."
    user = f"{ctx}\n\n[íšŒì‚¬ëª…] {c.get('company_name','')}"
    try:
        r = client.chat.completions.create(model=MODEL, temperature=0.2,
                                           messages=[{"role":"system","content":sys},{"role":"user","content":user}])
        return r.choices[0].message.content.strip()
    except Exception:
        return c.get("company_intro_site") or "íšŒì‚¬ ì†Œê°œ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

if st.button("íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°", type="primary"):
    if not company_name.strip():
        st.warning("íšŒì‚¬ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("íšŒì‚¬/ê³µê³ /ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
            cobj = build_company(company_name, homepage or None, role_title or None, job_url_in or None)
            intro = summarize_intro_only(cobj)
            st.session_state.company_state={"company":cobj, "intro":intro}
            # í•˜ë‹¨ ì´ˆê¸°í™”
            st.session_state.current_question=""
            st.session_state.answer_text=""
            st.session_state.history=[]
            st.session_state.rag_store={"chunks":[],"embeds":None}
        st.success("íšŒì‚¬ ì •ë³´ ê°±ì‹  ë° ê²°ê³¼ ì´ˆê¸°í™” ì™„ë£Œ")

company = st.session_state.get("company_state",{}).get("company")
intro   = st.session_state.get("company_state",{}).get("intro")

# ==========================================================
# â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´ (ì›ë¬¸)
# ==========================================================
st.subheader("â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´")
if company and intro:
    st.markdown(f"**íšŒì‚¬ëª…**: {company['company_name']}")
    st.markdown("**íšŒì‚¬ ì†Œê°œ(ìš”ì•½)**")
    st.markdown(intro)
    cols = st.columns(2)
    with cols[0]:
        if company.get("homepage"): st.link_button("í™ˆí˜ì´ì§€ ì—´ê¸°", company["homepage"])
    with cols[1]:
        if company.get("job_url"):  st.link_button("ì±„ìš© ê³µê³  ì—´ê¸°", company["job_url"])
    st.markdown("---")
    st.markdown(f"**ëª¨ì§‘ ë¶„ì•¼**: {company.get('role') or 'â€”'}")
    c1,c2,c3 = st.columns(3)
    with c1:
        st.markdown("**ì£¼ìš” ì—…ë¬´(ì›ë¬¸)**")
        arr = company.get("role_responsibilities") or []
        st.markdown("- " + "\n- ".join(arr) if arr else "_ê³µê³ ì—ì„œ ì¶”ì¶œëœ ì£¼ìš” ì—…ë¬´ê°€ ì—†ìŠµë‹ˆë‹¤._")
    with c2:
        st.markdown("**ìê²© ìš”ê±´(ì›ë¬¸)**")
        arr = company.get("role_qualifications") or []
        st.markdown("- " + "\n- ".join(arr) if arr else "_ê³µê³ ì—ì„œ ì¶”ì¶œëœ ìê²© ìš”ê±´ì´ ì—†ìŠµë‹ˆë‹¤._")
    with c3:
        st.markdown("**ìš°ëŒ€ ì‚¬í•­(ì›ë¬¸)**")
        arr = company.get("role_preferred") or []
        st.markdown("- " + "\n- ".join(arr) if arr else "_ê³µê³ ì—ì„œ ì¶”ì¶œëœ ìš°ëŒ€ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤._")
    with st.expander("ë””ë²„ê·¸: ê³µê³  ì¶”ì¶œ ìƒíƒœ"):
        st.write({
            "job_url": company.get("job_url"),
            "resp_cnt": len(company.get("role_responsibilities") or []),
            "qual_cnt": len(company.get("role_qualifications") or []),
            "pref_cnt": len(company.get("role_preferred") or []),
        })
else:
    st.info("ìœ„ ì…ë ¥ í›„ â€˜íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°â€™ë¥¼ ëˆŒëŸ¬ ì£¼ì„¸ìš”.")

# ==========================================================
# â‘¢ ì§ˆë¬¸ ìƒì„±
# ==========================================================
st.subheader("â‘¢ ì§ˆë¬¸ ìƒì„±")

def embed_texts(texts: list[str]) -> np.ndarray:
    if not texts: return np.zeros((0,1536), dtype=np.float32)
    r = client.embeddings.create(model="text-embedding-3-small", input=texts)
    return np.array([d.embedding for d in r.data], dtype=np.float32)

with st.expander("RAG ì˜µì…˜(ì„ íƒ)"):
    rag_on = st.toggle("íšŒì‚¬ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸/ì½”ì¹­ ì‚¬ìš©", value=True, key="rag_on")
    topk = st.slider("ê²€ìƒ‰ ìƒìœ„ K", 1, 8, 4, 1, key="topk")
    ups = st.file_uploader("íšŒì‚¬ ë¬¸ì„œ ì—…ë¡œë“œ (TXT/MD/PDF, ì—¬ëŸ¬ íŒŒì¼)", type=["txt","md","pdf"], accept_multiple_files=True)
    size = st.slider("ì²­í¬ ê¸¸ì´", 400, 2000, 900, 100)
    ovlp = st.slider("ì˜¤ë²„ë©", 0, 400, 150, 10)
    if ups:
        with st.spinner("ë¬¸ì„œ ì¸ë±ì‹± ì¤‘..."):
            chunks=[]
            for u in ups:
                t = read_file_to_text(u)
                if t: chunks += chunk_text(t, size, ovlp)
            if chunks:
                embs = embed_texts(chunks)
                st.session_state.rag_store={"chunks":chunks, "embeds":embs}
                st.success(f"ì²­í¬ {len(chunks)}ê°œ ì¸ë±ì‹± ì™„ë£Œ")

def cosine_topk(mat: np.ndarray, q: np.ndarray, k: int=4):
    if mat.size==0: return np.array([]), np.array([],dtype=int)
    mn = mat / (np.linalg.norm(mat, axis=1, keepdims=True)+1e-12)
    qn = q / (np.linalg.norm(q, axis=1, keepdims=True)+1e-12)
    sims = mn @ qn.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    return sims[idx], idx

def retrieve_supports(qtext: str, k: int):
    store = st.session_state.rag_store
    chs, embs = store.get("chunks", []), store.get("embeds")
    if not st.session_state.get("rag_on") or embs is None or not chs: return []
    qv = embed_texts([qtext])
    s, idx = cosine_topk(embs, qv, k=k)
    return [("íšŒì‚¬ìë£Œ", float(sc), chs[int(i)]) for sc,i in zip(s, idx)]

TYPE_INSTRUCTIONS = {
    "í–‰ë™(STAR)": "ê³¼ê±° ì‹¤ë¬´ ì‚¬ë¡€ë¥¼ ì´ëŒì–´ë‚´ëŠ” STAR",
    "ê¸°ìˆ  ì‹¬ì¸µ": "ì„±ëŠ¥/ë¹„ìš©/ì§€ì—°/ì •í™•ë„/ìš´ì˜ì„ í¬í•¨í•œ ê¸°ìˆ  ì‹¬ì¸µ",
    "í•µì‹¬ê°€ì¹˜ ì í•©ì„±": "íƒœë„/ê°€ì¹˜/í˜‘ì—… ìŠ¤íƒ€ì¼ ê²€ì¦",
    "ì—­ì§ˆë¬¸": "ì§€ì›ìê°€ íšŒì‚¬ë¥¼ í‰ê°€í•˜ëŠ” ì—­ì§ˆë¬¸",
}

def build_ctx(c: dict|None) -> str:
    if not c: return ""
    news = ", ".join([_snippetize(n["title"],70) for n in c.get("news",[])[:3]])
    return textwrap.dedent(f"""
    [íšŒì‚¬ëª…] {c.get('company_name','')}
    [ëª¨ì§‘ ë¶„ì•¼] {c.get('role','')}
    [ì£¼ìš” ì—…ë¬´] {", ".join(c.get('role_responsibilities', [])[:6])}
    [ìê²© ìš”ê±´] {", ".join(c.get('role_qualifications', [])[:6])}
    [í•µì‹¬ê°€ì¹˜] {", ".join(c.get('values', [])[:6])}
    [ìµœê·¼ ë‰´ìŠ¤] {news}
    """).strip()

def _sim(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def pick_diverse(cands: list[str], hist: list[str]) -> str:
    if not cands: return ""
    if not hist: return random.choice(cands)
    best=None; best_s=1e9
    for q in cands:
        sims=[_sim(q,h) for h in hist] or [0.0]
        s=(sum(sims)/len(sims)) + 0.35*np.std(sims)
        if s<best_s: best_s=s; best=q
    return best

q_type = st.selectbox("ì§ˆë¬¸ ìœ í˜•", list(TYPE_INSTRUCTIONS.keys()))
level  = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"])
hint   = st.text_input("ì§ˆë¬¸ ìƒì„± íŒíŠ¸(ì„ íƒ)", placeholder="ì˜ˆ: ì „í™˜ í¼ë„ / ëª¨ë¸ ì„±ëŠ¥-ë¹„ìš© / ë°ì´í„° í’ˆì§ˆ")

if "history" not in st.session_state: st.session_state.history=[]
if "current_question" not in st.session_state: st.session_state.current_question=""

if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", type="primary", use_container_width=True):
    st.session_state.answer_text=""
    try:
        sups=[]
        if st.session_state.get("rag_on"):
            base = hint.strip() or (company.get("role","") if company else "")
            sups = retrieve_supports(base, st.session_state.get("topk",4))
        ctx = build_ctx(company)
        focuses=[]
        if company:
            focuses += company.get("role_responsibilities", [])[:6] + company.get("role_qualifications", [])[:6]
        for _,_,txt in (sups or [])[:3]:
            focuses += [t.strip() for t in re.split(r"[â€¢\-\n\.]", txt) if 6<len(t.strip())<100][:3]
        focuses = [f for f in focuses if f]
        random.shuffle(focuses)
        focuses = focuses[:4]
        sys = f"""ë„ˆëŠ” '{q_type}' ìœ í˜•({TYPE_INSTRUCTIONS[q_type]})ì˜ ì§ˆë¬¸ 6ê°œë¥¼ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ë¼.
ê° ì§ˆë¬¸ì€ í¬ì»¤ìŠ¤ í‚¤ì›Œë“œë¥¼ 1ê°œ ì´ìƒ í¬í•¨í•˜ê³  í˜•íƒœ/ê´€ì /í‚¤ì›Œë“œê°€ ì„œë¡œ ë‹¤ë¥´ê²Œ í•˜ë¼. ë‚œì´ë„ {level}.
í¬ë§·: 1) ... 2) ... 3) ... (í•œ ì¤„ì”©)"""
        user = f"[ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n[í¬ì»¤ìŠ¤]\n- " + "\n- ".join(focuses)
        r = client.chat.completions.create(model=MODEL, temperature=0.95,
                                           messages=[{"role":"system","content":sys},{"role":"user","content":user}])
        raw=r.choices[0].message.content.strip()
        cands=[re.sub(r'^\s*\d+\)\s*','',ln).strip() for ln in raw.splitlines() if re.match(r'^\s*\d+\)', ln)]
        if not cands: cands=[ln.strip("- ").strip() for ln in raw.splitlines() if ln.strip()][:6]
        hist=[h["question"] for h in st.session_state.history][-10:]
        st.session_state.current_question = pick_diverse(cands, hist) or cands[0]
        st.session_state.last_supports_q = sups
    except Exception as e:
        st.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")

st.text_area("ì§ˆë¬¸", height=110, value=st.session_state.get("current_question",""))

# ==========================================================
# â‘£ ì½”ì¹­/ì±„ì  â€” ìë™ ë£¨ë¸Œë¦­ ì ìš© & ì ìˆ˜ ì¼ì›í™”
# ==========================================================
st.subheader("â‘£ ë‚˜ì˜ ë‹µë³€ / ì½”ì¹­")
AXES = ["ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜"]

KEYMAP = {
    "ë¬¸ì œì •ì˜":  [r"ë¬¸ì œ|ê°€ì„¤|ëª©í‘œ|ì œì•½|SLA|KPI|ìš”êµ¬ì‚¬í•­|íŠ¸ë ˆì´ë“œì˜¤í”„|ì„ íƒ"],
    "ë°ì´í„°/ì§€í‘œ":[r"ì§€í‘œ|ë°ì´í„°|í†µê³„|A/B|ì‹¤í—˜|ì •í™•|ì¬í˜„|ê²€ì •|ìƒ˜í”Œ|ê°€ì„¤|í”¼ì²˜|ëª¨ë¸|ëª¨ë‹ˆí„°ë§|ë¡œê·¸|ë©”íŠ¸ë¦­|ROC|ë¦¬ì½œ|ì •ë°€|F1|ì‹ ë¢°|í’ˆì§ˆ|ë°ì´í„° í’ˆì§ˆ|CI|í‘œë³¸"],
    "ì‹¤í–‰ë ¥/ì£¼ë„ì„±":[r"ì„¤ê³„|ì•„í‚¤í…ì²˜|êµ¬í˜„|ë°°í¬|íŒŒì´í”„ë¼ì¸|ìš´ì˜|ì¥ì• |ë³µêµ¬|ìŠ¤ì¼€ì¼|ì„±ëŠ¥|íŠœë‹|ë¦¬ë“œ|ì£¼ë„|Flink|Spark|Kafka|Airflow|dbt|ETL"],
    "í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜":[r"í˜‘ì—…|ì»¤ë®¤ë‹ˆì¼€ì´ì…˜|ì¡°ìœ¨|í•©ì˜|ì´í•´ê´€ê³„ì|ë¬¸ì„œ|ëŸ°ë¶|ë°ì´í„° ê³„ì•½|RFC|PR|ë¦¬ë·°|ì¡°ì •"],
    "ê³ ê°ê°€ì¹˜":  [r"ë¹„ìš©|ROI|ìˆ˜ìµ|ì „í™˜|ë¦¬í…ì…˜|NPS|ë§Œì¡±|ê·œì •|ë³´ì•ˆ|ê°œì¸ì •ë³´|ë¦¬ìŠ¤í¬|ë¹„ì¦ˆë‹ˆìŠ¤|ê°€ì¹˜|ì„íŒ©íŠ¸|íš¨ê³¼"],
}

RUBRIC_DESC = {
    "ë¬¸ì œì •ì˜": "ëª©í‘œÂ·ì œì•½(SLA/ë¹„ìš©/ì§€ì—°/ì •í™•ë„) ëª…í™•í™”, íŠ¸ë ˆì´ë“œì˜¤í”„ ì •ì˜, ì„ íƒ ê¸°ì¤€",
    "ë°ì´í„°/ì§€í‘œ":"ì •ëŸ‰ ì§€í‘œ/ì‹¤í—˜ì„¤ê³„/í’ˆì§ˆ ê²€ì¦, ì¬í˜„ì„±, ëª¨ë‹ˆí„°ë§",
    "ì‹¤í–‰ë ¥/ì£¼ë„ì„±":"ì•„í‚¤í…ì²˜/êµ¬í˜„/ìš´ì˜/ì¥ì• ëŒ€ì‘/ìŠ¤ì¼€ì¼ë§Â·ë¹„ìš© ìµœì í™”",
    "í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜":"ìš”êµ¬ì‚¬í•­ ìˆ˜ì§‘, ì´í•´ê´€ê³„ì ì¡°ìœ¨, ë¬¸ì„œí™”/ëŸ°ë¶, ë°ì´í„° ê³„ì•½",
    "ê³ ê°ê°€ì¹˜":"ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸Â·ë¹„ìš©/ë¦¬ìŠ¤í¬/ê·œì •/ë³´ì•ˆÂ·ê°œì¸ì •ë³´ ê³ ë ¤",
}

def detect_axes_from_question(q: str) -> list[bool]:
    ql = q.lower()
    applies=[False]*5
    for i,axis in enumerate(AXES):
        pats=KEYMAP[axis]
        if any(re.search(p, ql, re.I) for p in pats):
            applies[i]=True
    if sum(applies)==0: applies=[True,False,False,False,True]  # ìµœì†Œ ë³´ì¥
    return applies

def coach(company: dict|None, question: str, answer: str, supports, qtype: str) -> dict:
    ctx = build_ctx(company)
    applies = detect_axes_from_question(question)
    apply_text = ", ".join([f"{AXES[i]}({'O' if applies[i] else '-'})" for i in range(5)])
    details = "\n".join([f"- {k}: {RUBRIC_DESC[k]}" for k,a in zip(AXES, applies) if a])
    rag=""
    if supports:
        rag = "\n[íšŒì‚¬ ê·¼ê±° ë¬¸ì„œ ë°œì·Œ]\n" + "\n".join([f"- ({s:.3f}) {txt[:300]}" for _,s,txt in supports]) + "\n"
    sys = f"""ë„ˆëŠ” í•œêµ­ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. ì•„ë˜ í˜•ì‹ë§Œ ì¶œë ¥í•˜ë¼.
1) ì´ì : NN/100  # ì´ ì¤„ì€ ë°˜ë“œì‹œ ì²« 3ì¤„ ì•ˆì— ìœ„ì¹˜
2) ê°•ì : 2~3ê°œ ë¶ˆë¦¿
3) ë¦¬ìŠ¤í¬: 2~3ê°œ ë¶ˆë¦¿
4) ê°œì„  í¬ì¸íŠ¸: 3ê°œ ë¶ˆë¦¿(í–‰ë™Â·ì§€í‘œÂ·ì„íŒ©íŠ¸ ì¤‘ì‹¬)
5) ìˆ˜ì •ë³¸ ë‹µë³€: STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼) êµ¬ì¡°ë¡œ ê°„ê²°í•˜ê²Œ
6) ì—­ëŸ‰ ì ìˆ˜(ê° 0~20, ë¹„ì ìš©ì€ '-' ê·¸ëŒ€ë¡œ): [{', '.join(AXES)}] â€” ì´ ìˆœì„œë¡œ 5ê°œ ê°’ì„ ì‰¼í‘œë¡œ ì¶œë ¥
ì±„ì ì€ 'ì ìš© ì¶•'ì— í•œí•´ ìœ„ ë£¨ë¸Œë¦­ì„ ì ìš©í•˜ê³ , ë¹„ì ìš© ì¶•ì€ '-'ë¡œ ë‘”ë‹¤."""
    user = f"""[ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n{rag}[ì§ˆë¬¸ ìœ í˜•] {qtype}\n[ì ìš© ì¶•]\n{apply_text}\n[ë£¨ë¸Œë¦­]\n{details}\n\n[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n[í›„ë³´ì ë‹µë³€]\n{answer}"""
    r = client.chat.completions.create(model=MODEL, temperature=0.35,
                                       messages=[{"role":"system","content":sys},{"role":"user","content":user}])
    content = r.choices[0].message.content.strip()

    # ì—­ëŸ‰ íŒŒì‹±
    last = content.splitlines()[-1]
    toks = [t.strip() for t in re.split(r"[,\s]+", last) if t.strip()!=""]
    comps: list[Optional[int]]=[]
    for t in toks[:5]:
        if t in ["-","â€“","â€”"]: comps.append(None)
        elif re.fullmatch(r"\d{1,2}", t): comps.append(max(0,min(20,int(t))))
        else: comps.append(None)
    while len(comps)<5: comps.append(None)

    # ì´ì (ìš°ë¦¬ ê³„ì‚°): ì ìš©ì¶• í‰ê·  Ã—5
    used=[v for v,a in zip(comps, applies) if a and isinstance(v,int)]
    score = round(sum(used)/len(used)*5) if used else 0

    # ë³¸ë¬¸ ë‚´ ì´ì  ë¼ì¸ êµì²´
    lines=content.splitlines(); repl=False
    for i,L in enumerate(lines[:3]):
        if "ì´ì " in L:
            lines[i]=re.sub(r"ì´ì \s*:\s*\d{1,3}\s*/\s*100", f"ì´ì : {score}/100", L) if re.search(r"ì´ì \s*:", L) else f"ì´ì : {score}/100"
            repl=True; break
    if not repl:
        lines.insert(0, f"ì´ì : {score}/100")
    content_fixed="\n".join(lines)

    return {"raw": content_fixed, "score": score, "competencies": comps, "applies": applies}

ans = st.text_area("ì—¬ê¸°ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš” (STAR ê¶Œì¥: ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)", height=180, key="answer_text")

if st.button("ì±„ì  & ì½”ì¹­", type="primary", use_container_width=True):
    if not st.session_state.get("current_question"):
        st.warning("ë¨¼ì € 'ìƒˆ ì§ˆë¬¸ ë°›ê¸°'ë¡œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì½”ì¹­ ì¤‘..."):
            sups=[]
            if st.session_state.get("rag_on"):
                q_for_rag = st.session_state["current_question"] + "\n" + st.session_state.answer_text[:800]
                sups = retrieve_supports(q_for_rag, st.session_state.get("topk",4))
            res = coach(company, st.session_state["current_question"], st.session_state.answer_text, sups, q_type)
            st.session_state.history.append({
                "ts": pd.Timestamp.now(),
                "question": st.session_state["current_question"],
                "user_answer": st.session_state.answer_text,
                "score": res["score"],
                "feedback": res["raw"],
                "supports": sups,
                "competencies": res["competencies"],
                "applies": res["applies"],
            })

# ---------- ê²°ê³¼ ----------
st.divider()
st.subheader("í”¼ë“œë°± ê²°ê³¼")
if st.session_state.history:
    last = st.session_state.history[-1]
    total = last["score"]
    c1,c2 = st.columns([1,3])
    with c1:
        st.metric("ì´ì (/100)", total)
    with c2:
        st.markdown(f"**ì´ì (ì‹œìŠ¤í…œ ì‚°ì¶œ)**: {total}/100")
        st.markdown(last["feedback"])
else:
    st.info("ì•„ì§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ---------- ë ˆì´ë” ----------
st.divider()
st.subheader("ì—­ëŸ‰ ë ˆì´ë” (ì„¸ì…˜ ëˆ„ì )")
AX = ["ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜"]

def comp_df(hist):
    rows=[]
    for h in hist:
        vals = h.get("competencies")
        if not vals: continue
        rows.append([np.nan if v is None else float(v) for v in vals])
    if not rows: return None
    df = pd.DataFrame(rows, columns=AX)
    df["í•©ê³„"] = df[AX].sum(axis=1, skipna=True)
    return df

cdf = comp_df(st.session_state.history)
if cdf is not None:
    means = cdf[AX].mean(axis=0, skipna=True).tolist()
    radar = [0 if np.isnan(x) else float(x) for x in means]
    if PLOTLY_OK:
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(r=radar+[radar[0]], theta=AX+[AX[0]], fill='toself'))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,20])), showlegend=False, height=420)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.bar_chart(pd.DataFrame({"score": radar}, index=AX))
    st.dataframe(cdf.fillna("-"), use_container_width=True)
else:
    st.caption("ì•„ì§ ì—­ëŸ‰ ì ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ---------- CSV ----------
st.divider()
st.subheader("ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV)")
def build_report(hist):
    rows=[]
    for h in hist:
        row={"timestamp":h["ts"],"question":h["question"],"user_answer":h["user_answer"],
             "score":h["score"],"feedback_raw":h["feedback"]}
        comps=h.get("competencies") or []
        for k,v in zip(AX, comps): row[f"comp_{k}"]=("-" if v is None else v)
        row["comp_sum"]=sum([v for v in comps if isinstance(v,int)]) if comps else 0
        rows.append(row)
    return pd.DataFrame(rows) if rows else pd.DataFrame(columns=["timestamp","question","user_answer","score","feedback_raw"])
rep = build_report(st.session_state.history)
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=rep.to_csv(index=False).encode("utf-8-sig"),
                   file_name="interview_session_report.csv", mime="text/csv")

st.caption("ì†ë„ ìµœì í™”: ìƒì„¸ URL ìš°ì„  ì„ íƒ, HTTP ìºì‹œ, 'íšŒì‚¬ ì†Œê°œë§Œ' ìš”ì•½(í† í° ì ˆê°), RAG ì¡°ê±´ë¶€ ì‹¤í–‰")
