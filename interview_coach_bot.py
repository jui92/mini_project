# -*- coding: utf-8 -*-
import os, io, re, json, textwrap, urllib.parse, difflib, random, time, hashlib
from typing import List, Dict, Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st

# ---------------- Optional deps ----------------
try:
    import pypdf
except Exception:
    pypdf = None

try:
    import plotly.graph_objects as go
    PLOTLY_OK = True
except Exception:
    PLOTLY_OK = False

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

import requests
from bs4 import BeautifulSoup

# ---------------- Page config ----------------
st.set_page_config(page_title="íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜", page_icon="ğŸ¯", layout="wide")

# =========================================================
# Secrets / API keys
# =========================================================
def _secrets_file_exists() -> bool:
    candidates = [
        os.path.join(os.path.expanduser("~"), ".streamlit", "secrets.toml"),
        os.path.join(os.getcwd(), ".streamlit", "secrets.toml"),
    ]
    return any(os.path.exists(p) for p in candidates)

def load_api_key_from_env_or_secrets() -> Optional[str]:
    key = os.getenv("OPENAI_API_KEY")
    if key: return key
    try:
        if _secrets_file_exists() or hasattr(st, "secrets"):
            return st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        pass
    return None

def load_naver_keys():
    cid = os.getenv("NAVER_CLIENT_ID")
    csec = os.getenv("NAVER_CLIENT_SECRET")
    try:
        if hasattr(st, "secrets"):
            cid = cid or st.secrets.get("NAVER_CLIENT_ID", None)
            csec = csec or st.secrets.get("NAVER_CLIENT_SECRET", None)
    except Exception:
        pass
    return cid, csec

NAVER_ID, NAVER_SECRET = load_naver_keys()

# =========================================================
# Utils
# =========================================================
def _clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", t or "").strip()

def _snippetize(text: str, maxlen: int = 240) -> str:
    t = _clean_text(text)
    return t if len(t) <= maxlen else t[: maxlen - 1] + "â€¦"

def _domain(u: str|None) -> str|None:
    if not u: return None
    try:
        if not u.startswith("http"): u = "https://" + u
        return urllib.parse.urlparse(u).netloc.lower().replace("www.","")
    except Exception:
        return None

def _name_similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()

def read_file_to_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt", ".md")):
        for enc in ("utf-8", "cp949", "euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        if pypdf is None:
            st.warning("pypdfê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— pypdf ì¶”ê°€.")
            return ""
        try:
            reader = pypdf.PdfReader(io.BytesIO(data))
            return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
        except Exception as e:
            st.warning(f"PDF íŒŒì‹± ì‹¤íŒ¨({uploaded.name}): {e}")
            return ""
    return ""

def chunk_text(text: str, size: int = 900, overlap: int = 150):
    text = re.sub(r"\s+", " ", text).strip()
    if not text: return []
    out, start = [], 0
    while start < len(text):
        end = min(len(text), start + size)
        out.append(text[start:end])
        if end == len(text): break
        start = max(0, end - overlap)
    return out

def filehash(bytes_or_str: bytes|str) -> str:
    b = bytes_or_str if isinstance(bytes_or_str, bytes) else bytes(bytes_or_str, "utf-8", errors="ignore")
    return hashlib.md5(b).hexdigest()

# =========================================================
# NAVER Open API
# =========================================================
def _naver_api_get(api: str, params: dict, cid: str, csec: str):
    url = f"https://openapi.naver.com/v1/search/{api}.json"
    headers = {
        "X-Naver-Client-Id": cid,
        "X-Naver-Client-Secret": csec,
        "User-Agent": "Mozilla/5.0",
    }
    r = requests.get(url, headers=headers, params=params, timeout=8)
    if r.status_code != 200:
        return None
    return r.json()

@st.cache_data(show_spinner=False, ttl=1800)
def naver_search_news(query: str, display: int = 8, sort: str = "date") -> list[dict]:
    cid, csec = load_naver_keys()
    if not (cid and csec):
        return []
    js = _naver_api_get("news", {"query": query, "display": display, "sort": sort}, cid, csec)
    if not js: return []
    out = []
    for it in js.get("items", []):
        title = _clean_text(re.sub(r"</?b>|&quot;|&apos;|&amp;|&lt;|&gt;", "", it.get("title","")))
        out.append({"title": title, "link": it.get("link"), "pubDate": it.get("pubDate")})
    return out

@st.cache_data(show_spinner=False, ttl=3600)
def naver_search_web(query: str, display: int = 10, sort: str = "date") -> list[str]:
    cid, csec = load_naver_keys()
    if not (cid and csec):
        return []
    js = _naver_api_get("webkr", {"query": query, "display": display, "sort": sort}, cid, csec)
    if not js: return []
    links = []
    for it in js.get("items", []):
        link = it.get("link")
        if link and link not in links:
            links.append(link)
    return links

# =========================================================
# Site / Careers crawling (light)
# =========================================================
VAL_KEYWORDS = ["value","values","mission","vision","culture","ê³ ê°","ê°€ì¹˜","ë¬¸í™”","ì›ì¹™","ì² í•™","í˜ì‹ "]

@st.cache_data(show_spinner=False, ttl=3600)
def fetch_site_intro(base_url: str|None) -> dict:
    """
    íšŒì‚¬ ì†Œê°œë§Œ ìš”ì•½í•˜ê¸° ìœ„í•œ ê°€ë²¼ìš´ ë¬¸ì¥ í›„ë³´ ìˆ˜ì§‘ (about/intro).
    """
    if not base_url:
        return {"site_name": None, "about_candidates": []}
    url0 = base_url.strip()
    if not url0.startswith("http"): url0 = "https://" + url0

    cand_paths = ["", "/", "/about", "/company", "/about-us"]
    site_name = None
    about_candidates = []

    for path in cand_paths:
        url = url0.rstrip("/") + path
        try:
            r = requests.get(url, timeout=6, headers={"User-Agent":"Mozilla/5.0"})
            if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
                continue
            soup = BeautifulSoup(r.text, "html.parser")
            if site_name is None and soup.title and soup.title.string:
                site_name = _clean_text(soup.title.string.split("|")[0])

            for tag in soup.find_all(["p","div","li","section"]):
                txt = _clean_text(tag.get_text(" "))
                if 50 <= len(txt) <= 400 and any(k in txt.lower() for k in ["company","service","solution","platform","ê³ ê°","ì„œë¹„ìŠ¤","ì œí’ˆ","íšŒì‚¬"]):
                    about_candidates.append(txt)
        except Exception:
            continue

    # dedup
    seen=set(); outs=[]
    for t in about_candidates:
        if t not in seen:
            seen.add(t); outs.append(t)
    return {"site_name":site_name, "about_candidates":outs[:8]}

CAREER_HINTS = ["careers","recruit","jobs","career","ì±„ìš©","ì¸ì¬ì˜ì…","recruitment","join"]
@st.cache_data(show_spinner=False, ttl=3600)
def discover_job_from_homepage(homepage: str, limit: int = 5) -> list[str]:
    if not homepage: return []
    try:
        if not homepage.startswith("http"): homepage = "https://" + homepage
        r = requests.get(homepage, timeout=8, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
            return []
        soup = BeautifulSoup(r.text, "html.parser")
        links=[]
        for a in soup.find_all("a", href=True):
            href = a["href"]
            text = (a.get_text() or "").lower()
            if any(k in href.lower() or k in text for k in CAREER_HINTS):
                links.append(urllib.parse.urljoin(homepage, href))
        out=[]; seen=set()
        for lk in links:
            d = _domain(lk)
            if lk not in seen and d:
                seen.add(lk); out.append(lk)
                if len(out) >= limit: break
        return out[:limit]
    except Exception:
        return []

# =========================================================
# Job posting parsing + LLM ë¶„ë¥˜ ìš”ì•½ í´ë°±
# =========================================================
JOB_SITES = ["wanted.co.kr","saramin.co.kr","jobkorea.co.kr","rocketpunch.com","linkedin.com","indeed.com"]
@st.cache_data(show_spinner=False, ttl=3600)
def discover_job_posting_urls(company: str, role: str, homepage: str|None, limit: int=5) -> list[str]:
    urls=[]
    if homepage:
        urls += discover_job_from_homepage(homepage, limit=limit)
    if urls: return urls[:limit]

    # NAVER site: ê²€ìƒ‰
    if NAVER_ID and NAVER_SECRET:
        for dom in JOB_SITES:
            if len(urls)>=limit: break
            q = f"{company} {role} site:{dom}" if role else f"{company} ì±„ìš© site:{dom}"
            links = naver_search_web(q, display=5, sort="date")
            for lk in links:
                if _domain(lk) and dom in _domain(lk) and lk not in urls:
                    urls.append(lk)
                if len(urls)>=limit: break
    return urls[:limit]

def _extract_json_ld_job(soup: BeautifulSoup) -> Optional[dict]:
    for s in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(s.string or "")
            seq = data if isinstance(data, list) else [data]
            for obj in seq:
                typ = obj.get("@type") if isinstance(obj, dict) else None
                if (isinstance(typ, list) and "JobPosting" in typ) or typ == "JobPosting":
                    return obj
        except Exception:
            continue
    return None

@st.cache_data(show_spinner=False, ttl=900)
def fetch_page_text(url: str) -> str:
    try:
        r = requests.get(url, timeout=12, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
            return ""
        soup = BeautifulSoup(r.text, "html.parser")
        # ê°€ì‹œ í…ìŠ¤íŠ¸ë§Œ
        for s in soup(["script","style","noscript"]): s.decompose()
        txt = _clean_text(soup.get_text(separator="\n"))
        return txt
    except Exception:
        return ""

def parse_job_posting_structured(url: str) -> dict:
    """
    êµ¬ì¡°í™”(ì›ë¬¸) ìš°ì„  íŒŒì‹±. ì‹¤íŒ¨ ì‹œ ë¹ˆ ëª©ë¡.
    """
    out = {"responsibilities":[], "qualifications":[], "preferences":[]}
    try:
        r = requests.get(url, timeout=12, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""): return out
        soup = BeautifulSoup(r.text, "html.parser")

        # JSON-LD
        jp = _extract_json_ld_job(soup)
        if jp:
            desc = _clean_text(jp.get("description",""))
            if desc:
                bullets = [x.strip(" -â€¢Â·â–ªï¸â–¶ï¸") for x in re.split(r"[â€¢\-\nâ€¢Â·â–ªï¸â–¶ï¸]+", desc) if len(x.strip())>3]
                # ë¼ë²¨ì„ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ responsibilitiesì— ìš°ì„  ë‹´ê³ , ì•„ë˜ ì„¹ì…˜ íŒŒì‹±ìœ¼ë¡œ ë³´ì™„
                out["responsibilities"] += bullets[:12]

        # ì„¹ì…˜ ì¶”ì¶œ
        sections={}
        for h in soup.find_all(re.compile("^h[1-4]$")):
            head=_clean_text(h.get_text())
            if not head: continue
            nxt=[]; sib=h.find_next_sibling(); stop={"h1","h2","h3","h4"}
            while sib and sib.name not in stop:
                if sib.name in {"p","li","ul","ol","div"}:
                    t=_clean_text(sib.get_text(" "))
                    if len(t)>5: nxt.append(t)
                sib=sib.find_next_sibling()
            if nxt: sections[head]="\n".join(nxt)

        def explode(txt):
            return [x.strip() for x in re.split(r"[â€¢\-\nâ€¢Â·â–ªï¸â–¶ï¸]+", txt) if len(x.strip())>2][:12]

        # í•œêµ­ì–´/ì˜ì–´ í‚¤
        for k,v in sections.items():
            lk=k.lower()
            if any(s in lk for s in ["ì£¼ìš” ì—…ë¬´","ë‹´ë‹¹ ì—…ë¬´","ì—…ë¬´","responsibilities","what you will do","role"]):
                out["responsibilities"] += explode(v)
            if any(s in lk for s in ["ìê²© ìš”ê±´","ì§€ì› ìê²©","requirements","qualifications","must have"]):
                out["qualifications"] += explode(v)
            if any(s in lk for s in ["ìš°ëŒ€","prefer","nice to have","preferred"]):
                out["preferences"] += explode(v)

        # dedup
        for key in out:
            seen=set(); arr=[]
            for t in out[key]:
                if t not in seen:
                    seen.add(t); arr.append(_snippetize(t, 150))
            out[key]=arr[:12]
    except Exception:
        pass
    return out

# ---------------- LLM helpers ----------------
EVAL_FACTORS = [
    "ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜"
]
EVAL_SCHEMA_HINT = {
  "type":"object",
  "properties":{
    "overall": {"type":"integer", "minimum":0, "maximum":100},
    "factors": {
      "type":"object",
      "properties": {k: {
        "type":"object",
        "properties":{
          "score":{"type":"integer","minimum":0,"maximum":20},
          "comment":{"type":"string"},
          "deduct":{"type":"string"},
          "improve":{"type":"string"}
        },
        "required":["score"]
      } for k in EVAL_FACTORS},
      "additionalProperties": False
    },
    "strengths":{"type":"array","items":{"type":"string"}},
    "risks":{"type":"array","items":{"type":"string"}},
    "improvements":{"type":"array","items":{"type":"string"}},
    "revised":{"type":"string"}
  },
  "required":["overall","factors","revised"]
}

def get_openai_client() -> OpenAI:
    api_key = st.session_state.get("API_KEY")
    return OpenAI(api_key=api_key, timeout=30.0)

def embed_texts(client: OpenAI, embed_model: str, texts: list[str]) -> np.ndarray:
    if not texts:
        return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=embed_model, input=texts)
    return np.array([d.embedding for d in resp.data], dtype=np.float32)

def cosine_topk(matrix: np.ndarray, query: np.ndarray, k: int = 4):
    if matrix.size == 0:
        return np.array([]), np.array([], dtype=int)
    qn = query / (np.linalg.norm(query, axis=1, keepdims=True) + 1e-12)
    mn = matrix / (np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12)
    sims = mn @ qn.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    return sims[idx], idx

# =========================================================
# Sidebar
# =========================================================
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    API_KEY = load_api_key_from_env_or_secrets()
    if not API_KEY:
        st.info("í™˜ê²½ë³€ìˆ˜/Secretsì—ì„œ í‚¤ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ì— ì…ë ¥ í›„ ì—”í„°.")
        API_KEY = st.text_input("OPENAI_API_KEY", type="password")
    st.session_state.API_KEY = API_KEY

    MODEL = st.selectbox("ì±— ëª¨ë¸", ["gpt-4o-mini","gpt-4o","gpt-4.1-mini"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small","text-embedding-3-large"], index=0)

    with st.expander("ë””ë²„ê·¸: ì‹œí¬ë¦¿/ë²„ì „ ìƒíƒœ"):
        _openai_ver = None; _httpx_ver = None
        try:
            import openai as _openai_pkg; _openai_ver = getattr(_openai_pkg, "__version__", None)
        except Exception: pass
        try:
            import httpx as _httpx_pkg; _httpx_ver = getattr(_httpx_pkg, "__version__", None)
        except Exception: pass
        st.write({
            "api_key_provided": bool(API_KEY),
            "naver_keys": bool(NAVER_ID and NAVER_SECRET),
            "openai_version": _openai_ver,
            "httpx_version": _httpx_ver,
        })

if not OpenAI or not API_KEY:
    st.error("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. (Cloud: Settings â†’ Secrets)")
    st.stop()

# =========================================================
# â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥
# =========================================================
st.subheader("â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥")
colA, colB = st.columns(2)
with colA:
    company_name_input = st.text_input("íšŒì‚¬ ì´ë¦„", placeholder="ì˜ˆ: ë„¤ì´ë²„ / Kakao / ì‚¼ì„±SDS")
with colB:
    role_title = st.text_input("ì§€ì› ì§ë¬´ëª…", placeholder="ë°ì´í„° ì—”ì§€ë‹ˆì–´ / ML ì—”ì§€ë‹ˆì–´ ...")

job_url_input  = st.text_input("ì±„ìš© ê³µê³  URL(ì„ íƒ) â€” ì—†ìœ¼ë©´ ìë™ íƒìƒ‰/ìš”ì•½ í´ë°±")
homepage_input = st.text_input("ê³µì‹ í™ˆí˜ì´ì§€ URL(ì„ íƒ)", placeholder="https://...")

# ì„¸ì…˜ state ì¤€ë¹„
for key, default in [
    ("company", None),
    ("company_summary", None),
    ("rag_store", {"chunks":[], "embeds": None}),
    ("history", []),
    ("answer_text", ""),
]:
    if key not in st.session_state:
        st.session_state[key] = default

# =========================================================
# íšŒì‚¬ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì†Œê°œëŠ” ìš”ì•½ / ì±„ìš© 3ìš”ì†ŒëŠ” ì›ë¬¸ ìš°ì„ , ì‹¤íŒ¨ì‹œ LLM ìš”ì•½ ë¶„ë¥˜)
# =========================================================
def llm_summarize_intro(candidates: list[str], company: str) -> str:
    """
    í›„ë³´ ë¬¸ì¥ë“¤ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.
    """
    if not candidates:
        return ""
    client = get_openai_client()
    sys = "ë„ˆëŠ” ì±„ìš© ë‹´ë‹¹ìë‹¤. íšŒì‚¬ ì†Œê°œ ë¬¸ì¥ í›„ë³´ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë¼. ê³¼ì¥/ê´‘ê³  ë¬¸êµ¬ëŠ” ì œê±°."
    user = "íšŒì‚¬ëª…: {}\n\ní›„ë³´ ë¬¸ì¥:\n- {}".format(company, "\n- ".join([_snippetize(t, 400) for t in candidates[:8]]))
    resp = client.chat.completions.create(
        model=MODEL, temperature=0.2,
        messages=[{"role":"system","content":sys},{"role":"user","content":user}]
    )
    return resp.choices[0].message.content.strip()

def llm_split_job_text_to_sections(raw_text: str) -> dict:
    """
    ì›ë¬¸ì—ì„œ 'ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´/ìš°ëŒ€ì‚¬í•­'ì„ ë¶ˆë¦¿ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” JSON ì¶œë ¥ ê°•ì œ.
    """
    client = get_openai_client()
    schema = {
        "type":"object",
        "properties":{
            "responsibilities":{"type":"array","items":{"type":"string"}},
            "qualifications":{"type":"array","items":{"type":"string"}},
            "preferences":{"type":"array","items":{"type":"string"}}
        },
        "required":["responsibilities","qualifications","preferences"]
    }
    sys = (
        "ì…ë ¥ ì›ë¬¸ì—ì„œ 'ì£¼ìš”ì—…ë¬´','ìê²©ìš”ê±´','ìš°ëŒ€ì‚¬í•­'ì„ ê°ê° 3~8ê°œì˜ í•œêµ­ì–´ ë¶ˆë¦¿ìœ¼ë¡œ ë¶„ë¥˜í•´ JSONë§Œ ì¶œë ¥."
        "ëª¨í˜¸í•˜ë©´ ë¹ˆ ë°°ì—´([]) ìœ ì§€, ì¶”ì¸¡ ê¸ˆì§€. ë¶ˆë¦¿ì€ ê°„ê²°í•œ ëª…ì‚¬í˜•/ë¬¸ì¥í˜•ìœ¼ë¡œ."
        f"ìŠ¤í‚¤ë§ˆ: {json.dumps(schema, ensure_ascii=False)}"
    )
    resp = client.chat.completions.create(
        model=MODEL, temperature=0.2,
        messages=[{"role":"system","content":sys},{"role":"user","content":raw_text[:6000]}],
        response_format={"type":"json_object"}
    )
    data = json.loads(resp.choices[0].message.content)
    # ê¸¸ì´ ì œí•œ
    for k in data:
        data[k] = [ _snippetize(x, 140) for x in data[k] ][:12]
    return data

def build_company_context(name: str, homepage: str|None, role: str|None, job_url: str|None) -> dict:
    """
    1) íšŒì‚¬ ì†Œê°œ(ìš”ì•½) : í™ˆí˜ì´ì§€ intro í›„ë³´ â†’ ìš”ì•½
    2) ì±„ìš© 3ìš”ì†Œ(ì›ë¬¸ ìš°ì„ , ì‹¤íŒ¨ ì‹œ ìš”ì•½ ë¶„ë¥˜)
    3) ìµœì‹  ë‰´ìŠ¤(ë„¤ì´ë²„)
    """
    # ì†Œê°œ ìš”ì•½
    site_info = fetch_site_intro(homepage or "")
    intro = llm_summarize_intro(site_info.get("about_candidates", []), name) if site_info.get("about_candidates") else ""

    # ì±„ìš© 3ìš”ì†Œ
    responsibilities, qualifications, preferences = [], [], []

    discovered = [job_url] if job_url else discover_job_posting_urls(name, role or "", homepage, limit=3)
    raw_text = ""
    if discovered:
        # ì›ë¬¸ êµ¬ì¡° íŒŒì‹±
        parsed = parse_job_posting_structured(discovered[0])
        responsibilities = parsed["responsibilities"]
        qualifications = parsed["qualifications"]
        preferences = parsed["preferences"]
        # ë¶€ì¡±í•˜ë©´ ìš”ì•½ ë¶„ë¥˜ í´ë°±
        if (not qualifications) or (not preferences):
            raw_text = fetch_page_text(discovered[0])
    else:
        # ê³µê³  URL ì—†ìœ¼ë©´ í™ˆí˜ì´ì§€/í¬í„¸ í…ìŠ¤íŠ¸ë¡œ í´ë°± ìš”ì•½
        # (í™ˆí˜ì´ì§€ í…ìŠ¤íŠ¸ + ê°„ë‹¨ ê²€ìƒ‰)
        texts = []
        if homepage:
            texts.append(fetch_page_text(homepage))
        links = []
        if NAVER_ID and NAVER_SECRET:
            for dom in JOB_SITES:
                links += naver_search_web(f"{name} ì±„ìš© site:{dom}", display=3, sort="date")
        for lk in links[:2]:
            texts.append(fetch_page_text(lk))
        raw_text = "\n\n".join([t for t in texts if t])

    if raw_text and (not qualifications or not preferences):
        try:
            sections = llm_split_job_text_to_sections(raw_text)
            # ì´ë¯¸ ì›ë¬¸ìœ¼ë¡œ ê°€ì ¸ì˜¨ í•­ëª©ì´ ìˆìœ¼ë©´ í•©ì¹˜ë˜ ì¤‘ë³µ ì œê±°
            def merge(a,b):
                seen=set(a); out=a[:]
                for x in b:
                    if x not in seen: seen.add(x); out.append(x)
                return out[:12]
            responsibilities = merge(responsibilities, sections.get("responsibilities",[]))
            qualifications = merge(qualifications, sections.get("qualifications",[]))
            preferences = merge(preferences, sections.get("preferences",[]))
        except Exception:
            pass

    news_items = naver_search_news(name, display=6, sort="date")

    return {
        "company_name": name.strip() or "(íšŒì‚¬ëª… ë¯¸ì„¤ì •)",
        "homepage": homepage or None,
        "role": role or "",
        "company_intro": intro,
        "job_url": discovered[0] if discovered else (job_url or None),
        "responsibilities": responsibilities,
        "qualifications": qualifications,
        "preferences": preferences,
        "news": news_items
    }

# =========================================================
# ë²„íŠ¼: íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° (Primary)
# =========================================================
if st.button("íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°", type="primary"):
    if not company_name_input.strip():
        st.warning("íšŒì‚¬ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("íšŒì‚¬/ì±„ìš©/ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ìš”ì•½ ì¤‘..."):
            st.session_state.company = build_company_context(
                company_name_input, homepage_input or None, role_title or None, job_url_input or None
            )
        # íšŒì‚¬ ë³€ê²½ ì‹œ ì•„ë˜ ì‹¤í–‰ê²°ê³¼ ì´ˆê¸°í™”
        st.session_state.history = []
        st.session_state.answer_text = ""
        st.success("íšŒì‚¬ ì •ë³´ ê°±ì‹  ë° ì‹¤í–‰ê²°ê³¼ ì´ˆê¸°í™” ì™„ë£Œ!")

company = st.session_state.get("company")

# =========================================================
# â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´ (ì„¸ë¡œí˜•)
# =========================================================
st.subheader("â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´")

if company:
    st.markdown(f"**íšŒì‚¬ëª…**  \n{company['company_name']}")
    intro = company.get("company_intro") or "íšŒì‚¬ ì†Œê°œë¥¼ ìš”ì•½í•  ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    st.markdown(f"**ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ(ìš”ì•½)**  \n{intro}")

    link_cols = st.columns(2)
    with link_cols[0]:
        if company.get("job_url"): st.link_button("ì±„ìš© ê³µê³  ì—´ê¸°", company["job_url"])
    with link_cols[1]:
        if company.get("homepage"): st.link_button("í™ˆí˜ì´ì§€ ì—´ê¸°", company["homepage"])

    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    def vlist(col, title, items):
        with col:
            st.markdown(f"### {title}(ìš”ì•½)")
            if items:
                st.markdown("\n".join([f"- {x}" for x in items]))
            else:
                st.caption("ìš”ì•½ ê°€ëŠ¥í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    vlist(col1, "ì£¼ìš”ì—…ë¬´", company.get("responsibilities", []))
    vlist(col2, "ìê²©ìš”ê±´", company.get("qualifications", []))
    vlist(col3, "ìš°ëŒ€ì‚¬í•­", company.get("preferences", []))

    with st.expander("ë””ë²„ê·¸: ê³µê³  ìš”ì•½ ìƒíƒœ"):
        st.json({
            "job_url": company.get("job_url"),
            "resp_cnt": len(company.get("responsibilities",[])),
            "qual_cnt": len(company.get("qualifications",[])),
            "pref_cnt": len(company.get("preferences",[]))
        })
else:
    st.info("ìœ„ì˜ ì…ë ¥ì„ ì™„ë£Œí•˜ê³  â€˜íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°â€™ë¥¼ ëˆŒëŸ¬ ìš”ì•½ì„ ìƒì„±í•˜ì„¸ìš”.")

# =========================================================
# â‘¢ ì§ˆë¬¸ ìƒì„±
# =========================================================
st.subheader("â‘¢ ì§ˆë¬¸ ìƒì„±")

TYPE_INSTRUCTIONS = {
    "í–‰ë™(STAR)": "S(ìƒí™©)-T(ê³¼ì œ)-A(í–‰ë™)-R(ì„±ê³¼)ë¥¼ ìœ ë„í•˜ëŠ” ì‹¤ë¬´ ì‚¬ë¡€ ì§ˆë¬¸",
    "ê¸°ìˆ  ì‹¬ì¸µ": "í•µì‹¬ ê¸°ìˆ ì  ì˜ì‚¬ê²°ì •Â·íŠ¸ë ˆì´ë“œì˜¤í”„Â·ì„±ëŠ¥/ë¹„ìš©/í’ˆì§ˆ ì§€í‘œë¥¼ íŒŒê³ ë“œëŠ” ì‹¬ì¸µ ì§ˆë¬¸",
    "í•µì‹¬ê°€ì¹˜ ì í•©ì„±": "í•µì‹¬ê°€ì¹˜ì™€ íƒœë„ë¥¼ ê²€ì¦í•˜ëŠ” ìƒí™©í˜• ì§ˆë¬¸",
    "ì—­ì§ˆë¬¸": "ì§€ì›ìê°€ íšŒì‚¬ë¥¼ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í†µì°°ë ¥ ìˆëŠ” ì—­ì§ˆë¬¸"
}
q_type = st.selectbox("ì§ˆë¬¸ ìœ í˜•", list(TYPE_INSTRUCTIONS.keys()))
level  = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"])
hint   = st.text_input("ì§ˆë¬¸ ìƒì„± íŒíŠ¸(ì„ íƒ)", placeholder="ì˜ˆ: ì „í™˜ í¼ë„ / ëª¨ë¸ ì„±ëŠ¥-ë¹„ìš© / ë°ì´í„° í’ˆì§ˆ")

def build_ctx_for_q(c: dict) -> str:
    if not c: return ""
    news = ", ".join([_snippetize(n["title"], 70) for n in c.get("news", [])[:3]])
    return textwrap.dedent(f"""
    [íšŒì‚¬ëª…] {c.get('company_name','')}
    [ëª¨ì§‘ ë¶„ì•¼] {c.get('role','')}
    [ì£¼ìš” ì—…ë¬´] {", ".join(c.get('responsibilities', [])[:6])}
    [ìê²© ìš”ê±´] {", ".join(c.get('qualifications', [])[:6])}
    [ìš°ëŒ€ ì‚¬í•­] {", ".join(c.get('preferences', [])[:4])}
    [ìµœê·¼ ì´ìŠˆ/ë‰´ìŠ¤] {news}
    """).strip()

def _similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def pick_diverse(cands: list[str], hist: list[str], gamma: float = 0.35) -> str:
    if not cands: return ""
    if not hist:  return random.choice(cands)
    best=None; best_score=1e9
    for q in cands:
        sims=[_similarity(q,h) for h in hist] or [0.0]
        score=(sum(sims)/len(sims)) + gamma*np.std(sims)
        if score < best_score:
            best_score=score; best=q
    return best

if "current_question" not in st.session_state:
    st.session_state.current_question = ""

# Primary ë²„íŠ¼ + í´ë¦­ ì‹œ ë‹µë³€ì¹¸ ì´ˆê¸°í™”
if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", use_container_width=True, type="primary"):
    st.session_state.answer_text = ""
    try:
        client = get_openai_client()
        ctx = build_ctx_for_q(company or {})
        seed = int(time.time()*1000) % 2_147_483_647
        sys = f"""ë„ˆëŠ” '{company.get('company_name','') if company else 'í•´ë‹¹ íšŒì‚¬'}'ì˜ ë©´ì ‘ê´€ì´ë‹¤.
ì»¨í…ìŠ¤íŠ¸/ì±„ìš© 3ìš”ì†Œ/ìµœê·¼ ì´ìŠˆë¥¼ ë°˜ì˜í•˜ì—¬ **{q_type}** ìœ í˜•({TYPE_INSTRUCTIONS[q_type]})ì˜ ì§ˆë¬¸ **6ê°œ**ë¥¼ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ë¼.
ì„œë¡œ í˜•íƒœÂ·ê´€ì Â·í‚¤ì›Œë“œê°€ ë‹¬ë¼ì•¼ í•˜ë©° ë‚œì´ë„ëŠ” {level}. 
í¬ë§·: 1) ... 2) ... 3) ... (í•œ ì¤„ì”©)"""
        user = f"""[ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n\n[íŒíŠ¸]\n{hint}\n[ëœë¤ì‹œë“œ] {seed}"""
        resp = client.chat.completions.create(
            model=MODEL, temperature=0.9,
            messages=[{"role":"system","content":sys},{"role":"user","content":user}]
        )
        raw = resp.choices[0].message.content.strip()
        cands = [re.sub(r'^\s*\d+\)\s*','',line).strip() for line in raw.splitlines() if re.match(r'^\s*\d+\)', line)]
        if not cands:
            cands = [l.strip("- ").strip() for l in raw.splitlines() if len(l.strip())>0][:6]
        hist_qs = [h["question"] for h in st.session_state.history][-12:]
        st.session_state.current_question = pick_diverse(cands, hist_qs) or (cands[0] if cands else "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
    except Exception as e:
        st.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")

st.text_area("ì§ˆë¬¸", height=110, value=st.session_state.get("current_question",""))

# =========================================================
# â‘£ ë‚˜ì˜ ë‹µë³€ / ì±„ì  & ì½”ì¹­ (JSON ê³ ì •, ì´ì =í•©ì‚°)
# =========================================================
st.subheader("â‘£ ë‚˜ì˜ ë‹µë³€ / ì½”ì¹­")
ans = st.text_area("ì—¬ê¸°ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš” (STAR ê¶Œì¥: ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)", height=200, key="answer_text")

def evaluate_answer(company: dict, question: str, answer: str) -> dict:
    client = get_openai_client()
    news = ", ".join([_snippetize(n["title"], 70) for n in (company or {}).get("news", [])[:3]])
    ctx = textwrap.dedent(f"""
    [íšŒì‚¬ëª…] {(company or {}).get('company_name','')}
    [ëª¨ì§‘ ë¶„ì•¼] {(company or {}).get('role','')}
    [ì£¼ìš” ì—…ë¬´] {", ".join((company or {}).get('responsibilities', [])[:6])}
    [ìê²© ìš”ê±´] {", ".join((company or {}).get('qualifications', [])[:6])}
    [ìš°ëŒ€ ì‚¬í•­] {", ".join((company or {}).get('preferences', [])[:4])}
    [ìµœê·¼ ì´ìŠˆ/ë‰´ìŠ¤] {news}
    """).strip()

    sys = (
        "ë„ˆëŠ” í†±í‹°ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. ì•„ë˜ ìŠ¤í‚¤ë§ˆì— ë§ì¶˜ **í•œêµ­ì–´ JSONë§Œ** ì¶œë ¥í•˜ë¼.\n"
        f"ìŠ¤í‚¤ë§ˆ: {json.dumps(EVAL_SCHEMA_HINT, ensure_ascii=False)}\n"
        "ì„¤ëª…/ì¶”ê°€ í…ìŠ¤íŠ¸ ê¸ˆì§€. ê° ê¸°ì¤€(score 0~20)ì€ ì§ˆë¬¸ê³¼ ë‹µë³€/íšŒì‚¬ ë§¥ë½/ì±„ìš© 3ìš”ì†Œ ë¶€í•© ì—¬ë¶€ë¡œ ì±„ì í•˜ë¼."
        "ê° ê¸°ì¤€ì— ëŒ€í•´ comment(ì§§ì€ ì¹­ì°¬/í•µì‹¬ìš”ì§€), deduct(ê°ì ìš”ì¸), improve(ê°œì„  í¬ì¸íŠ¸)ë¥¼ ê°„ë‹¨íˆ ì±„ì›Œë¼."
    )
    user = f"""[íšŒì‚¬/ì§ë¬´ ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n\n[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n[í›„ë³´ì ë‹µë³€]\n{answer}"""
    resp = client.chat.completions.create(
        model=MODEL, temperature=0.3,
        messages=[{"role":"system","content":sys},{"role":"user","content":user}],
        response_format={"type":"json_object"}
    )
    data = json.loads(resp.choices[0].message.content)

    # ì´ì  ì¼ì›í™”: í•©ì‚°
    factors = data.get("factors", {})
    sum_score = sum(int(factors[k]["score"]) for k in EVAL_FACTORS if k in factors and isinstance(factors[k].get("score"), int))
    data["sum_score"] = max(0, min(100, sum_score))  # 5*20 = 100
    return data

if st.button("ì±„ì  & ì½”ì¹­", type="primary", use_container_width=True):
    if not st.session_state.get("current_question"):
        st.warning("ë¨¼ì € 'ìƒˆ ì§ˆë¬¸ ë°›ê¸°'ë¡œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì±„ì /ì½”ì¹­ ì¤‘..."):
            data = evaluate_answer(company or {}, st.session_state["current_question"], st.session_state.answer_text)

            # íˆìŠ¤í† ë¦¬ ì €ì¥ (ëˆ„ì ì— ì‚¬ìš©)
            row = {k: (data["factors"].get(k,{}).get("score") if data.get("factors") else None) for k in EVAL_FACTORS}
            st.session_state.history.append({
                "ts": pd.Timestamp.now(),
                "question": st.session_state["current_question"],
                "answer": st.session_state.answer_text,
                "sum_score": data.get("sum_score"),
                "factors": row,
                "comments": {k: data["factors"].get(k,{}).get("comment") for k in EVAL_FACTORS} if data.get("factors") else {},
                "deducts": {k: data["factors"].get(k,{}).get("deduct") for k in EVAL_FACTORS} if data.get("factors") else {},
                "improves": {k: data["factors"].get(k,{}).get("improve") for k in EVAL_FACTORS} if data.get("factors") else {},
                "strengths": data.get("strengths",[]),
                "risks": data.get("risks",[]),
                "improvements": data.get("improvements",[]),
                "revised": data.get("revised",""),
                "raw": data
            })

# =========================================================
# ê²°ê³¼ ë Œë”ë§ (ì´ì =í•©ì‚°ìœ¼ë¡œ ì¼ì›í™” / í‘œ+ìˆ˜ì •ë³¸)
# =========================================================
st.divider()
st.subheader("í”¼ë“œë°± ê²°ê³¼")

if st.session_state.history:
    last = st.session_state.history[-1]
    # ì¢Œì¸¡/ìš°ì¸¡ ì´ì  ë™ì¼ (sum_score)
    c1,c2 = st.columns([1,3])
    with c1:
        st.metric("ì´ì (/100)", last.get("sum_score","â€”"))
    with c2:
        st.markdown(f"**ì´ì : {last.get('sum_score','â€”')}/100**")
        # ê¸°ì¤€ë³„ ê·¼ê±°(ì ìˆ˜/ê°ì /ê°œì„ )
        st.markdown("**2. ê¸°ì¤€ë³„ ê·¼ê±°(ì ìˆ˜/ê°ì /ê°œì„ ):**")
        table_rows=[]
        for k in EVAL_FACTORS:
            sc = last["factors"].get(k)
            comment = (last["comments"] or {}).get(k,"") if isinstance(last.get("comments"), dict) else ""
            deduct  = (last["deducts"] or {}).get(k,"") if isinstance(last.get("deducts"), dict) else ""
            improve = (last["improves"] or {}).get(k,"") if isinstance(last.get("improves"), dict) else ""
            table_rows.append((f"{k}({sc if sc is not None else '-'}/20)", f"ê°•ì : {comment or '-'} / ê°ì : {deduct or '-'} / ê°œì„ : {improve or '-'}"))
        df = pd.DataFrame(table_rows, columns=["ê¸°ì¤€(ì ìˆ˜)","ì½”ë©˜íŠ¸"])
        st.dataframe(df, use_container_width=True, hide_index=True)

        # ê°•ì /ë¦¬ìŠ¤í¬/ê°œì„  í¬ì¸íŠ¸(ëª¨ë¸ ì œê³µ)
        if last.get("strengths"):
            st.markdown("**3. ê°•ì :**\n" + "\n".join([f"- {x}" for x in last["strengths"]]))
        if last.get("risks"):
            st.markdown("**4. ë¦¬ìŠ¤í¬:**\n" + "\n".join([f"- {x}" for x in last["risks"]]))
        if last.get("improvements"):
            st.markdown("**5. ê°œì„  í¬ì¸íŠ¸:**\n" + "\n".join([f"- {x}" for x in last["improvements"]]))

        # ìˆ˜ì •ë³¸ ë‹µë³€
        if last.get("revised"):
            st.markdown("**6. ìˆ˜ì •ë³¸ ë‹µë³€:**")
            st.markdown(last["revised"])

else:
    st.caption("ì•„ì§ ì±„ì  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# =========================================================
# â‘¥ ì—­ëŸ‰ ë ˆì´ë” (ìµœê·¼ vs ì„¸ì…˜ í‰ê· ) + ëˆ„ì  í…Œì´ë¸”(í•©ê³„ í¬í•¨)
# =========================================================
st.divider()
st.subheader("ì—­ëŸ‰ ë ˆì´ë” (ì„¸ì…˜ ëˆ„ì , NAëŠ” 0ìœ¼ë¡œ í‘œì‹œ)")

def history_df(hist):
    """
    ìµœê·¼ ì ìˆ˜ / ì„¸ì…˜ í‰ê·  DataFrame
    """
    if not hist: return None, None
    rows=[]
    for h in hist:
        rows.append([h["factors"].get(k) for k in EVAL_FACTORS])
    df = pd.DataFrame(rows, columns=EVAL_FACTORS)

    # ìµœê·¼
    latest = df.iloc[-1].copy()
    # í‰ê·  (NaN ì œì™¸)
    avg = df.astype("float").mean(skipna=True)
    return latest, avg

latest, avg = history_df(st.session_state.history)

if latest is not None:
    # ë ˆì´ë” ë°ì´í„°(NAëŠ” 0ìœ¼ë¡œ ëŒ€ì²´í•´ ì‹œê°í™”ë§Œ)
    r_latest = [float(x) if pd.notna(x) else 0.0 for x in latest.tolist()]
    r_avg = [float(x) if pd.notna(x) else 0.0 for x in avg.tolist()]

    if PLOTLY_OK:
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(
            r=r_latest+[r_latest[0]], theta=EVAL_FACTORS+[EVAL_FACTORS[0]],
            fill='toself', name="ìµœê·¼"
        ))
        fig.add_trace(go.Scatterpolar(
            r=r_avg+[r_avg[0]], theta=EVAL_FACTORS+[EVAL_FACTORS[0]],
            fill='toself', name="ì„¸ì…˜ í‰ê· ", opacity=0.35
        ))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,20])), showlegend=True, height=450)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.bar_chart(pd.DataFrame({"ìµœê·¼": r_latest, "ì„¸ì…˜í‰ê· ": r_avg}, index=EVAL_FACTORS))

    # ëˆ„ì  í…Œì´ë¸” (ìµœê·¼ í–‰ + í•©ê³„ ì—´)
    table = pd.DataFrame([latest.tolist()], columns=EVAL_FACTORS)
    table["í•©ê³„(0~100)"] = table[EVAL_FACTORS].sum(axis=1, numeric_only=True)
    st.dataframe(table, use_container_width=True)
    st.caption("í‘œì˜ ê° ì¶•ì€ ìµœì‹  ê²°ê³¼ì˜ ì ìˆ˜(NAëŠ” '-')ì…ë‹ˆë‹¤. ìœ„ ë ˆì´ë”/ì•„ë˜ í‘œì—ëŠ” í•©ê³„(0~100)ì™€ ì„¸ì…˜ ëˆ„ì  í‰ê· ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
else:
    st.caption("ì•„ì§ ì—­ëŸ‰ ì ìˆ˜ê°€ íŒŒì‹±ëœ ì½”ì¹­ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# =========================================================
# â‘¦ ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV)
# =========================================================
st.divider()
st.subheader("ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV)")

def build_report(hist):
    rows=[]
    for h in hist:
        row={"timestamp":h.get("ts"),"question":h.get("question"),"answer":h.get("answer"),
             "sum_score":h.get("sum_score")}
        # factors
        for k in EVAL_FACTORS:
            row[f"score_{k}"] = (h.get("factors") or {}).get(k)
            row[f"comment_{k}"] = (h.get("comments") or {}).get(k)
            row[f"deduct_{k}"] = (h.get("deducts") or {}).get(k)
            row[f"improve_{k}"] = (h.get("improves") or {}).get(k)
        row["revised"] = h.get("revised","")
        rows.append(row)
    cols = ["timestamp","question","answer","sum_score"] + \
           [f"score_{k}" for k in EVAL_FACTORS] + \
           [f"comment_{k}" for k in EVAL_FACTORS] + \
           [f"deduct_{k}" for k in EVAL_FACTORS] + \
           [f"improve_{k}" for k in EVAL_FACTORS] + ["revised"]
    return pd.DataFrame(rows)[cols] if rows else pd.DataFrame(columns=cols)

rep = build_report(st.session_state.history)
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=rep.to_csv(index=False).encode("utf-8-sig"),
                   file_name="interview_session_report.csv", mime="text/csv")

st.caption("Tip) ê³µê³  URLì´ ì—†ë”ë¼ë„ í™ˆí˜ì´ì§€/í¬í„¸ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½ ë¶„ë¥˜ í´ë°±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. íšŒì‚¬ ë³€ê²½ ì‹œ ê²°ê³¼ëŠ” ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")
