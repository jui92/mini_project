# -*- coding: utf-8 -*-
# =====================================================================
# íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜ (KR ìµœì í™”)
# - íšŒì‚¬ ì†Œê°œë§Œ LLM ìš”ì•½ / ì—…ë¬´Â·ìê²©Â·ìš°ëŒ€ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ë…¸ì¶œ
# - íšŒì‚¬ ë³€ê²½ì‹œ í•˜ë‹¨ ê²°ê³¼ ì´ˆê¸°í™”
# - 100ì ì œ ì±„ì (ì§ˆë¬¸ ìœ í˜•ë³„ ë£¨ë¸Œë¦­ ì ìš©, ë¹„ì ìš© í•­ëª©ì€ '-')
# - ì´ì  í‘œì‹œ ì¼ê´€í™”(ì¢ŒÂ·ìš° ë™ì¼)
# - ë ˆì´ë” í‘œì— 'í•©ê³„' ì»¬ëŸ¼ ì¶”ê°€, ê³„ì‚° ì˜¤ë¥˜ ìˆ˜ì •
# =====================================================================

import os, io, re, json, textwrap, urllib.parse, difflib, random, time, functools
from typing import List, Dict, Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st

# ---------------- Optional deps ----------------
try:
    import pypdf
except Exception:
    pypdf = None

try:
    import plotly.graph_objects as go
    PLOTLY_OK = True
except Exception:
    PLOTLY_OK = False

try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

import requests
from bs4 import BeautifulSoup

# ---------------- Page config ----------------
st.set_page_config(page_title="íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜", page_icon="ğŸ¯", layout="wide")

# ---------------- Secrets/Keys ----------------
def _secrets_file_exists() -> bool:
    paths = [
        os.path.join(os.path.expanduser("~"), ".streamlit", "secrets.toml"),
        os.path.join(os.getcwd(), ".streamlit", "secrets.toml"),
    ]
    return any(os.path.exists(p) for p in paths)

def load_api_key_from_env_or_secrets() -> Optional[str]:
    key = os.getenv("OPENAI_API_KEY")
    if key: return key
    try:
        if _secrets_file_exists() or hasattr(st, "secrets"):
            return st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        pass
    return None

def load_naver_keys():
    cid = os.getenv("NAVER_CLIENT_ID")
    csec = os.getenv("NAVER_CLIENT_SECRET")
    try:
        if hasattr(st, "secrets"):
            cid = cid or st.secrets.get("NAVER_CLIENT_ID", None)
            csec = csec or st.secrets.get("NAVER_CLIENT_SECRET", None)
    except Exception:
        pass
    return cid, csec

NAVER_ID, NAVER_SECRET = load_naver_keys()

# ---------------- Utils ----------------
def _clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", (t or "")).strip()

def _snippetize(text: str, maxlen: int = 220) -> str:
    t = _clean_text(text)
    return t if len(t) <= maxlen else t[: maxlen - 1] + "â€¦"

def chunk_text(text: str, size: int = 900, overlap: int = 150):
    text = re.sub(r"\s+", " ", text).strip()
    if not text: return []
    out, start = [], 0
    while start < len(text):
        end = min(len(text), start + size)
        out.append(text[start:end])
        if end == len(text): break
        start = max(0, end - overlap)
    return out

def read_file_to_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt", ".md")):
        for enc in ("utf-8", "cp949", "euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        if pypdf is None:
            st.warning("pypdfê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— pypdf ì¶”ê°€.")
            return ""
        try:
            reader = pypdf.PdfReader(io.BytesIO(data))
            return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
        except Exception as e:
            st.warning(f"PDF íŒŒì‹± ì‹¤íŒ¨({uploaded.name}): {e}")
            return ""
    return ""

VAL_KEYWORDS = ["í•µì‹¬ê°€ì¹˜","ê°€ì¹˜","ë¯¸ì…˜","ë¹„ì „","ë¬¸í™”","ì›ì¹™","ì² í•™","ê³ ê°","ë°ì´í„°","í˜ì‹ ",
                "values","mission","vision","culture","principles","philosophy","customer","data","innovation"]

def _domain(u: str|None) -> str|None:
    if not u: return None
    try:
        if not u.startswith("http"): u = "https://" + u
        return urllib.parse.urlparse(u).netloc.lower().replace("www.","")
    except Exception:
        return None

def _name_similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()

# ---------------- Simple cache wrapper ----------------
@functools.lru_cache(maxsize=256)
def _cached_get(url: str, timeout: int = 8) -> Optional[str]:
    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code == 200 and "text/html" in r.headers.get("content-type",""):
            return r.text
    except Exception:
        pass
    return None

# ---------------- NAVER OPEN API ----------------
def _naver_api_get(api: str, params: dict, cid: str, csec: str):
    url = f"https://openapi.naver.com/v1/search/{api}.json"
    headers = {
        "X-Naver-Client-Id": cid,
        "X-Naver-Client-Secret": csec,
        "User-Agent": "Mozilla/5.0",
    }
    try:
        r = requests.get(url, headers=headers, params=params, timeout=8)
        if r.status_code != 200:
            return None
        return r.json()
    except Exception:
        return None

def naver_search_news(query: str, display: int = 6, sort: str = "date") -> list[dict]:
    cid, csec = load_naver_keys()
    if not (cid and csec): return []
    js = _naver_api_get("news", {"query": query, "display": display, "sort": sort}, cid, csec)
    if not js: return []
    out=[]
    for it in js.get("items", []):
        title = _clean_text(re.sub(r"</?b>|&quot;|&apos;|&amp;|&lt;|&gt;", "", it.get("title","")))
        out.append({"title": title, "link": it.get("link"), "pubDate": it.get("pubDate")})
    return out

def naver_search_web(query: str, display: int = 5, sort: str = "date") -> list[str]:
    cid, csec = load_naver_keys()
    if not (cid and csec): return []
    js = _naver_api_get("webkr", {"query": query, "display": display, "sort": sort}, cid, csec)
    if not js: return []
    links=[]
    for it in js.get("items", []):
        link = it.get("link")
        if link and link not in links: links.append(link)
    return links

# ---------------- Crawl: site snippets ----------------
def fetch_site_snippets(base_url: str | None, company_name_hint: str | None = None) -> dict:
    if not base_url:
        return {"values": [], "recent": [], "site_name": None, "about": None}
    url0 = base_url.strip()
    if not url0.startswith("http"): url0 = "https://" + url0
    cand_paths = ["", "/", "/about", "/company", "/about-us", "/mission", "/values", "/culture"]
    values_found, recent_found = [], []
    site_name, about_para = None, None
    for path in cand_paths[:4]:  # ì†ë„: ê²½ë¡œ ìˆ˜ ì œí•œ
        url = url0.rstrip("/") + path
        html = _cached_get(url, timeout=6)
        if not html: continue
        soup = BeautifulSoup(html, "html.parser")
        if site_name is None:
            og = soup.find("meta", {"property":"og:site_name"}) or soup.find("meta", {"name":"application-name"})
            if og and og.get("content"): site_name = _clean_text(og["content"])
            elif soup.title and soup.title.string: site_name = _clean_text(soup.title.string.split("|")[0])
        if about_para is None:
            hero = soup.find(["p","div"], class_=re.compile(r"(lead|hero|intro)", re.I))
            if hero: about_para = _snippetize(hero.get_text(" "))
        for tag in soup.find_all(["h1","h2","h3","p","li"]):
            txt = _clean_text(tag.get_text(separator=" "))
            if 10 <= len(txt) <= 220:
                if any(k.lower() in txt.lower() for k in VAL_KEYWORDS):
                    values_found.append(txt)
                if any(k in txt for k in ["í”„ë¡œì íŠ¸","ê°œë°œ","ì¶œì‹œ","ì„±ê³¼","project","launched","release","delivered","improved"]):
                    recent_found.append(txt)
    if company_name_hint and site_name and _name_similarity(company_name_hint, site_name) < 0.35:
        values_found, recent_found = [], []
    def dedup(lst):
        seen=set(); out=[]
        for x in lst:
            if x not in seen: seen.add(x); out.append(x)
        return out
    values_found = dedup(values_found)[:5]
    recent_found = dedup(recent_found)[:5]
    trimmed=[]
    for v in values_found:
        v2 = v.split(":",1)[-1]
        if len(v2)>60 and "," in v2:
            trimmed += [p.strip() for p in v2.split(",") if 2<=len(p.strip())<=24][:6]
        else:
            trimmed.append(v2[:60])
    return {"values": trimmed[:6], "recent": recent_found, "site_name": site_name, "about": about_para}

# ---------------- Discover job/career links ----------------
CAREER_HINTS = ["careers","career","jobs","job","recruit","recruiting","join","hire","hiring","ì±„ìš©","ì¸ì¬","ì…ì‚¬ì§€ì›","ì±„ìš©ê³µê³ ","ì¸ì¬ì˜ì…","ì»¤ë¦¬ì–´"]

def discover_job_from_homepage(homepage: str, limit: int = 4) -> list[str]:
    if not homepage: return []
    try:
        if not homepage.startswith("http"): homepage = "https://" + homepage
        html = _cached_get(homepage, timeout=8)
        if not html: return []
        soup = BeautifulSoup(html, "html.parser")
        links=[]
        for path in ["careers","recruit","jobs","career","ì±„ìš©","ì¸ì¬ì˜ì…","recruitment","join"]:
            links.append(urllib.parse.urljoin(homepage.rstrip("/") + "/", path))
        for a in soup.find_all("a", href=True):
            href = a["href"]; text = (a.get_text() or "").lower()
            if any(k in href.lower() or k in text for k in CAREER_HINTS):
                links.append(urllib.parse.urljoin(homepage, href))
        out=[]; seen=set()
        for lk in links:
            d = _domain(lk)
            if lk not in seen and d:
                seen.add(lk); out.append(lk)
                if len(out) >= limit: break
        return out[:limit]
    except Exception:
        return []

# ---------------- News fetch ----------------
def fetch_news(company_name: str, max_items: int = 6) -> list[dict]:
    news = naver_search_news(company_name, display=max_items, sort="date")
    if news: return news
    # fallback: Google News RSS
    q = urllib.parse.quote(company_name)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    items=[]
    try:
        r = requests.get(url, timeout=8); 
        if r.status_code != 200: return []
        soup = BeautifulSoup(r.text, "xml")
        for it in soup.find_all("item")[:max_items]:
            title = _clean_text(it.title.get_text()) if it.title else ""
            link  = it.link.get_text() if it.link else ""
            pub   = it.pubDate.get_text() if it.pubDate else ""
            items.append({"title": title, "link": link, "pubDate": pub})
    except Exception:
        return []
    return items

# ---------------- Job posting parser ----------------
def _extract_json_ld_job(soup: BeautifulSoup) -> Optional[dict]:
    for s in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(s.string or "")
            seq = data if isinstance(data, list) else [data]
            for obj in seq:
                typ = obj.get("@type") if isinstance(obj, dict) else None
                if (isinstance(typ, list) and "JobPosting" in typ) or typ == "JobPosting":
                    return obj
        except Exception:
            continue
    return None

def parse_job_posting(url: str) -> dict:
    """
    ë°˜í™˜: title, responsibilities(ì›ë¬¸ ë¦¬ìŠ¤íŠ¸), qualifications(ì›ë¬¸ ë¦¬ìŠ¤íŠ¸), preferred(ì›ë¬¸ ë¦¬ìŠ¤íŠ¸), company_intro
    - 'ì›ë¬¸ ê·¸ëŒ€ë¡œ'ë¥¼ ìµœëŒ€í•œ ë³´ì¡´
    """
    out = {"title": None, "responsibilities": [], "qualifications": [], "preferred": [], "company_intro": None}
    try:
        r = requests.get(url, timeout=12, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""): return out
        soup = BeautifulSoup(r.text, "html.parser")

        # JSON-LD ìš°ì„ 
        jp = _extract_json_ld_job(soup)
        if jp:
            out["title"] = jp.get("title")
            desc = _clean_text(jp.get("description", ""))
            if desc:
                bullets = re.split(r"[\n\r]+|[â€¢Â·â–ªï¸â–¶ï¸\-]\s+", desc)
                bullets = [b.strip(" -â€¢Â·â–ªï¸â–¶ï¸") for b in bullets if len(b.strip()) > 3]
                for b in bullets:
                    low = b.lower()
                    if any(k in low for k in ["preferred","ìš°ëŒ€","nice to have"]):
                        out["preferred"].append(b)
                    elif any(k in low for k in ["requirement","ìê²©","ìš”ê±´","qualification","í•„ìˆ˜"]):
                        out["qualifications"].append(b)
                    else:
                        out["responsibilities"].append(b)

        # í—¤ë”/ì„¹ì…˜ íœ´ë¦¬ìŠ¤í‹±
        sections = {}
        for h in soup.find_all(re.compile("^h[1-4]$")):
            head = _clean_text(h.get_text())
            if not head: continue
            nxt=[]; sib=h.find_next_sibling(); stop={"h1","h2","h3","h4"}
            while sib and sib.name not in stop:
                if sib.name in {"p","li","ul","ol","div"}:
                    txt=_clean_text(sib.get_text(" "))
                    if len(txt)>5: nxt.append(txt)
                sib=sib.find_next_sibling()
            if nxt: sections[head]="\n".join(nxt)

        def pick(keys):
            for k in sections:
                if any(kk.lower() in k.lower() for kk in keys): return sections[k]
            return None

        resp = pick(["ì£¼ìš” ì—…ë¬´","ë‹´ë‹¹ ì—…ë¬´","ì—…ë¬´","Responsibilities","What you will do","Role"])
        qual = pick(["ìê²© ìš”ê±´","ì§€ì› ìê²©","Requirements","Qualifications","Must have"])
        pref = pick(["ìš°ëŒ€","ìš°ëŒ€ì‚¬í•­","Preferred","Nice to have"])

        def bullets_from(txt):
            if not txt: return []
            arr = re.split(r"[\n\r]+|[â€¢Â·â–ªï¸â–¶ï¸\-]\s+", txt)
            return [a.strip(" -â€¢Â·â–ªï¸â–¶ï¸") for a in arr if len(a.strip())>3][:20]

        if resp and not out["responsibilities"]: out["responsibilities"]=bullets_from(resp)
        if qual and not out["qualifications"]:   out["qualifications"]=bullets_from(qual)
        if pref and not out["preferred"]:        out["preferred"]=bullets_from(pref)

        meta_desc = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
        if meta_desc and meta_desc.get("content"): out["company_intro"]=_snippetize(meta_desc["content"], 220)

        # ì›ë¬¸ ë³´ì¡´(ìš”ì•½ ê¸ˆì§€) â†’ ê¸¸ì´ë§Œ í´ë¨í”„
        out["responsibilities"]=[_snippetize(x,200) for x in out["responsibilities"]][:15]
        out["qualifications"]  =[_snippetize(x,200) for x in out["qualifications"]][:15]
        out["preferred"]       =[_snippetize(x,200) for x in out["preferred"]][:15]

    except Exception:
        pass
    return out

# ---------------- OpenAI setup ----------------
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    API_KEY = load_api_key_from_env_or_secrets()
    if not API_KEY:
        st.info("í™˜ê²½ë³€ìˆ˜/Secretsì—ì„œ í‚¤ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ì— ì…ë ¥ í›„ ì—”í„°.")
        API_KEY = st.text_input("OPENAI_API_KEY", type="password")
    MODEL = st.selectbox("ì±— ëª¨ë¸", ["gpt-4o-mini","gpt-4o","gpt-4.1-mini"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small","text-embedding-3-large"], index=0)

    _openai_ver = None; _httpx_ver = None
    try:
        import openai as _openai_pkg; _openai_ver = getattr(_openai_pkg, "__version__", None)
    except Exception: pass
    try:
        import httpx as _httpx_pkg; _httpx_ver = getattr(_httpx_pkg, "__version__", None)
    except Exception: pass
    with st.expander("ë””ë²„ê·¸: ì‹œí¬ë¦¿/ë²„ì „ ìƒíƒœ"):
        st.write({
            "api_key_provided": bool(API_KEY),
            "naver_keys": bool(NAVER_ID and NAVER_SECRET),
            "openai_version": _openai_ver,
            "httpx_version": _httpx_ver,
        })

if not API_KEY:
    st.error("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."); st.stop()
try:
    client = OpenAI(api_key=API_KEY, timeout=30.0)
except Exception as e:
    st.error(f"OpenAI ì´ˆê¸°í™” ì˜¤ë¥˜: {e}"); st.stop()

# =====================================================================
# â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥
# =====================================================================
st.subheader("â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥")
company_name_input = st.text_input("íšŒì‚¬ ì´ë¦„ (ê·¸ëŒ€ë¡œ ì‚¬ìš©)", placeholder="ì˜ˆ: ë„¤ì´ë²„ / Kakao / ì‚¼ì„±SDS")
homepage_input     = st.text_input("ê³µì‹ í™ˆí˜ì´ì§€ URL(ì„ íƒ)", placeholder="https://...")
role_title         = st.text_input("ì§€ì› ì§ë¬´ëª…", placeholder="ë°ì´í„° ì• ë„ë¦¬ìŠ¤íŠ¸ / ML ì—”ì§€ë‹ˆì–´ ...")
job_url_input      = st.text_input("ì±„ìš© ê³µê³  URL(ì„ íƒ) â€” ì—†ë‹¤ë©´ ìë™ íƒìƒ‰")

# ì„¸ì…˜ ì´ˆê¸°ê°’
for key, val in [
    ("company_state", {}),
    ("history", []),
    ("current_question", ""),
    ("answer_text", ""),
    ("rag_store", {"chunks": [], "embeds": None}),
]:
    if key not in st.session_state: st.session_state[key] = val

def discover_job_posting_urls(company_name: str, role: str, homepage: str|None, limit: int = 4) -> list[str]:
    urls=[]
    urls += discover_job_from_homepage(homepage, limit=limit) if homepage else []
    if urls: return urls[:limit]
    if NAVER_ID and NAVER_SECRET:
        JOB_SITES = ["wanted.co.kr","saramin.co.kr","jobkorea.co.kr","rocketpunch.com",
                     "indeed.com","linkedin.com","recruit.navercorp.com","kakao.recruit","naver"]
        for dom in JOB_SITES:
            if len(urls) >= limit: break
            q = f"{company_name} {role} site:{dom}" if role else f"{company_name} ì±„ìš© site:{dom}"
            links = naver_search_web(q, display=5, sort="date")
            for lk in links:
                if lk not in urls: urls.append(lk)
            if len(urls) >= limit: break
    if urls: return urls[:limit]
    # fallback duckduckgo
    engine = "https://duckduckgo.com/html/?q={query}"
    site_part = " OR ".join([f'site:{d}' for d in ["wanted.co.kr","saramin.co.kr","jobkorea.co.kr","rocketpunch.com"]])
    q = f'{company_name} {role} ({site_part})' if role else f'{company_name} ì±„ìš© ({site_part})'
    url = engine.format(query=urllib.parse.quote(q))
    html = _cached_get(url, timeout=8)
    if html:
        soup = BeautifulSoup(html, "html")
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if href.startswith("/l/?kh=-1&uddg="):
                href = urllib.parse.unquote(href.split("/l/?kh=-1&uddg=")[-1])
            if href not in urls: urls.append(href)
            if len(urls) >= limit: break
    return urls[:limit]

def build_company_obj(name: str, homepage: str|None, role: str|None, job_url: str|None) -> dict:
    site = fetch_site_snippets(homepage or None, name)
    discovered = [job_url] if job_url else discover_job_posting_urls(name, role or "", homepage, limit=3)
    jp = parse_job_posting(discovered[0]) if discovered else {"title":None,"responsibilities":[],"qualifications":[],"preferred":[],"company_intro":None}
    news_items = fetch_news(name, max_items=6)
    return {
        "company_name": name.strip() or "(íšŒì‚¬ëª… ë¯¸ì„¤ì •)",
        "homepage": homepage or None,
        "values": site.get("values", []),
        "recent_projects": site.get("recent", []),
        "company_intro_site": site.get("about"),
        "role": role or "",
        "job_url": discovered[0] if discovered else (job_url or None),
        # ---- ì›ë¬¸ ê·¸ëŒ€ë¡œ(ìš”ì•½ ê¸ˆì§€) ----
        "role_responsibilities": jp.get("responsibilities", []),
        "role_qualifications":   jp.get("qualifications", []),
        "role_preferred":        jp.get("preferred", []),
        "news": news_items
    }

def summarize_company_intro_only(c: dict) -> str:
    """íšŒì‚¬ ì†Œê°œë§Œ LLM ìš”ì•½. ì—…ë¬´Â·ìê²©Â·ìš°ëŒ€ëŠ” ì•„ë˜ ì„¹ì…˜ì—ì„œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë…¸ì¶œ."""
    ctx = textwrap.dedent(f"""
    [í™ˆí˜ì´ì§€ ì†Œê°œ(ë°œì·Œ)] {c.get('company_intro_site') or ''}
    [ìµœê·¼ ì´ìŠˆ/ë‰´ìŠ¤ íƒ€ì´í‹€] {', '.join([_snippetize(n['title'],70) for n in c.get('news', [])[:3]])}
    """).strip()
    sys = ("ë„ˆëŠ” ì±„ìš©ë‹´ë‹¹ìë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'íšŒì‚¬ ì†Œê°œ'ë§Œ 2~3ë¬¸ì¥ í•œêµ­ì–´ ìš”ì•½ìœ¼ë¡œ ì‘ì„±í•˜ë¼. "
           "ê´‘ê³ ì„± ë¬¸êµ¬/í˜•ìš©ì‚¬ëŠ” ìµœì†Œí™”í•˜ê³ , ì‚¬ì‹¤ ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê²Œ.")
    user = f"{ctx}\n\n[íšŒì‚¬ëª…] {c.get('company_name','')}"
    try:
        resp = client.chat.completions.create(
            model=MODEL, temperature=0.2,
            messages=[{"role":"system","content":sys},{"role":"user","content":user}]
        )
        return resp.choices[0].message.content.strip()
    except Exception:
        return c.get("company_intro_site") or "íšŒì‚¬ ì†Œê°œ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

# ---- íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° ----
if st.button("íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°", type="primary"):
    if not company_name_input.strip():
        st.warning("íšŒì‚¬ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("íšŒì‚¬/ê³µê³ /ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
            cobj = build_company_obj(company_name_input, homepage_input or None, role_title or None, job_url_input or None)
            intro_md = summarize_company_intro_only(cobj)
            # ìƒíƒœ ì €ì¥
            st.session_state.company_state = {"company": cobj, "intro_md": intro_md}
            # ---- íšŒì‚¬ ë³€ê²½ ì‹œ í•˜ë‹¨ ì´ˆê¸°í™” ----
            st.session_state.current_question = ""
            st.session_state.answer_text = ""
            st.session_state.history = []
            st.session_state.rag_store = {"chunks": [], "embeds": None}
        st.success("íšŒì‚¬ ì •ë³´ ê°±ì‹  ë° ê²°ê³¼ ì´ˆê¸°í™” ì™„ë£Œ")

company = st.session_state.get("company_state",{}).get("company", None)
intro_md = st.session_state.get("company_state",{}).get("intro_md", None)

# =====================================================================
# â‘¡ íšŒì‚¬ ìš”ì•½ (ì†Œê°œë§Œ ìš”ì•½) + ì—…ë¬´/ìê²©/ìš°ëŒ€ ì›ë¬¸
# =====================================================================
st.subheader("â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´")
if company and intro_md:
    st.markdown(f"**íšŒì‚¬ëª…**: {company.get('company_name')}")
    st.markdown("**íšŒì‚¬ ì†Œê°œ(ìš”ì•½)**")
    st.markdown(intro_md)
    if company.get("homepage"): st.link_button("í™ˆí˜ì´ì§€ ì—´ê¸°", company["homepage"])
    if company.get("job_url"):  st.link_button("ì±„ìš© ê³µê³  ì—´ê¸°", company["job_url"])
    st.markdown("---")
    st.markdown(f"**ëª¨ì§‘ ë¶„ì•¼**: {company.get('role') or 'â€”'}")
    c1,c2,c3 = st.columns(3)
    with c1:
        st.markdown("**ì£¼ìš” ì—…ë¬´(ì›ë¬¸)**")
        if company["role_responsibilities"]:
            st.markdown("- " + "\n- ".join(company["role_responsibilities"]))
        else:
            st.caption("ê³µê³ ì—ì„œ ì¶”ì¶œëœ ì£¼ìš” ì—…ë¬´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    with c2:
        st.markdown("**ìê²© ìš”ê±´(ì›ë¬¸)**")
        if company["role_qualifications"]:
            st.markdown("- " + "\n- ".join(company["role_qualifications"]))
        else:
            st.caption("ê³µê³ ì—ì„œ ì¶”ì¶œëœ ìê²© ìš”ê±´ì´ ì—†ìŠµë‹ˆë‹¤.")
    with c3:
        st.markdown("**ìš°ëŒ€ ì‚¬í•­(ì›ë¬¸)**")
        if company["role_preferred"]:
            st.markdown("- " + "\n- ".join(company["role_preferred"]))
        else:
            st.caption("ê³µê³ ì—ì„œ ì¶”ì¶œëœ ìš°ëŒ€ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.info("ìœ„ ì…ë ¥ í›„ â€˜íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°â€™ë¥¼ ëˆŒëŸ¬ ìš”ì•½ì„ ìƒì„±í•˜ì„¸ìš”.")

# =====================================================================
# â‘¢ ì§ˆë¬¸ ìƒì„±
# =====================================================================
st.subheader("â‘¢ ì§ˆë¬¸ ìƒì„±")

def embed_texts(client: OpenAI, embed_model: str, texts: list[str]) -> np.ndarray:
    if not texts: return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=embed_model, input=texts)
    return np.array([d.embedding for d in resp.data], dtype=np.float32)

with st.expander("RAG ì˜µì…˜ (ì„ íƒ)"):
    rag_enabled = st.toggle("íšŒì‚¬ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸/ì½”ì¹­ ì‚¬ìš©", value=True, key="rag_on")
    top_k = st.slider("ê²€ìƒ‰ ìƒìœ„ K", 1, 8, 4, 1, key="topk")
    docs = st.file_uploader("íšŒì‚¬ ë¬¸ì„œ ì—…ë¡œë“œ (TXT/MD/PDF, ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥)", type=["txt","md","pdf"], accept_multiple_files=True)
    chunk_size = st.slider("ì²­í¬ ê¸¸ì´(ë¬¸ì)", 400, 2000, 900, 100)
    chunk_ovlp = st.slider("ì˜¤ë²„ë©(ë¬¸ì)", 0, 400, 150, 10)
    if docs:
        with st.spinner("ë¬¸ì„œ ì¸ë±ì‹± ì¤‘..."):
            chunks=[]
            for up in docs:
                t = read_file_to_text(up)
                if t: chunks += chunk_text(t, chunk_size, chunk_ovlp)
            if chunks:
                embs = embed_texts(client, "text-embedding-3-small", chunks)
                st.session_state.rag_store["chunks"] = chunks
                st.session_state.rag_store["embeds"] = embs
                st.success(f"ì²­í¬ {len(chunks)}ê°œ ì¸ë±ì‹± ì™„ë£Œ")

def cosine_topk(matrix: np.ndarray, query: np.ndarray, k: int = 4):
    if matrix.size == 0:
        return np.array([]), np.array([], dtype=int)
    qn = query / (np.linalg.norm(query, axis=1, keepdims=True) + 1e-12)
    mn = matrix / (np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12)
    sims = mn @ qn.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    return sims[idx], idx

def retrieve_supports(qtext: str, k: int):
    store = st.session_state.rag_store
    chs, embs = store.get("chunks", []), store.get("embeds")
    if not st.session_state.get("rag_on") or embs is None or not chs: return []
    qv = embed_texts(client, "text-embedding-3-small", [qtext])
    scores, idxs = cosine_topk(embs, qv, k=k)
    return [("íšŒì‚¬ìë£Œ", float(s), chs[int(i)]) for s,i in zip(scores, idxs)]

TYPE_INSTRUCTIONS = {
    "í–‰ë™(STAR)": "ê³¼ê±° ì‹¤ë¬´ ì‚¬ë¡€ë¥¼ ëŒì–´ë‚´ë„ë¡ S(ìƒí™©)-T(ê³¼ì œ)-A(í–‰ë™)-R(ì„±ê³¼)ë¥¼ ìœ ë„",
    "ê¸°ìˆ  ì‹¬ì¸µ": "í•µì‹¬ ê¸°ìˆ ì  ì˜ì‚¬ê²°ì •Â·íŠ¸ë ˆì´ë“œì˜¤í”„Â·ì„±ëŠ¥/ë¹„ìš©/í’ˆì§ˆ ì§€í‘œë¥¼ íŒŒê³ ë“œëŠ” ì‹¬ì¸µ ì§ˆë¬¸",
    "í•µì‹¬ê°€ì¹˜ ì í•©ì„±": "í•µì‹¬ê°€ì¹˜/íƒœë„ ì í•©ì„±ì„ ìƒí™©ê¸°ë°˜ìœ¼ë¡œ ê²€ì¦",
    "ì—­ì§ˆë¬¸": "ì§€ì›ìê°€ íšŒì‚¬ë¥¼ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í†µì°°ë ¥ ìˆëŠ” ì—­ì§ˆë¬¸",
}

def build_ctx(c: dict) -> str:
    news = ", ".join([_snippetize(n["title"], 70) for n in c.get("news", [])[:3]]) if c else ""
    return textwrap.dedent(f"""
    [íšŒì‚¬ëª…] {c.get('company_name','') if c else ''}
    [ëª¨ì§‘ ë¶„ì•¼] {c.get('role','') if c else ''}
    [ì£¼ìš” ì—…ë¬´] {", ".join(c.get('role_responsibilities', [])[:6]) if c else ''}
    [ìê²© ìš”ê±´] {", ".join(c.get('role_qualifications', [])[:6]) if c else ''}
    [í•µì‹¬ê°€ì¹˜] {", ".join(c.get('values', [])[:6]) if c else ''}
    [ìµœê·¼ ë‰´ìŠ¤] {news}
    """).strip()

def build_focuses(c: dict, supports: list[Tuple[str,float,str]], k: int = 4) -> list[str]:
    pool=[]
    if c:
        if c.get("role"): pool.append(c["role"])
        pool += c.get("role_responsibilities", [])[:6]
        pool += c.get("role_qualifications", [])[:6]
        pool += c.get("values", [])[:6]
        pool += [ _snippetize(n['title'], 60) for n in c.get("news", [])[:4] ]
    for _,_,txt in (supports or [])[:3]:
        pool += [t.strip() for t in re.split(r"[â€¢\-\n\.]", txt) if 6 < len(t.strip()) < 100][:3]
    pool=[p for p in pool if p]; random.shuffle(pool)
    return pool[:k]

def _similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def pick_diverse(cands: list[str], hist: list[str], gamma: float = 0.35) -> str:
    if not cands: return ""
    if not hist:  return random.choice(cands)
    best=None; best_score=1e9
    for q in cands:
        sims=[_similarity(q,h) for h in hist] or [0.0]
        score=(sum(sims)/len(sims)) + gamma*np.std(sims)
        if score < best_score:
            best_score=score; best=q
    return best

q_type = st.selectbox("ì§ˆë¬¸ ìœ í˜•", list(TYPE_INSTRUCTIONS.keys()))
level  = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"])
hint   = st.text_input("ì§ˆë¬¸ ìƒì„± íŒíŠ¸(ì„ íƒ)", placeholder="ì˜ˆ: ì „í™˜ í¼ë„ / ëª¨ë¸ ì„±ëŠ¥-ë¹„ìš© / ë°ì´í„° í’ˆì§ˆ")

if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", use_container_width=True, type="primary"):
    st.session_state.answer_text = ""  # ì´ì „ ë‹µë³€ ì´ˆê¸°í™”
    try:
        supports=[]
        if st.session_state.get("rag_on"):
            base_q = hint.strip() or (company.get('role','') if company else '')
            supports = retrieve_supports(base_q, st.session_state.get("topk",4))
        ctx = build_ctx(company) if company else ""
        focuses = build_focuses(company, supports, k=4)
        rag_note = ""
        if supports:
            joined="\n".join([f"- ({s:.2f}) {txt[:200]}" for _,s,txt in supports[:3]])
            rag_note=f"\n[ê·¼ê±° ë°œì·Œ]\n{joined}"
        seed = int(time.time()*1000) % 2_147_483_647
        sys = f"""ë„ˆëŠ” ë©´ì ‘ê´€ì´ë‹¤. **{q_type}** ìœ í˜•({TYPE_INSTRUCTIONS[q_type]})ì˜ ì§ˆë¬¸ **6ê°œ í›„ë³´**ë¥¼ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ë¼.
ì„œë¡œ í˜•íƒœÂ·ê´€ì Â·í‚¤ì›Œë“œê°€ ë‹¬ë¼ì•¼ í•˜ë©° ë‚œì´ë„ëŠ” {level}. 'í¬ì»¤ìŠ¤' í‚¤ì›Œë“œ ì¤‘ ìµœì†Œ 1ê°œë¥¼ ë¬¸ì¥ì— ëª…ì‹œì ìœ¼ë¡œ í¬í•¨.
í¬ë§·: 1) ... 2) ... 3) ... (í•œ ì¤„ì”©)"""
        user = f"""[íšŒì‚¬/ì§ë¬´ ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n[í¬ì»¤ìŠ¤]\n- {chr(10).join(focuses)}{rag_note}\n[ëœë¤ì‹œë“œ] {seed}"""
        resp = client.chat.completions.create(
            model=MODEL, temperature=0.95,
            messages=[{"role":"system","content":sys},{"role":"user","content":user}]
        )
        raw = resp.choices[0].message.content.strip()
        cands = [re.sub(r'^\s*\d+\)\s*','',line).strip() for line in raw.splitlines() if re.match(r'^\s*\d+\)', line)]
        if not cands:
            cands = [l.strip("- ").strip() for l in raw.splitlines() if len(l.strip())>0][:6]
        hist_qs = [h["question"] for h in st.session_state.get("history", [])][-10:]
        selected = pick_diverse(cands, hist_qs)
        st.session_state.current_question = selected or (cands[0] if cands else "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
        st.session_state.last_supports_q = supports
    except Exception as e:
        st.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")

st.text_area("ì§ˆë¬¸", height=110, value=st.session_state.get("current_question",""))

if st.session_state.get("rag_on") and st.session_state.get("last_supports_q"):
    with st.expander("ì§ˆë¬¸ ìƒì„±ì— ì‚¬ìš©ëœ ê·¼ê±° ë³´ê¸°"):
        for i, (_, sc, txt) in enumerate(st.session_state.last_supports_q, 1):
            st.markdown(f"**[{i}] sim={sc:.3f}**\n\n{txt[:600]}{'...' if len(txt)>600 else ''}")
            st.markdown("---")

# =====================================================================
# â‘£ ë‚˜ì˜ ë‹µë³€ / ì½”ì¹­ (ì§ˆë¬¸ ìœ í˜•ë³„ ë£¨ë¸Œë¦­ ì ìš©)
# =====================================================================
st.subheader("â‘£ ë‚˜ì˜ ë‹µë³€ / ì½”ì¹­")

# ë£¨ë¸Œë¦­ ì ìš©: ì–´ë–¤ ì¶•ì„ ì ìˆ˜í™”í• ì§€ ê²°ì •
AXES = ["ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜"]
def applicable_axes(question_type: str) -> list[bool]:
    if question_type == "í–‰ë™(STAR)":
        return [True, True, True, True, True]
    if question_type == "ê¸°ìˆ  ì‹¬ì¸µ":
        return [True, True, True, False, False]
    if question_type == "í•µì‹¬ê°€ì¹˜ ì í•©ì„±":
        return [True, False, False, True, True]
    if question_type == "ì—­ì§ˆë¬¸":
        return [True, False, False, True, True]
    return [True, True, True, True, True]

def coach_answer(company: dict, question: str, answer: str, supports, qtype: str) -> dict:
    news = ", ".join([_snippetize(n["title"], 70) for n in (company.get("news", []) if company else [])[:3]])
    ctx = build_ctx(company) if company else ""
    # ì ìš© ì¶• ì•ˆë‚´(ë¹„ì ìš©ì€ '-')
    applies = applicable_axes(qtype)
    apply_text = ", ".join([f"{AXES[i]}({'O' if applies[i] else '-'})" for i in range(5)])
    rag_note=""
    if supports:
        joined="\n".join([f"- ({s:.3f}) {txt[:300]}" for (_,s,txt) in supports])
        rag_note=f"\n[íšŒì‚¬ ê·¼ê±° ë¬¸ì„œ ë°œì·Œ]\n{joined}\n"
    sys = f"""ë„ˆëŠ” í†±í‹°ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. í•œêµ­ì–´ë¡œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ë¼.
1) ì´ì : 0~100 ì •ìˆ˜ 1ê°œ  # ë°˜ë“œì‹œ 'ì´ì : NN/100' í˜•ì‹ì˜ í•œ ì¤„ë¡œ ì¶œë ¥
2) ê°•ì : 2~3ê°œ ë¶ˆë¦¿
3) ë¦¬ìŠ¤í¬: 2~3ê°œ ë¶ˆë¦¿
4) ê°œì„  í¬ì¸íŠ¸: 3ê°œ ë¶ˆë¦¿ (í–‰ë™Â·ì§€í‘œÂ·ì„íŒ©íŠ¸ ì¤‘ì‹¬)
5) ìˆ˜ì •ë³¸ ë‹µë³€: STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼) êµ¬ì¡°ë¡œ ê°„ê²°í•˜ê²Œ
6) ì—­ëŸ‰ ì ìˆ˜(ê° 0~20, ë¹„ì ìš©ì€ '-'ë¡œë§Œ ì¶œë ¥): [{', '.join(AXES)}] â€” ì´ ìˆœì„œë¡œ ìˆ«ì 5ê°œ í˜¹ì€ '-'ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ í•œ ì¤„ì— ì¶œë ¥
ì±„ì ì€ '{qtype}' ìœ í˜•ì— ë§ì¶° ë¹„ì ìš© í•­ëª©ì€ ë°˜ë“œì‹œ '-'ë¡œ í‘œê¸°í•˜ë¼.
ì´ì ì€ ì ìš©ëœ í•­ëª©ë“¤ì˜ í‰ê· (0~20)ì„ 5ë°° í•˜ì—¬ 0~100ìœ¼ë¡œ í™˜ì‚°í•˜ë¼."""
    user = f"""[íšŒì‚¬/ì§ë¬´ ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n{rag_note}[ì§ˆë¬¸ ìœ í˜• ì ìš© ì¶•]\n{apply_text}\n\n[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n[í›„ë³´ì ë‹µë³€]\n{answer}"""
    resp = client.chat.completions.create(model=MODEL, temperature=0.35,
                                          messages=[{"role":"system","content":sys},{"role":"user","content":user}])
    content = resp.choices[0].message.content.strip()

    # --- ì´ì  íŒŒì‹±(ì—„ê²©): 'ì´ì :' ë¼ì¸ì—ì„œë§Œ ì¶”ì¶œ ---
    score=None
    for line in content.splitlines():
        if "ì´ì " in line:
            m = re.search(r'ì´ì \s*:\s*(\d{1,3})\s*/\s*100', line)
            if m:
                score = max(0, min(100, int(m.group(1))))
            break
    # ë³´ì •: ì—†ìœ¼ë©´ ì—­ëŸ‰ ì ìˆ˜ì—ì„œ ê³„ì‚°
    comps_raw_line = content.splitlines()[-1]
    tokens = [t.strip() for t in re.split(r"[,\s]+", comps_raw_line) if t.strip()!=""]
    comps: list[Optional[int]] = []
    for i, tok in enumerate(tokens[:5]):
        if tok == "-" or tok == "â€“":
            comps.append(None)
        elif tok.isdigit():
            comps.append(max(0, min(20, int(tok))))
        else:
            comps.append(None)
    # ê¸¸ì´ê°€ 5 ë¯¸ë§Œì´ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë³´ì¡° ì¶”ì¶œ(ìˆ«ìë§Œ)
    while len(comps) < 5: comps.append(None)

    # ì´ì ì´ ì—†ìœ¼ë©´ ì ìš© í•­ëª© í‰ê· ìœ¼ë¡œ ì‚°ì¶œ
    applies = applicable_axes(qtype)
    used = [c for c, a in zip(comps, applies) if a and isinstance(c, int)]
    if score is None and used:
        score = round(sum(used)/len(used)*5)
    score = score if score is not None else 0

    # ë¹„ì ìš© í•­ëª©('-')ì€ Noneìœ¼ë¡œ ë³´ê´€
    comp_scores = [c if isinstance(c, int) else None for c in comps[:5]]

    return {"raw": content, "score": score, "competencies": comp_scores}

ans = st.text_area("ì—¬ê¸°ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš” (STAR ê¶Œì¥: ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)", height=180, key="answer_text")

if st.button("ì±„ì  & ì½”ì¹­", type="primary", use_container_width=True):
    if not st.session_state.get("current_question"):
        st.warning("ë¨¼ì € 'ìƒˆ ì§ˆë¬¸ ë°›ê¸°'ë¡œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì½”ì¹­ ì¤‘..."):
            sups=[]
            if st.session_state.get("rag_on"):
                q_for_rag = st.session_state["current_question"] + "\n" + st.session_state.answer_text[:800]
                sups = retrieve_supports(q_for_rag, st.session_state.get("topk",4))
            res = coach_answer(company, st.session_state["current_question"], st.session_state.answer_text, sups, q_type)
            st.session_state.history.append({
                "ts": pd.Timestamp.now(),
                "question": st.session_state["current_question"],
                "user_answer": st.session_state.answer_text,
                "score": res.get("score"),
                "feedback": res.get("raw"),
                "supports": sups,
                "competencies": res.get("competencies")
            })

# ---------------- ê²°ê³¼/ë ˆì´ë”/CSV ----------------
st.divider()
st.subheader("í”¼ë“œë°± ê²°ê³¼")

if st.session_state.history:
    last = st.session_state.history[-1]
    # ì¢Œ/ìš° ì´ì  ì¼ê´€í™”: ì¢Œì¸¡ ë©”íŠ¸ë¦­ì€ ì €ì¥ëœ scoreë§Œ ì‚¬ìš©
    total_score = last.get("score","â€”")
    c1,c2 = st.columns([1,3])
    with c1: st.metric("ì´ì (/100)", total_score)
    with c2:
        # ìš°ì¸¡ë„ 'ì´ì : NN/100'ì„ í•œ ë²ˆ ë” ëª…ì‹œí•˜ì—¬ ì‹œê°ì  ì¼ì¹˜ ê°•í™”
        if isinstance(total_score, int):
            st.markdown(f"**ì´ì (ì‹œìŠ¤í…œ ì‚°ì¶œ)**: {total_score}/100")
        st.markdown(last.get("feedback",""))

    if st.session_state.get("rag_on") and last.get("supports"):
        with st.expander("ì½”ì¹­ì— ì‚¬ìš©ëœ ê·¼ê±° ë³´ê¸°"):
            for i,(_,sc,txt) in enumerate(last["supports"],1):
                st.markdown(f"**[{i}] sim={sc:.3f}**\n\n{txt[:800]}{'...' if len(txt)>800 else ''}")
                st.markdown("---")
else:
    st.info("ì•„ì§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("ì—­ëŸ‰ ë ˆì´ë” (ì„¸ì…˜ ëˆ„ì )")

def comp_df(hist):
    rows=[]
    for h in hist:
        comps=h.get("competencies")
        if not comps: continue
        # Noneì€ ë¹„ì ìš©('-') â†’ 0ìœ¼ë¡œ ì±„ìš°ì§€ ì•Šê³  ë³„ë„ ì²˜ë¦¬ ìœ„í•´ NaN ì‚¬ìš©
        vals=[(float(v) if isinstance(v,int) else np.nan) for v in comps]
        rows.append(vals)
    if not rows: return None
    df = pd.DataFrame(rows, columns=AXES)
    # ê° í–‰ì˜ í•©ê³„(ë¹„ì ìš©ì€ ì œì™¸í•˜ê³  í•©ì‚°)
    df["í•©ê³„"] = df[AXES].sum(axis=1, skipna=True)
    return df

cdf = comp_df(st.session_state.history)
if cdf is not None:
    # í‰ê· (ë¹„ì ìš© ì œì™¸)
    means = cdf[AXES].mean(axis=0, skipna=True).tolist()
    # ë ˆì´ë”ìš©: NaNâ†’0ìœ¼ë¡œ í‘œì‹œ(ì‹œê°í™”ìš©)
    radar_vals = [0 if (np.isnan(v)) else float(v) for v in means]
    if PLOTLY_OK:
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(r=radar_vals+[radar_vals[0]], theta=AXES+[AXES[0]], fill='toself'))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,20])), showlegend=False, height=420)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.bar_chart(pd.DataFrame({"score": radar_vals}, index=AXES))
    st.dataframe(cdf.fillna("-"), use_container_width=True)
else:
    st.caption("ì•„ì§ ì—­ëŸ‰ ì ìˆ˜ê°€ íŒŒì‹±ëœ ì½”ì¹­ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV)")
def build_report(hist):
    rows=[]
    for h in hist:
        row={"timestamp":h.get("ts"),"question":h.get("question"),"user_answer":h.get("user_answer"),
             "score":h.get("score"),"feedback_raw":h.get("feedback")}
        comps=h.get("competencies")
        if comps and len(comps)==5:
            for k,v in zip(AXES, comps): row[f"comp_{k}"]=("-" if v is None else v)
            # í•©ê³„(ë¹„ì ìš© ì œì™¸)
            cvals=[v for v in comps if isinstance(v,int)]
            row["comp_sum"]=sum(cvals) if cvals else 0
        sups=h.get("supports") or []
        row["supports_preview"]=" || ".join([s[2][:120].replace("\n"," ") for s in sups])
        rows.append(row)
    return pd.DataFrame(rows) if rows else pd.DataFrame(columns=["timestamp","question","user_answer","score","feedback_raw","supports_preview"])
rep = build_report(st.session_state.history)
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=rep.to_csv(index=False).encode("utf-8-sig"),
                   file_name="interview_session_report.csv", mime="text/csv")

st.caption("Tip) í™ˆí˜ì´ì§€/ê³µê³  URLì„ ë„£ìœ¼ë©´ ì •í™•ë„ê°€ í¬ê²Œ ì˜¬ë¼ê°‘ë‹ˆë‹¤. ìºì‹œ/ìš”ì²­ìˆ˜ ì œí•œìœ¼ë¡œ ì†ë„ë¥¼ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.")
