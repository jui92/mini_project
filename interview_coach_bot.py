# interview_coach_bot.py
# v2.3 â€” WebBaseLoader ê¸°ë°˜ ê³µê³  ë³¸ë¬¸ ìˆ˜ì§‘/ìš”ì•½ ì¶”ì¶œ ì¶”ê°€
#        (ì›ë¬¸ íŒŒì„œì™€ ë³‘í–‰, ì‹¤íŒ¨ ì‹œ í´ë°±) + ê¸°ì¡´ ì±„ì /ë ˆì´ë”/CSV ê·¸ëŒ€ë¡œ ìœ ì§€

import os, io, re, json, textwrap, urllib.parse, difflib, random, time
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import pandas as pd
import streamlit as st

# ---------- Optional deps ----------
try:
    import pypdf
except Exception:
    pypdf = None

try:
    import plotly.graph_objects as go
    PLOTLY_OK = True
except Exception:
    PLOTLY_OK = False

try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# NEW: WebBaseLoader (ì—†ìœ¼ë©´ ìë™ í´ë°±)
try:
    from langchain_community.document_loaders import WebBaseLoader
    WEBBASE_OK = True
except Exception:
    WEBBASE_OK = False

import requests
from bs4 import BeautifulSoup

# ---------- Page config ----------
st.set_page_config(page_title="íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜", page_icon="ğŸ¤–", layout="wide")

# ---------- Secrets ----------
def _secrets_file_exists() -> bool:
    cands = [
        os.path.join(os.path.expanduser("~"), ".streamlit", "secrets.toml"),
        os.path.join(os.getcwd(), ".streamlit", "secrets.toml"),
    ]
    return any(os.path.exists(p) for p in cands)

def load_api_key_from_env_or_secrets() -> Optional[str]:
    k = os.getenv("OPENAI_API_KEY")
    if k: return k
    try:
        if _secrets_file_exists() or hasattr(st, "secrets"):
            return st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        pass
    return None

def load_naver_keys():
    cid = os.getenv("NAVER_CLIENT_ID")
    csec = os.getenv("NAVER_CLIENT_SECRET")
    try:
        if hasattr(st, "secrets"):
            cid = cid or st.secrets.get("NAVER_CLIENT_ID", None)
            csec = csec or st.secrets.get("NAVER_CLIENT_SECRET", None)
    except Exception:
        pass
    return cid, csec

NAVER_ID, NAVER_SECRET = load_naver_keys()
UA = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36"}

# ---------- Utils ----------
def _clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", (t or "")).strip()

def _snippetize(text: str, maxlen: int = 220) -> str:
    t = _clean_text(text)
    return t if len(t) <= maxlen else t[: maxlen - 1] + "â€¦"

def chunk_text(text: str, size: int = 900, overlap: int = 150):
    text = re.sub(r"\s+", " ", text).strip()
    if not text: return []
    out, start = [], 0
    while start < len(text):
        end = min(len(text), start + size)
        out.append(text[start:end])
        if end == len(text): break
        start = max(0, end - overlap)
    return out

def read_file_to_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt", ".md")):
        for enc in ("utf-8", "cp949", "euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        if pypdf is None:
            st.warning("pypdfê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— pypdf ì¶”ê°€.")
            return ""
        try:
            reader = pypdf.PdfReader(io.BytesIO(data))
            return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
        except Exception as e:
            st.warning(f"PDF íŒŒì‹± ì‹¤íŒ¨({uploaded.name}): {e}")
            return ""
    return ""

def _domain(u: str|None) -> str|None:
    if not u: return None
    try:
        if not u.startswith("http"): u = "https://" + u
        return urllib.parse.urlparse(u).netloc.lower().replace("www.","")
    except Exception:
        return None

# ---------- NAVER ----------
def _naver_api_get(api: str, params: dict, cid: str, csec: str):
    url = f"https://openapi.naver.com/v1/search/{api}.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec, **UA}
    r = requests.get(url, headers=headers, params=params, timeout=8)
    if r.status_code != 200:
        return None
    return r.json()

def naver_search_news(query: str, display: int = 10, sort: str = "date") -> list[dict]:
    cid, csec = load_naver_keys()
    if not (cid and csec):
        return []
    js = _naver_api_get("news", {"query": query, "display": display, "sort": sort}, cid, csec)
    if not js: return []
    out = []
    for it in js.get("items", []):
        title = _clean_text(re.sub(r"</?b>|&quot;|&apos;|&amp;|&lt;|&gt;", "", it.get("title","")))
        out.append({"title": title, "link": it.get("link"), "pubDate": it.get("pubDate")})
    return out

def naver_search_web(query: str, display: int = 10, sort: str = "date") -> list[str]:
    cid, csec = load_naver_keys()
    if not (cid and csec):
        return []
    js = _naver_api_get("webkr", {"query": query, "display": display, "sort": sort}, cid, csec)
    if not js: return []
    links = []
    for it in js.get("items", []):
        link = it.get("link")
        if link and link not in links:
            links.append(link)
    return links

@st.cache_data(ttl=3600)
def fetch_news(company_name: str, max_items: int = 6) -> list[dict]:
    news = naver_search_news(company_name, display=max_items, sort="date")
    if news:
        return news
    # Google RSS fallback
    q = urllib.parse.quote(company_name)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    items = []
    try:
        r = requests.get(url, timeout=8, headers=UA)
        if r.status_code != 200: return []
        soup = BeautifulSoup(r.text, "xml")
        for it in soup.find_all("item")[:max_items]:
            title = _clean_text(it.title.get_text()) if it.title else ""
            link  = it.link.get_text() if it.link else ""
            pub   = it.pubDate.get_text() if it.pubDate else ""
            items.append({"title": title, "link": link, "pubDate": pub})
    except Exception:
        return []
    return items

# ---------- í™ˆí˜ì´ì§€ ìš”ì•½ í›„ë³´ ----------
VAL_KEYS = ["í•µì‹¬ê°€ì¹˜","ê°€ì¹˜","ë¯¸ì…˜","ë¹„ì „","ë¬¸í™”","ì›ì¹™","ì² í•™","ê³ ê°","ë°ì´í„°","í˜ì‹ ",
            "values","mission","vision","culture","principles","philosophy","customer","data","innovation"]

@st.cache_data(ttl=3600)
def fetch_site_snippets(base_url: str | None, company_name_hint: str | None = None) -> dict:
    if not base_url:
        return {"values": [], "recent": [], "site_name": None, "about": None}
    url0 = base_url.strip()
    if not url0.startswith("http"): url0 = "https://" + url0
    cand_paths = ["", "/", "/about", "/company", "/about-us", "/mission", "/values", "/culture"]
    values_found, recent_found = [], []
    site_name, about_para = None, None

    for path in cand_paths:
        url = url0.rstrip("/") + path
        try:
            r = requests.get(url, timeout=6, headers=UA)
            if r.status_code != 200 or "text/html" not in r.headers.get("content-type", ""):
                continue
            soup = BeautifulSoup(r.text, "html.parser")

            if site_name is None:
                og = soup.find("meta", {"property":"og:site_name"}) or soup.find("meta", {"name":"application-name"})
                if og and og.get("content"): site_name = _clean_text(og["content"])
                elif soup.title and soup.title.string: site_name = _clean_text(soup.title.string.split("|")[0])

            if about_para is None:
                hero = soup.find(["p","div"], class_=re.compile(r"(lead|hero|intro)", re.I)) if soup else None
                if hero: about_para = _snippetize(hero.get_text(" "))

            for tag in soup.find_all(["h1","h2","h3","p","li"]):
                txt = _clean_text(tag.get_text(separator=" "))
                if 10 <= len(txt) <= 240:
                    if any(k.lower() in txt.lower() for k in VAL_KEYS):
                        values_found.append(txt)
                    if any(k in txt for k in ["í”„ë¡œì íŠ¸","ê°œë°œ","ì¶œì‹œ","ì„±ê³¼","project","launched","release","delivered","improved"]):
                        recent_found.append(txt)
        except Exception:
            continue

    def dedup(lst):
        seen=set(); out=[]
        for x in lst:
            if x not in seen: seen.add(x); out.append(x)
        return out
    values_found = dedup(values_found)[:6]
    recent_found = dedup(recent_found)[:6]
    return {"values": values_found, "recent": recent_found, "site_name": site_name, "about": about_para}

# ---------- ì±„ìš© ë§í¬ íƒìƒ‰ + ë¦¬ìŠ¤íŠ¸â†’ìƒì„¸ ê³µê³  ì¶”ì  ----------
CAREER_HINTS = ["careers","career","jobs","job","recruit","recruiting","join","hire","hiring",
                "ì±„ìš©","ì¸ì¬","ì…ì‚¬ì§€ì›","ì±„ìš©ê³µê³ ","ì¸ì¬ì˜ì…","ì‚¬ëŒ","ì»¤ë¦¬ì–´"]
SEARCH_ENGINES = ["https://duckduckgo.com/html/?q={query}"]
JOB_SITES = ["wanted.co.kr","saramin.co.kr","jobkorea.co.kr","rocketpunch.com",
             "indeed.com","linkedin.com","recruit.navercorp.com","kakao.recruit","naver"]

@st.cache_data(ttl=3600)
def discover_job_from_homepage(homepage: str, limit: int = 5) -> list[str]:
    if not homepage: return []
    try:
        if not homepage.startswith("http"): homepage = "https://" + homepage
        r = requests.get(homepage, timeout=8, headers=UA)
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
            return []
        soup = BeautifulSoup(r.text, "html.parser")
        links=[]
        for path in ["careers","recruit","jobs","career","ì±„ìš©","ì¸ì¬ì˜ì…","recruitment","join"]:
            links.append(urllib.parse.urljoin(homepage.rstrip("/") + "/", path))
        for a in soup.find_all("a", href=True):
            href = a["href"]
            text = (a.get_text() or "").lower()
            if any(k in href.lower() or k in text for k in CAREER_HINTS):
                links.append(urllib.parse.urljoin(homepage, href))
        out=[]; seen=set()
        for lk in links:
            d = _domain(lk)
            if lk not in seen and d:
                seen.add(lk); out.append(lk)
                if len(out) >= limit: break
        return out[:limit]
    except Exception:
        return []

def _first_detail_from_list(url: str, role_hint: str="") -> Optional[str]:
    try:
        r = requests.get(url, timeout=8, headers=UA)
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""): return None
        soup = BeautifulSoup(r.text, "html.parser")
        dom = _domain(url) or ""
        if "wanted.co.kr" in dom:
            for a in soup.select("a[href*='/wd/']"):
                href = urllib.parse.urljoin(url, a.get("href"))
                title = (a.get_text() or "").strip()
                if role_hint and role_hint not in title:
                    continue
                return href
        if "saramin.co.kr" in dom:
            for a in soup.select("a[href*='view?idx=']"):
                return urllib.parse.urljoin(url, a.get("href"))
        if "jobkorea.co.kr" in dom:
            for a in soup.select("a[href*='/Recruit/GI_Read/']"):
                return urllib.parse.urljoin(url, a.get("href"))
        for a in soup.find_all("a", href=True):
            href = urllib.parse.urljoin(url, a.get("href"))
            if any(s in (_domain(href) or "") for s in JOB_SITES):
                if re.search(r"/(wd|jobs|job|view|read|detail|posting)/", href, re.I):
                    return href
    except Exception:
        return None
    return None

@st.cache_data(ttl=3600)
def discover_job_posting_urls(company_name: str, role: str, homepage: str|None, limit: int = 5) -> list[str]:
    urls = []
    urls += discover_job_from_homepage(homepage, limit=limit) if homepage else []
    resolved=[]
    for u in urls:
        if re.search(r"/(wd|view|read|detail|posting)/", u, re.I):
            resolved.append(u)
        else:
            detail = _first_detail_from_list(u, role_hint=role or "")
            if detail: resolved.append(detail)
    urls = resolved[:]
    if urls: return urls[:limit]

    if NAVER_ID and NAVER_SECRET:
        for dom in JOB_SITES:
            if len(urls) >= limit: break
            q = f"{company_name} {role} site:{dom}" if role else f"{company_name} ì±„ìš© site:{dom}"
            links = naver_search_web(q, display=7, sort="date")
            for lk in links:
                if _domain(lk) and dom in _domain(lk) and lk not in urls:
                    if not re.search(r"/(wd|view|read|detail|posting)/", lk, re.I):
                        lk2 = _first_detail_from_list(lk, role_hint=role or "")
                        if lk2: lk = lk2
                    urls.append(lk)
                if len(urls) >= limit: break
        if urls: return urls[:limit]

    site_part = " OR ".join([f'site:{d}' for d in JOB_SITES])
    q = f'{company_name} {role} ({site_part})' if role else f'{company_name} ì±„ìš© ({site_part})'
    for engine in SEARCH_ENGINES:
        url = engine.format(query=urllib.parse.quote(q))
        try:
            r = requests.get(url, timeout=8, headers=UA)
            if r.status_code != 200: continue
            soup = BeautifulSoup(r.text, "html.parser")
            for a in soup.find_all("a", href=True):
                href = a["href"]
                if href.startswith("/l/?kh=-1&uddg="):
                    href = urllib.parse.unquote(href.split("/l/?kh=-1&uddg=")[-1])
                dom = _domain(href)
                if not dom: continue
                if any(d in dom for d in JOB_SITES):
                    if not re.search(r"/(wd|view|read|detail|posting)/", href, re.I):
                        d2 = _first_detail_from_list(href, role_hint=role or "")
                        if d2: href = d2
                    if href not in urls: urls.append(href)
                if len(urls) >= limit: break
        except Exception:
            continue
    return urls[:limit]

# ---------- ê³µê³  íŒŒì„œ(ì›ë¬¸) ----------
def _text_items_from_container(el) -> list[str]:
    if el is None: return []
    items = []
    for li in el.find_all(["li","p"]):
        t = _clean_text(li.get_text(" "))
        if len(t) >= 2: items.append(t)
    if not items:
        t = _clean_text(el.get_text(" "))
        parts = [x.strip(" â€¢Â·â–ªï¸-â€”") for x in re.split(r"[â€¢Â·â–ªï¸\-\n\r\t]+", t) if len(x.strip())>1]
        items = parts[:]
    seen=set(); out=[]
    for x in items:
        if x not in seen:
            seen.add(x); out.append(x[:300])
    return out

def _extract_by_headings(soup: BeautifulSoup, heads: list[str]) -> Optional[list[str]]:
    if soup is None: return None
    pat = re.compile("|".join(heads), re.I)
    for h in soup.find_all(re.compile("^h[1-4]$")):
        title = _clean_text(h.get_text())
        if not pat.search(title): continue
        nxt = h.find_next_sibling()
        buf=[]; stop={"h1","h2","h3","h4"}
        while nxt and nxt.name not in stop:
            if nxt.name in {"div","section","article","ul","ol","p"}:
                buf.extend(_text_items_from_container(nxt))
            nxt = nxt.find_next_sibling()
        buf = [b for b in buf if len(b)>1]
        if buf: return buf[:24]
    return None

def _jsonld_job(soup: BeautifulSoup) -> dict:
    out = {"responsibilities": None, "qualifications": None, "preferences": None}
    for s in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(s.string or "")
            seq = data if isinstance(data, list) else [data]
            for obj in seq:
                typ = obj.get("@type") if isinstance(obj, dict) else None
                if (isinstance(typ, list) and "JobPosting" in typ) or typ == "JobPosting":
                    desc = _clean_text(obj.get("description",""))
                    if not desc: continue
                    parts = [p.strip(" -â€¢Â·â–ªï¸â€”") for p in re.split(r"[â€¢\n\r\t]+", desc) if len(p.strip())>2]
                    resp, qual, pref = [], [], []
                    for p in parts:
                        if re.search(r"ìê²©|ìš”ê±´|qual", p, re.I): qual.append(p)
                        elif re.search(r"ìš°ëŒ€|prefer|plus|nice", p, re.I): pref.append(p)
                        else: resp.append(p)
                    out = {"responsibilities":resp or None, "qualifications":qual or None, "preferences":pref or None}
                    return out
        except Exception:
            continue
    return out

def _whole_document_fallback(soup: BeautifulSoup) -> dict:
    text = _clean_text(soup.get_text(" "))
    patterns = {
        "responsibilities": r"(ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|ì—…ë¬´\s*ë‚´ìš©|Responsibilities|Role|What\s+you('|â€™)ll\s+do)",
        "qualifications":   r"(ìê²©\s*ìš”ê±´|ì§€ì›\s*ìê²©|Requirements|Qualifications|Must\s*have)",
        "preferences":      r"(ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€|Preferred|Plus|Nice\s*to\s*have)",
    }
    result={"responsibilities":[], "qualifications":[], "preferences":[]}
    for key, pat in patterns.items():
        m = re.search(pat, text, re.I)
        if not m: continue
        start = m.end()
        next_pat = re.compile("|".join([p for k,p in patterns.items() if k!=key]), re.I)
        m2 = next_pat.search(text, start)
        chunk = text[start:(m2.start() if m2 else start+1500)]
        items = [x.strip(" -â€¢Â·â–ªï¸â€”") for x in re.split(r"[â€¢\n\r\t]+", chunk)]
        items = [i for i in items if 2<len(i)<300]
        result[key] = items[:24]
    return result

# ---------- NEW: WebBaseLoaderë¡œ ê³µê³  ë³¸ë¬¸ ë¡œë“œ + LLM êµ¬ì¡°í™” ----------
def extract_with_webbase_and_llm(url: str, client: OpenAI, model: str, force_summary: bool=False) -> dict:
    """WebBaseLoaderë¡œ ë³¸ë¬¸ì„ ì½ê³  LLMìœ¼ë¡œ 'ì£¼ìš”ì—…ë¬´/ìê²©ìš”ê±´/ìš°ëŒ€ì‚¬í•­'ì„ JSONìœ¼ë¡œ ì¶”ì¶œ."""
    out = {"title": None, "responsibilities": [], "qualifications": [], "preferences": [], "company_intro": None}
    if not WEBBASE_OK:
        return out
    try:
        loader = WebBaseLoader(url)
        docs = loader.load()
        body = "\n".join([d.page_content for d in docs if d.page_content])[:120_000]
        if not body.strip():
            return out

        sys = ("ë„ˆëŠ” ì±„ìš©ê³µê³  ì „ë¬¸ ìš”ì•½ê¸°ë‹¤. ì•„ë˜ ì›ë¬¸ì—ì„œ 'ì£¼ìš”ì—…ë¬´','ìê²©ìš”ê±´','ìš°ëŒ€ì‚¬í•­'ì„ **ì›ë¬¸ ë¬¸êµ¬ë¥¼ ìµœëŒ€í•œ ë³´ì¡´**í•˜ëŠ” "
               "ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí•˜ê³  JSONë§Œ ë°˜í™˜í•˜ë¼. í‚¤ëŠ” responsibilities, qualifications, preferences. "
               "ê° ë¦¬ìŠ¤íŠ¸ëŠ” 3~12ê°œ í•­ëª©. ë¶ˆí•„ìš”í•œ ë§ ê¸ˆì§€.")
        if force_summary:
            sys = sys.replace("ìµœëŒ€í•œ ë³´ì¡´", "í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨", "ìš”ì•½")

        user = f"[ì›ë¬¸]\n{body}"
        resp = client.chat.completions.create(
            model=model, temperature=0.2,
            messages=[{"role":"system","content":sys},{"role":"user","content":user}]
        )
        text = resp.choices[0].message.content.strip()
        m = re.search(r"\{.*\}", text, re.S)
        if not m: 
            return out
        data = json.loads(m.group(0))
        for k in ["responsibilities","qualifications","preferences"]:
            lst = data.get(k) or []
            lst = [ _clean_text(x)[:300] for x in lst if _clean_text(x) ]
            out[k] = lst[:24]
        return out
    except Exception:
        return out

@st.cache_data(ttl=1800)
def parse_job_posting(url: str, client: OpenAI, model: str,
                      prefer_webbase: bool=True, force_summary: bool=False) -> dict:
    """prefer_webbase=Trueë©´ WebBaseLoader+LLM ìš°ì„ , ë¶€ì¡±í•˜ë©´ ì›ë¬¸ íŒŒì„œ í´ë°±."""
    # 0) WebBaseLoader + LLM
    if prefer_webbase and WEBBASE_OK:
        via_llm = extract_with_webbase_and_llm(url, client, model, force_summary=force_summary)
        if any(via_llm[k] for k in ("responsibilities","qualifications","preferences")):
            return via_llm

    # 1) BeautifulSoup ê¸°ë°˜ ì›ë¬¸ íŒŒì‹±
    out = {"title": None, "responsibilities": [], "qualifications": [], "preferences": [], "company_intro": None}
    try:
        r = requests.get(url, timeout=12, headers=UA)
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""): return out
        soup = BeautifulSoup(r.text, "html.parser")
        resp = _extract_by_headings(soup, [r"ì£¼ìš”\s*ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|ì—…ë¬´|Responsibilities|Role|What\s+you('|â€™)ll\s+do"])
        qual = _extract_by_headings(soup, [r"ìê²©\s*ìš”ê±´|ì§€ì›\s*ìê²©|Requirements|Qualifications|Must"])
        pref = _extract_by_headings(soup, [r"ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€|Preferred|Plus|Nice\s*to\s*have"])

        if not (resp and qual and pref):
            jd = _jsonld_job(soup)
            resp = resp or jd.get("responsibilities")
            qual = qual or jd.get("qualifications")
            pref = pref or jd.get("preferences")

        if not (resp or qual or pref):
            whole = _whole_document_fallback(soup)
            resp = whole.get("responsibilities")
            qual = whole.get("qualifications")
            pref = whole.get("preferences")

        meta_desc = soup.find("meta", {"name":"description"}) or soup.find("meta", {"property":"og:description"})
        if meta_desc and meta_desc.get("content"): out["company_intro"]=_snippetize(meta_desc["content"], 220)

        for k,vals in [("responsibilities",resp),("qualifications",qual),("preferences",pref)]:
            if vals:
                vals = [_clean_text(v)[:300] for v in vals if len(_clean_text(v))>1]
                out[k] = vals[:24]
    except Exception:
        pass
    return out

# ---------- OpenAI ----------
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    API_KEY = load_api_key_from_env_or_secrets()
    if not API_KEY:
        st.info("í™˜ê²½ë³€ìˆ˜/Secretsì—ì„œ í‚¤ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ì— ì…ë ¥ í›„ ì—”í„°.")
        API_KEY = st.text_input("OPENAI_API_KEY", type="password")
    MODEL = st.selectbox("ì±— ëª¨ë¸", ["gpt-4o-mini","gpt-4o","gpt-4.1-mini"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small","text-embedding-3-large"], index=0)

    try:
        import openai as _openai_pkg; _openai_ver = getattr(_openai_pkg, "__version__", None)
    except Exception: _openai_ver=None
    try:
        import httpx as _httpx_pkg; _httpx_ver = getattr(_httpx_pkg, "__version__", None)
    except Exception: _httpx_ver=None
    with st.expander("ë””ë²„ê·¸: ì‹œí¬ë¦¿/ë²„ì „ ìƒíƒœ"):
        st.write({
            "api_key_provided": bool(API_KEY),
            "naver_keys": bool(NAVER_ID and NAVER_SECRET),
            "openai_version": _openai_ver,
            "httpx_version": _httpx_ver,
            "webbaseloader_available": WEBBASE_OK
        })

if not API_KEY:
    st.error("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    st.stop()
client = OpenAI(api_key=API_KEY, timeout=30.0)

# ==========================================================
# â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥ + ê³ ê¸‰ ìˆ˜ì§‘ ì˜µì…˜
# ==========================================================
st.subheader("â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥")
company_name_input = st.text_input("íšŒì‚¬ ì´ë¦„", placeholder="ì˜ˆ: ë„¤ì´ë²„ / Kakao / ì‚¼ì„±SDS")
role_title         = st.text_input("ì§€ì› ì§ë¬´ëª…", placeholder="ë°ì´í„° ì• ë„ë¦¬ìŠ¤íŠ¸ / ML ì—”ì§€ë‹ˆì–´ ...")
job_url_input      = st.text_input("ì±„ìš© ê³µê³  URL(ì„ íƒ) â€” ì—†ë‹¤ë©´ ìë™ íƒìƒ‰")
homepage_input     = st.text_input("ê³µì‹ í™ˆí˜ì´ì§€ URL(ì„ íƒ)", placeholder="https://...")

with st.expander("ê³ ê¸‰ ìˆ˜ì§‘ ì˜µì…˜"):
    prefer_webbase = st.checkbox("WebBaseLoaderë¡œ ê³µê³  ë³¸ë¬¸ ìˆ˜ì§‘ **ìš°ì„ **", value=True, help="ê°€ëŠ¥í•˜ë©´ WebBaseLoaderë¡œ ë³¸ë¬¸ì„ ì½ì–´ LLMìœ¼ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.")
    force_summary  = st.checkbox("LLM ìš”ì•½ ê°•ì œ(ì›ë¬¸ íŒŒì„œ ë¬´ì‹œ)", value=False, help="íŒŒì„œê°€ ì¢…ì¢… ì‹¤íŒ¨í•  ë•Œ ê°•ì œë¡œ ìš”ì•½ ì¶”ì¶œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

for k,v in [("company_state",{}),("answer_text",""),("history",[]),("current_question","")]:
    if k not in st.session_state: st.session_state[k]=v

def build_company_obj(name: str, homepage: str|None, role: str|None, job_url: str|None) -> dict:
    def _site(): return fetch_site_snippets(homepage or None, name)
    def _news(): return fetch_news(name, max_items=6)
    def _urls():
        if job_url: return [job_url]
        return discover_job_posting_urls(name, role or "", homepage, limit=6)

    with ThreadPoolExecutor(max_workers=3) as ex:
        fut = {ex.submit(fn): key for fn, key in [(_site,"site"),(_news,"news"),(_urls,"urls")]}
        ret = {}
        for f in as_completed(fut):
            try: ret[fut[f]] = f.result()
            except Exception: ret[fut[f]] = None

    site = ret.get("site") or {"values": [], "recent": [], "site_name": None, "about": None}
    urls = ret.get("urls") or []
    news = ret.get("news") or []

    jp = {"responsibilities": [], "qualifications": [], "preferences": [], "company_intro": None}
    if urls:
        jp = parse_job_posting(urls[0], client, MODEL, prefer_webbase=prefer_webbase, force_summary=force_summary)

    return {
        "company_name": name.strip() or "(íšŒì‚¬ëª… ë¯¸ì„¤ì •)",
        "homepage": homepage or None,
        "values": site.get("values", []),
        "recent_projects": site.get("recent", []),
        "company_intro_site": site.get("about"),
        "role": role or "",
        "responsibilities": jp["responsibilities"],
        "qualifications": jp["qualifications"],
        "preferences": jp["preferences"],
        "job_url": urls[0] if urls else (job_url or None),
        "news": news
    }

def generate_company_summary(c: dict) -> str:
    intro_src = c.get("company_intro_site") or ""
    news_titles = ", ".join([_snippetize(n["title"],70) for n in c.get("news", [])[:3]])
    sys = "ë„ˆëŠ” ì±„ìš©ë‹´ë‹¹ìë‹¤. íšŒì‚¬ ì†Œê°œë§Œ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°íˆ ìš”ì•½(ê´‘ê³  í‘œí˜„ ì œê±°, ì‚¬ì‹¤ ìœ„ì£¼)í•˜ë¼. í•œêµ­ì–´."
    user = f"[íšŒì‚¬ëª…] {c.get('company_name','')}\n[íšŒì‚¬ ì†Œê°œ ì›ë¬¸ í›„ë³´]\n{intro_src}\n[ìµœê·¼ ë‰´ìŠ¤ íƒ€ì´í‹€]\n{news_titles}"
    try:
        resp = client.chat.completions.create(
            model=MODEL, temperature=0.2,
            messages=[{"role":"system","content":sys},{"role":"user","content":user}]
        )
        intro = resp.choices[0].message.content.strip()
    except Exception:
        intro = intro_src or "íšŒì‚¬ ì†Œê°œ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    md = f"""**íšŒì‚¬ëª…**  
{c.get('company_name')}

**ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ(ìš”ì•½)**  
{intro}
"""
    return md

# ë¶ˆëŸ¬ì˜¤ê¸° ë²„íŠ¼
if st.button("íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°", type="primary"):
    if not company_name_input.strip():
        st.warning("íšŒì‚¬ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        # íšŒì‚¬ ë³€ê²½ ì‹œ ì•„ë˜ ì‹¤í–‰ê²°ê³¼ ì´ˆê¸°í™”
        st.session_state.history = []
        st.session_state.current_question = ""
        st.session_state.answer_text = ""
        with st.spinner("íšŒì‚¬/ì§ë¬´/ê³µê³ /ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
            cobj = build_company_obj(company_name_input, homepage_input or None, role_title or None, job_url_input or None)
            summary_md = generate_company_summary(cobj)
            st.session_state.company_state["company"] = cobj
            st.session_state.company_state["summary_md"] = summary_md
        st.success("íšŒì‚¬ ì •ë³´ ê°±ì‹  ì™„ë£Œ")

company = st.session_state["company_state"].get("company", {
    "company_name": "(íšŒì‚¬ëª… ë¯¸ì„¤ì •)", "homepage": None, "values": [], "recent_projects": [],
    "company_intro_site": None, "role": "", "responsibilities": [], "qualifications": [], "preferences": [],
    "job_url": None, "news": []
})
summary_md = st.session_state["company_state"].get("summary_md", None)

# ==========================================================
# â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´ (LLM ìš”ì•½/ì›ë¬¸ í˜¼í•©)
# ==========================================================
st.subheader("â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´")
if summary_md:
    st.markdown(summary_md)
    cols = st.columns(2)
    with cols[0]:
        if company.get("job_url"): st.link_button("ì±„ìš© ê³µê³  ì—´ê¸°", company["job_url"])
    with cols[1]:
        if company.get("news"):
            st.write("ìµœê·¼ ë‰´ìŠ¤:")
            for n in company["news"][:3]:
                st.markdown(f"- [{_clean_text(n['title'])}]({n['link']})")

    st.markdown("---")
    c1,c2,c3 = st.columns(3)

    def _bullet(lst, title):
        st.markdown(f"**{title}(ìš”ì•½)**" if prefer_webbase or force_summary else f"**{title}(ì›ë¬¸)**")
        if lst:
            for it in lst: st.markdown(f"- {it}")
        else:
            st.markdown(f"*ê³µê³ ì—ì„œ ì¶”ì¶œëœ {title}ì´(ê°€) ì—†ìŠµë‹ˆë‹¤.*")

    with c1: _bullet(company.get("responsibilities") or [], "ì£¼ìš”ì—…ë¬´")
    with c2: _bullet(company.get("qualifications") or [], "ìê²©ìš”ê±´")
    with c3: _bullet(company.get("preferences") or [], "ìš°ëŒ€ì‚¬í•­")
else:
    st.info("ìœ„ ì…ë ¥ì„ ì™„ë£Œí•˜ê³  â€˜íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°â€™ë¥¼ ëˆ„ë¥´ë©´ í‘œì‹œë©ë‹ˆë‹¤.")

# ==========================================================
# â‘¢ ì§ˆë¬¸ ìƒì„± (RAG ì„ íƒ) â€” ì´í•˜ ê¸°ì¡´ ê·¸ëŒ€ë¡œ
# ==========================================================
st.subheader("â‘¢ ì§ˆë¬¸ ìƒì„±")

@st.cache_data(ttl=3600)
def cached_embeddings(api_key: str, model: str, texts: list[str]) -> np.ndarray:
    if not texts:
        return np.zeros((0, 3), dtype=np.float32)
    _client = OpenAI(api_key=api_key)
    r = _client.embeddings.create(model=model, input=texts)
    return np.array([d.embedding for d in r.data], dtype=np.float32)

def embed_texts(client: OpenAI, embed_model: str, texts: list[str]) -> np.ndarray:
    return cached_embeddings(client.api_key, embed_model, texts)

with st.expander("RAG ì˜µì…˜ (ì„ íƒ)"):
    rag_enabled = st.toggle("íšŒì‚¬ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸/ì½”ì¹­ ì‚¬ìš©", value=True, key="rag_on")
    top_k = st.slider("ê²€ìƒ‰ ìƒìœ„ K", 1, 8, 4, 1, key="topk")
    if "rag_store" not in st.session_state:
        st.session_state.rag_store = {"chunks": [], "embeds": None}
    docs = st.file_uploader("íšŒì‚¬ ë¬¸ì„œ ì—…ë¡œë“œ (TXT/MD/PDF, ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥)", type=["txt","md","pdf"], accept_multiple_files=True)
    chunk_size = st.slider("ì²­í¬ ê¸¸ì´(ë¬¸ì)", 400, 2000, 900, 100)
    chunk_ovlp = st.slider("ì˜¤ë²„ë©(ë¬¸ì)", 0, 400, 150, 10)
    if docs:
        with st.spinner("ë¬¸ì„œ ì¸ë±ì‹± ì¤‘..."):
            chunks=[]
            for up in docs:
                t = read_file_to_text(up)
                if t: chunks += chunk_text(t, chunk_size, chunk_ovlp)
            if chunks:
                embs = embed_texts(client, "text-embedding-3-small", chunks)
                st.session_state.rag_store["chunks"] += chunks
                if st.session_state.rag_store["embeds"] is None or st.session_state.rag_store["embeds"].size==0:
                    st.session_state.rag_store["embeds"] = embs
                else:
                    st.session_state.rag_store["embeds"] = np.vstack([st.session_state.rag_store["embeds"], embs])
                st.success(f"ì¶”ê°€ ì²­í¬ {len(chunks)}ê°œ")

def cosine_topk(matrix: np.ndarray, query: np.ndarray, k: int = 4):
    if matrix.size == 0:
        return np.array([]), np.array([], dtype=int)
    qn = query / (np.linalg.norm(query, axis=1, keepdims=True) + 1e-12)
    mn = matrix / (np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12)
    sims = mn @ qn.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    return sims[idx], idx

def retrieve_supports(qtext: str, k: int):
    store = st.session_state.rag_store
    chs, embs = store.get("chunks", []), store.get("embeds")
    if not st.session_state.get("rag_on") or embs is None or not chs:
        return []
    qv = embed_texts(client, "text-embedding-3-small", [qtext])
    scores, idxs = cosine_topk(embs, qv, k=k)
    return [("íšŒì‚¬ìë£Œ", float(s), chs[int(i)]) for s,i in zip(scores, idxs)]

TYPE_INSTRUCTIONS = {
    "í–‰ë™(STAR)": "ê³¼ê±° ì‹¤ë¬´ ì‚¬ë¡€ë¥¼ ëŒì–´ë‚´ë„ë¡ S(ìƒí™©)-T(ê³¼ì œ)-A(í–‰ë™)-R(ì„±ê³¼)ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸",
    "ê¸°ìˆ  ì‹¬ì¸µ": "í•µì‹¬ ê¸°ìˆ ì  ì˜ì‚¬ê²°ì •Â·íŠ¸ë ˆì´ë“œì˜¤í”„Â·ì„±ëŠ¥/ë¹„ìš©/í’ˆì§ˆ ì§€í‘œë¥¼ íŒŒê³ ë“œëŠ” ì‹¬ì¸µ ì§ˆë¬¸",
    "í•µì‹¬ê°€ì¹˜ ì í•©ì„±": "í•µì‹¬ê°€ì¹˜ì™€ íƒœë„ë¥¼ ê²€ì¦í•˜ëŠ”, ìƒí™©ê¸°ë°˜ í–‰ë™ì„ ìœ ë„í•˜ëŠ” ì§ˆë¬¸",
    "ì—­ì§ˆë¬¸": "ì§€ì›ìê°€ íšŒì‚¬ë¥¼ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í†µì°°ë ¥ ìˆëŠ” ì—­ì§ˆë¬¸"
}
def _similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()
def pick_diverse(cands: list[str], hist: list[str], gamma: float = 0.35) -> str:
    if not cands: return ""
    if not hist:  return random.choice(cands)
    best=None; best_score=1e9
    for q in cands:
        sims=[_similarity(q,h) for h in hist] or [0.0]
        score=(sum(sims)/len(sims)) + gamma*np.std(sims)
        if score < best_score:
            best_score=score; best=q
    return best

q_type = st.selectbox("ì§ˆë¬¸ ìœ í˜•", list(TYPE_INSTRUCTIONS.keys()))
level  = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"])
hint   = st.text_input("ì§ˆë¬¸ ìƒì„± íŒíŠ¸(ì„ íƒ)", placeholder="ì˜ˆ: ì „í™˜ í¼ë„ / ëª¨ë¸ ì„±ëŠ¥-ë¹„ìš© / ë°ì´í„° í’ˆì§ˆ")

if "history" not in st.session_state:
    st.session_state.history = []
if "current_question" not in st.session_state:
    st.session_state.current_question = ""

if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", use_container_width=True, type="primary"):
    st.session_state.answer_text = ""
    try:
        supports=[]
        if st.session_state.get("rag_on"):
            base_q = hint.strip() or f"{company.get('role','')} {' '.join(company.get('responsibilities', [])[:3])}"
            supports = retrieve_supports(base_q, st.session_state.get("topk",4))

        ctx = textwrap.dedent(f"""
        [íšŒì‚¬ëª…] {company.get('company_name','')}
        [íšŒì‚¬ ì†Œê°œ] {company.get('company_intro_site') or ''}
        [ëª¨ì§‘ ë¶„ì•¼] {company.get('role','')}
        [ì£¼ìš” ì—…ë¬´] {", ".join(company.get('responsibilities', [])[:6])}
        [ìê²© ìš”ê±´] {", ".join(company.get('qualifications', [])[:6])}
        [ìš°ëŒ€ ì‚¬í•­] {", ".join(company.get('preferences', [])[:6])}
        [í•µì‹¬ê°€ì¹˜] {", ".join(company.get('values', [])[:6])}
        """).strip()
        rag_note = ""
        if supports:
            joined="\n".join([f"- ({s:.2f}) {txt[:200]}" for _,s,txt in supports[:3]])
            rag_note=f"\n[ê·¼ê±° ë°œì·Œ]\n{joined}"

        seed = int(time.time()*1000) % 2_147_483_647
        sys = f"""ë„ˆëŠ” '{company.get('company_name','')}'ì˜ '{company.get('role','')}' ë©´ì ‘ê´€ì´ë‹¤.
íšŒì‚¬/ì§ë¬´ ì»¨í…ìŠ¤íŠ¸(ì—…ë¬´/ìê²©/ìš°ëŒ€), (ìˆë‹¤ë©´) ê·¼ê±° ë¬¸ì„œë¥¼ ë°˜ì˜í•˜ì—¬ **{q_type}** ìœ í˜•({TYPE_INSTRUCTIONS[q_type]})ì˜ ì§ˆë¬¸ **6ê°œ í›„ë³´**ë¥¼ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ë¼.
ì„œë¡œ **í˜•íƒœÂ·ê´€ì Â·í‚¤ì›Œë“œ**ê°€ ë‹¬ë¼ì•¼ í•˜ë©° ë‚œì´ë„ëŠ” {level}. ì§€í‘œ/ìˆ˜ì¹˜/ê¸°ê°„/ê·œëª¨/ë¦¬ìŠ¤í¬ ìš”ì†Œë¥¼ ì„ì–´ë¼.
í¬ë§·: 1) ... 2) ... 3) ... ... (í•œ ì¤„ì”©)"""
        user = f"""[ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n[íŒíŠ¸]\n{hint}{rag_note}\n[ëœë¤ì‹œë“œ] {seed}"""

        resp = client.chat.completions.create(
            model=MODEL, temperature=0.9,
            messages=[{"role":"system","content":sys},{"role":"user","content":user}]
        )
        raw = resp.choices[0].message.content.strip()
        cands = [re.sub(r'^\s*\d+\)\s*','',line).strip() for line in raw.splitlines() if re.match(r'^\s*\d+\)', line)]
        if not cands:
            cands = [l.strip("- ").strip() for l in raw.splitlines() if len(l.strip())>0][:6]
        hist_qs = [h["question"] for h in st.session_state.get("history", [])][-10:]
        selected = pick_diverse(cands, hist_qs)
        st.session_state.current_question = selected or (cands[0] if cands else "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
        st.session_state.last_supports_q = supports
    except Exception as e:
        st.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")

st.text_area("ì§ˆë¬¸", height=110, value=st.session_state.get("current_question",""))
if st.session_state.get("rag_on") and st.session_state.get("last_supports_q"):
    with st.expander("ì§ˆë¬¸ ìƒì„±ì— ì‚¬ìš©ëœ ê·¼ê±° ë³´ê¸°"):
        for i, (_, sc, txt) in enumerate(st.session_state.last_supports_q, 1):
            st.markdown(f"**[{i}] sim={sc:.3f}**\n\n{txt[:600]}{'...' if len(txt)>600 else ''}")
            st.markdown("---")

# ==========================================================
# â‘£ ë‚˜ì˜ ë‹µë³€ / ì½”ì¹­ â€” (ê¸°ì¡´ v2.2 ì±„ì /ê°œì„ /ìˆ˜ì •ë³¸ ë‹µë³€ ìœ ì§€)
# ==========================================================
st.subheader("â‘£ ë‚˜ì˜ ë‹µë³€ / ì½”ì¹­")

CRITERIA = [
    "ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜",
    "ì‹œìŠ¤í…œì„¤ê³„","íŠ¸ë ˆì´ë“œì˜¤í”„","ì„±ëŠ¥/ë¹„ìš©","í’ˆì§ˆ/ì‹ ë¢°ì„±","ë¦¬ìŠ¤í¬ê´€ë¦¬",
    "ë³´ì•ˆ/ì»´í”Œë¼ì´ì–¸ìŠ¤","ì‹¤í—˜/ê²€ì¦","ì˜í–¥ë„","ì„œìˆ ë ¥"
]
QUESTION_MAP = [
    (re.compile(r"ë°°ì¹˜|ìŠ¤íŠ¸ë¦¬ë°|ì¹´í”„ì¹´|í”Œë§í¬|ìŠ¤íŒŒí¬|íŒŒì´í”„ë¼ì¸|ì•„í‚¤í…ì²˜", re.I),
     {"ì‹œìŠ¤í…œì„¤ê³„":0.3,"íŠ¸ë ˆì´ë“œì˜¤í”„":0.25,"ì„±ëŠ¥/ë¹„ìš©":0.2,"í’ˆì§ˆ/ì‹ ë¢°ì„±":0.15,"ë°ì´í„°/ì§€í‘œ":0.1}),
    (re.compile(r"ì§€í‘œ|kpi|metric|ì¸¡ì •|í¼ë„|ë¶„ì„", re.I),
     {"ë°ì´í„°/ì§€í‘œ":0.4,"ë¬¸ì œì •ì˜":0.2,"ì˜í–¥ë„":0.2,"ì‹¤í—˜/ê²€ì¦":0.2}),
    (re.compile(r"ë³´ì•ˆ|security|ì¹¨í•´|ì»´í”Œë¼ì´ì–¸ìŠ¤|gdpr|hipaa|ì¸ì¦", re.I),
     {"ë³´ì•ˆ/ì»´í”Œë¼ì´ì–¸ìŠ¤":0.4,"ë¦¬ìŠ¤í¬ê´€ë¦¬":0.3,"í’ˆì§ˆ/ì‹ ë¢°ì„±":0.15,"ì‹œìŠ¤í…œì„¤ê³„":0.15}),
    (re.compile(r"í˜‘ì—…|ê°ˆë“±|ì»¤ë®¤ë‹ˆì¼€ì´ì…˜|í˜‘ì˜|ì¡°ìœ¨", re.I),
     {"í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜":0.5,"ë¬¸ì œì •ì˜":0.2,"ì˜í–¥ë„":0.3}),
]
def detect_criteria_weights(question: str) -> Dict[str,float]:
    q = question or ""
    for pat, weights in QUESTION_MAP:
        if pat.search(q):
            return weights
    return {"ë¬¸ì œì •ì˜":0.2,"ë°ì´í„°/ì§€í‘œ":0.2,"ì‹¤í–‰ë ¥/ì£¼ë„ì„±":0.3,"í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜":0.15,"ê³ ê°ê°€ì¹˜":0.15}

def coach_answer(company: dict, question: str, answer: str, supports: list[Tuple[str,float,str]]) -> dict:
    q_trim = (question or "")[:500]
    a_trim = (answer or "")[:1400]
    ctx = textwrap.dedent(f"""
    [íšŒì‚¬ëª…] {company.get('company_name','')}
    [ëª¨ì§‘ ë¶„ì•¼] {company.get('role','')}
    [ì£¼ìš” ì—…ë¬´] {", ".join(company.get('responsibilities', [])[:6])}
    [ìê²© ìš”ê±´] {", ".join(company.get('qualifications', [])[:6])}
    [ìš°ëŒ€ ì‚¬í•­] {", ".join(company.get('preferences', [])[:6])}
    """).strip()

    weights = detect_criteria_weights(q_trim)
    crit_list = list(weights.keys())

    sys = (
        "ë„ˆëŠ” í†±í‹°ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. ì•„ë˜ ê¸°ì¤€ë“¤ì— ëŒ€í•´ì„œë§Œ 0~20 ì •ìˆ˜ ì ìˆ˜ì™€ 'ê°ì  ìš”ì¸', 'ê°œì„  í¬ì¸íŠ¸'ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•˜ê³ , "
        "'revised_answer' í‚¤ì— ìˆ˜ì •ë³¸ ë‹µë³€(STAR, 10~14ë¬¸ì¥)ì„ ë‹´ì•„ë¼.\n"
        "ì¶œë ¥ ì˜ˆì‹œ: {\"ë¬¸ì œì •ì˜\":{\"score\":14,\"penalty\":\"í•µì‹¬ ì œì•½ ëˆ„ë½\",\"improve\":\"ë¬¸ì œ ê²½ê³„ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ KPIë¥¼ ë¨¼ì € ëª…ì‹œ\"}, ... , \"revised_answer\":\"...\"}\n"
        "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ì¤€ì€ ë„£ì§€ ë§ê³ , í•œêµ­ì–´ë¡œ ê°„ê²°/êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ë¼."
    )
    user = f"""[ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n\n[ë©´ì ‘ ì§ˆë¬¸]\n{q_trim}\n\n[í›„ë³´ì ë‹µë³€]\n{a_trim}\n\n[ì±„ì  ê¸°ì¤€ ëª©ë¡]\n{', '.join(crit_list)}"""

    resp = client.chat_completions.create if hasattr(client, "chat_completions") else client.chat.completions.create
    out = resp(model=MODEL, temperature=0.25,
               messages=[{"role":"system","content":sys},{"role":"user","content":user}])
    content = out.choices[0].message.content.strip()

    data=None
    try:
        m = re.search(r"\{.*\}", content, re.S)
        if m: data = json.loads(m.group(0))
    except Exception:
        data=None

    scores = {c: None for c in CRITERIA}
    penalties, improves = {}, {}
    revised = ""
    if isinstance(data, dict):
        revised = data.get("revised_answer","")
        for c in crit_list:
            obj = data.get(c) or {}
            try:
                sc = int(obj.get("score"))
                sc = max(0, min(20, sc))
                scores[c] = sc
                penalties[c] = _clean_text(obj.get("penalty",""))
                improves[c]  = _clean_text(obj.get("improve",""))
            except Exception:
                pass

    used = [(c, s, weights[c]) for c,s in scores.items() if s is not None and c in weights]
    if used:
        num = sum(s * w for _,s,w in used)
        den = sum(w for _,_,w in used)
        final_score = int(round((num/den) * 5))  # 0~100
    else:
        final_score = 0

    lines = [f"ì´ì : {final_score}/100", "", "2. ê¸°ì¤€ë³„ ê·¼ê±°(ì ìˆ˜/ê°ì /ê°œì„ ):"]
    if used:
        for c,s,_ in used:
            p = penalties.get(c,"")
            im= improves.get(c,"")
            lines.append(f"- **{c}({s}/20)**  ê°ì : {p or 'â€”'} / ê°œì„ : {im or 'â€”'}")
    else:
        lines.append("- (í•´ë‹¹ ê¸°ì¤€ ì—†ìŒ)")
    if revised:
        lines += ["", "5. ìˆ˜ì •ë³¸ ë‹µë³€:", revised]

    comp_vector = [scores.get(c) if scores.get(c) is not None else 0 for c in CRITERIA]

    return {
        "raw": "\n".join(lines),
        "score": final_score,
        "competencies": comp_vector,
        "labels": CRITERIA,
        "scores_raw": scores,
    }

ans = st.text_area("ì—¬ê¸°ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš” (STAR ê¶Œì¥: ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)", height=180, key="answer_text")

if st.button("ì±„ì  & ì½”ì¹­", type="primary", use_container_width=True):
    if not st.session_state.get("current_question"):
        st.warning("ë¨¼ì € 'ìƒˆ ì§ˆë¬¸ ë°›ê¸°'ë¡œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì½”ì¹­ ì¤‘..."):
            sups=[]
            if st.session_state.get("rag_on"):
                q_for_rag = (st.session_state["current_question"][:500]
                             + "\n" + st.session_state.answer_text[:800])
                sups = retrieve_supports(q_for_rag, st.session_state.get("topk",4))
            res = coach_answer(company, st.session_state["current_question"], st.session_state.answer_text, sups)
            st.session_state.history.append({
                "ts": pd.Timestamp.now(),
                "question": st.session_state["current_question"],
                "user_answer": st.session_state.answer_text,
                "score": res.get("score"),
                "feedback": res.get("raw"),
                "supports": sups,
                "competencies": res.get("competencies"),
                "labels": res.get("labels"),
                "scores_raw": res.get("scores_raw")
            })

# ==========================================================
# â‘¤ ê²°ê³¼/ë ˆì´ë”/CSV â€” ìµœì‹  vs ì„¸ì…˜í‰ê·  (2ì¤‘ í´ë¼), ëˆ„ì í•©/ì‹œë„íšŸìˆ˜ í‘œì‹œ
# ==========================================================
st.divider()
st.subheader("í”¼ë“œë°± ê²°ê³¼")
if st.session_state.history:
    last = st.session_state.history[-1]
    c1,c2 = st.columns([1,3])
    with c1: st.metric("ì´ì (/100)", last.get("score","â€”"))
    with c2: st.markdown(last.get("feedback",""))

    if st.session_state.get("rag_on") and last.get("supports"):
        with st.expander("ì½”ì¹­ì— ì‚¬ìš©ëœ ê·¼ê±° ë³´ê¸°"):
            for i,(_,sc,txt) in enumerate(last["supports"],1):
                st.markdown(f"**[{i}] sim={sc:.3f}**\n\n{txt[:800]}{'...' if len(txt)>800 else ''}")
                st.markdown("---")
else:
    st.info("ì•„ì§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("ì—­ëŸ‰ ë ˆì´ë” (ì„¸ì…˜ ëˆ„ì , NAëŠ” 0ìœ¼ë¡œ í‘œì‹œ)")

def comp_df(hist):
    rows=[]; labels=CRITERIA
    for h in hist:
        vec = h.get("competencies")
        if not vec: continue
        if len(vec) < len(labels): vec += [0]*(len(labels)-len(vec))
        rows.append(vec[:len(labels)])
    if not rows: return None, labels
    df = pd.DataFrame(rows, columns=labels)
    return df, labels

cdf, labels = comp_df(st.session_state.history)
if cdf is not None:
    latest = cdf.iloc[-1].tolist()
    avg    = cdf.mean().tolist()  # ì„¸ì…˜ í‰ê· (ëˆ„ì  ì„±í–¥)
    if PLOTLY_OK:
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(r=latest+[latest[0]], theta=labels+[labels[0]],
                                      fill='toself', name="ìµœì‹ ", opacity=0.7))
        fig.add_trace(go.Scatterpolar(r=avg+[avg[0]], theta=labels+[labels[0]],
                                      fill='toself', name="ì„¸ì…˜ í‰ê· ", opacity=0.4))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,20])),
                          showlegend=True, height=440)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.bar_chart(pd.DataFrame({"latest": latest, "avg": avg}, index=labels))

    attempts = len(cdf)
    sum_row  = (cdf.sum()).astype(int).to_dict()
    last = st.session_state.history[-1]
    raw = last.get("scores_raw", {})
    disp = []
    for c in labels:
        v = raw.get(c)
        disp.append("-" if v is None else v)
    disp_df = pd.DataFrame([disp], columns=labels)
    disp_df["ëˆ„ì í•©(ê°ì¶•)"] = int(sum(v for v in cdf.iloc[:, :].sum().tolist()))
    disp_df["ì‹œë„íšŸìˆ˜"] = attempts
    st.dataframe(disp_df, use_container_width=True)
    st.caption("íŒŒë€ìƒ‰: ìµœì‹  / ì´ˆë¡ìƒ‰: ì„¸ì…˜ í‰ê· . í‘œëŠ” ìµœì‹  ì ìˆ˜(NA='-')ì™€ ì„¸ì…˜ ëˆ„ì í•©Â·ì‹œë„íšŸìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
else:
    st.caption("ì•„ì§ ì—­ëŸ‰ ì ìˆ˜ê°€ íŒŒì‹±ëœ ì½”ì¹­ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV)")
def build_report(hist):
    rows=[]
    for h in hist:
        row={"timestamp":h.get("ts"),"question":h.get("question"),"user_answer":h.get("user_answer"),
             "score":h.get("score"),"feedback_raw":h.get("feedback")}
        comps=h.get("competencies")
        if comps:
            for k,v in zip(CRITERIA, comps): row[f"comp_{k}"]=v
            row["comp_sum"] = sum([int(v) for v in comps])
        sups=h.get("supports") or []
        row["supports_preview"]=" || ".join([s[2][:120].replace("\n"," ") for s in sups])
        rows.append(row)
    return pd.DataFrame(rows) if rows else pd.DataFrame(columns=["timestamp","question","user_answer","score","feedback_raw","supports_preview","comp_sum"])
rep = build_report(st.session_state.history)
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=rep.to_csv(index=False).encode("utf-8-sig"),
                   file_name="interview_session_report.csv", mime="text/csv")

st.caption("ê²€ìƒ‰â†’ìƒì„¸ ê³µê³  URLâ†’(WebBaseLoader ë³¸ë¬¸ ìˆ˜ì§‘)â†’LLM êµ¬ì¡°í™” ì¶”ì¶œ. ì‹¤íŒ¨ ì‹œ ì›ë¬¸ íŒŒì„œë¡œ í´ë°±í•©ë‹ˆë‹¤.")
