# -*- coding: utf-8 -*-
import os, io, re, json, textwrap, urllib.parse, difflib, random, time
from typing import List, Dict, Tuple, Optional

import numpy as np
import pandas as pd
import streamlit as st

# ------------------------------ Optional deps ------------------------------
try:
    import pypdf
except Exception:
    pypdf = None

try:
    import plotly.graph_objects as go
    PLOTLY_OK = True
except Exception:
    PLOTLY_OK = False

try:
    from openai import OpenAI
except ImportError:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openaië¥¼ ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

import requests
from bs4 import BeautifulSoup

# ------------------------------ Page config ------------------------------
st.set_page_config(page_title="íšŒì‚¬ íŠ¹í™” ê°€ìƒ ë©´ì ‘ ì½”ì¹˜", page_icon="ğŸ¯", layout="wide")

# ------------------------------ Secrets loader ------------------------------
def _secrets_file_exists() -> bool:
    candidates = [
        os.path.join(os.path.expanduser("~"), ".streamlit", "secrets.toml"),
        os.path.join(os.getcwd(), ".streamlit", "secrets.toml"),
    ]
    return any(os.path.exists(p) for p in candidates)

def load_api_key_from_env_or_secrets() -> Optional[str]:
    key = os.getenv("OPENAI_API_KEY")
    if key: return key
    try:
        if _secrets_file_exists() or hasattr(st, "secrets"):
            return st.secrets.get("OPENAI_API_KEY", None)
    except Exception:
        pass
    return None

def load_naver_keys():
    cid = os.getenv("NAVER_CLIENT_ID")
    csec = os.getenv("NAVER_CLIENT_SECRET")
    try:
        if hasattr(st, "secrets"):
            cid = cid or st.secrets.get("NAVER_CLIENT_ID", None)
            csec = csec or st.secrets.get("NAVER_CLIENT_SECRET", None)
    except Exception:
        pass
    return cid, csec

NAVER_ID, NAVER_SECRET = load_naver_keys()
UA = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"}

# ------------------------------ Text utils ------------------------------
def _clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", t or "").strip()

def chunk_text(text: str, size: int = 900, overlap: int = 150):
    text = re.sub(r"\s+", " ", text).strip()
    if not text: return []
    out, start = [], 0
    while start < len(text):
        end = min(len(text), start + size)
        out.append(text[start:end])
        if end == len(text): break
        start = max(0, end - overlap)
    return out

def read_file_to_text(uploaded) -> str:
    name = uploaded.name.lower()
    data = uploaded.read()
    if name.endswith((".txt", ".md")):
        for enc in ("utf-8", "cp949", "euc-kr"):
            try: return data.decode(enc)
            except Exception: continue
        return data.decode("utf-8", errors="ignore")
    elif name.endswith(".pdf"):
        if pypdf is None:
            st.warning("pypdfê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— pypdf ì¶”ê°€.")
            return ""
        try:
            reader = pypdf.PdfReader(io.BytesIO(data))
            return "\n\n".join([(reader.pages[i].extract_text() or "") for i in range(len(reader.pages))])
        except Exception as e:
            st.warning(f"PDF íŒŒì‹± ì‹¤íŒ¨({uploaded.name}): {e}")
            return ""
    return ""

# ------------------------------ NAVER Open API (ë‰´ìŠ¤/ì›¹ê²€ìƒ‰) ------------------------------
def _naver_api_get(api: str, params: dict, cid: str, csec: str):
    url = f"https://openapi.naver.com/v1/search/{api}.json"
    headers = {
        "X-Naver-Client-Id": cid,
        "X-Naver-Client-Secret": csec,
        "User-Agent": UA["User-Agent"],
    }
    r = requests.get(url, headers=headers, params=params, timeout=8)
    if r.status_code != 200:
        return None
    return r.json()

def naver_search_news(query: str, display: int = 10, sort: str = "date") -> list[dict]:
    cid, csec = load_naver_keys()
    if not (cid and csec): return []
    js = _naver_api_get("news", {"query": query, "display": display, "sort": sort}, cid, csec)
    if not js: return []
    out = []
    for it in js.get("items", []):
        title = _clean_text(re.sub(r"</?b>|&quot;|&apos;|&amp;|&lt;|&gt;", "", it.get("title","")))
        out.append({"title": title, "link": it.get("link"), "pubDate": it.get("pubDate")})
    return out

def naver_search_web(query: str, display: int = 10, sort: str = "date") -> list[str]:
    cid, csec = load_naver_keys()
    if not (cid and csec): return []
    js = _naver_api_get("webkr", {"query": query, "display": display, "sort": sort}, cid, csec)
    if not js: return []
    links = []
    for it in js.get("items", []):
        link = it.get("link")
        if link and link not in links:
            links.append(link)
    return links

# ------------------------------ ì±„ìš©ê³µê³  URL ë°œê²¬/ìƒì„¸ ì§„ì… ------------------------------
SEARCH_ENGINES = ["https://duckduckgo.com/html/?q={query}"]
JOB_SITES = ["wanted.co.kr","saramin.co.kr","jobkorea.co.kr","rocketpunch.com",
             "indeed.com","linkedin.com","recruit.navercorp.com","kakao.recruit","naver"]

def _domain(u: str|None) -> str|None:
    if not u: return None
    try:
        if not u.startswith("http"): u = "https://" + u
        return urllib.parse.urlparse(u).netloc.lower().replace("www.","")
    except Exception:
        return None

def discover_job_from_homepage(homepage: str, limit: int = 5) -> list[str]:
    if not homepage: return []
    try:
        if not homepage.startswith("http"): homepage = "https://" + homepage
        r = requests.get(homepage, timeout=8, headers=UA)
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
            return []
        soup = BeautifulSoup(r.text, "html.parser")
        links=[]
        for path in ["careers","recruit","jobs","career","ì±„ìš©","ì¸ì¬ì˜ì…","recruitment","join"]:
            links.append(urllib.parse.urljoin(homepage.rstrip("/") + "/", path))
        for a in soup.find_all("a", href=True):
            href = a["href"]
            text = (a.get_text() or "").lower()
            if any(k in href.lower() or k in text for k in ["careers","recruit","jobs","ì±„ìš©","ì¸ì¬","ì…ì‚¬ì§€ì›","ì±„ìš©ê³µê³ ","ì¸ì¬ì˜ì…","ì»¤ë¦¬ì–´"]):
                links.append(urllib.parse.urljoin(homepage, href))
        out=[]; seen=set()
        for lk in links:
            d = _domain(lk)
            if lk not in seen and d:
                seen.add(lk); out.append(lk)
                if len(out) >= limit: break
        return out[:limit]
    except Exception:
        return []

def discover_job_posting_urls(company_name: str, role: str, homepage: str|None, limit: int = 5) -> list[str]:
    urls = []
    urls += discover_job_from_homepage(homepage, limit=limit) if homepage else []
    if urls: return urls[:limit]

    if NAVER_ID and NAVER_SECRET:
        for dom in JOB_SITES:
            if len(urls) >= limit: break
            q = f"{company_name} {role} site:{dom}" if role else f"{company_name} ì±„ìš© site:{dom}"
            links = naver_search_web(q, display=5, sort="date")
            for lk in links:
                if _domain(lk) and dom in _domain(lk) and lk not in urls:
                    urls.append(lk)
                if len(urls) >= limit: break
        if urls: return urls[:limit]

    site_part = " OR ".join([f'site:{d}' for d in JOB_SITES])
    q = f'{company_name} {role} ({site_part})' if role else f'{company_name} ì±„ìš© ({site_part})'
    for engine in SEARCH_ENGINES:
        url = engine.format(query=urllib.parse.quote(q))
        try:
            r = requests.get(url, timeout=8, headers=UA)
            if r.status_code != 200: continue
            soup = BeautifulSoup(r.text, "html.parser")
            for a in soup.find_all("a", href=True):
                href = a["href"]
                if href.startswith("/l/?kh=-1&uddg="):
                    href = urllib.parse.unquote(href.split("/l/?kh=-1&uddg=")[-1])
                dom = _domain(href)
                if not dom: continue
                if any(d in dom for d in JOB_SITES):
                    if href not in urls:
                        urls.append(href)
                if len(urls) >= limit: break
        except Exception:
            continue
    return urls[:limit]

def _first_detail_from_list(url: str, role_hint: str = "") -> Optional[str]:
    try:
        r = requests.get(url, timeout=10, headers=UA)
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
            return None
        soup = BeautifulSoup(r.text, "html.parser")
        dom = _domain(url) or ""
        # Wanted
        if "wanted.co.kr" in dom:
            for a in soup.select("a[href*='/wd/']"):
                href = urllib.parse.urljoin(url, a.get("href"))
                title = (a.get_text() or "").strip()
                if (not role_hint) or (role_hint in title):
                    return href
        # Saramin
        if "saramin.co.kr" in dom:
            for a in soup.select("a[href*='view?idx=']"):
                return urllib.parse.urljoin(url, a.get("href"))
        # JobKorea
        if "jobkorea.co.kr" in dom:
            for a in soup.select("a[href*='/Recruit/GI_Read/']"):
                return urllib.parse.urljoin(url, a.get("href"))
        # Generic
        for a in soup.find_all("a", href=True):
            href = urllib.parse.urljoin(url, a["href"])
            if re.search(r"/(wd|view|read|detail|posting|jobs?)/", href, re.I):
                return href
    except Exception:
        return None
    return None

def ensure_detail_url(u: str, role_hint: str) -> str:
    if re.search(r"/(wd|view|read|detail|posting|jobs?)/", u, re.I): return u
    deep = _first_detail_from_list(u, role_hint)
    return deep or u

# ------------------------------ ì±„ìš© í…ìŠ¤íŠ¸ ìˆ˜ì§‘ + LLM ìš”ì•½ ------------------------------
def fetch_page_text(url: str, max_chars: int = 16000) -> str:
    try:
        r = requests.get(url, timeout=12, headers=UA)
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""): return ""
        soup = BeautifulSoup(r.text, "html.parser")
        # remove script/style
        for s in soup(["script","style","noscript"]): s.decompose()
        text = _clean_text(soup.get_text(" "))
        return text[:max_chars]
    except Exception:
        return ""

def llm_summarize_job_sections(client: OpenAI, model: str, raw_text: str) -> dict:
    """
    LLMì—ê²Œ JSONìœ¼ë¡œ ì„¸ ì„¹ì…˜ì„ ìš”ì•½ ìš”ì²­.
    ë°˜í™˜: {"responsibilities": [...], "qualifications":[...], "preferences":[...]}
    """
    if not raw_text.strip():
        return {"responsibilities":[], "qualifications":[], "preferences":[]}
    sys = ("ë„ˆëŠ” ì±„ìš© ê³µê³  ì „ë¬¸ì„ ë¶„ì„í•´ ì•„ë˜ ì„¸ ì„¹ì…˜ì„ í•œêµ­ì–´ ë¶ˆë¦¿ìœ¼ë¡œ **ìš”ì•½**í•œë‹¤. "
           "ê° í•­ëª©ì€ ìµœëŒ€ 12ê°œ, ë¬¸ì¥ì€ ê°„ê²°í•˜ê²Œ. "
           "ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥: "
           "{\"responsibilities\":[], \"qualifications\":[], \"preferences\":[]}")
    user = f"ë‹¤ìŒì€ ì±„ìš©ê³µê³  ì›ë¬¸ì´ë‹¤. ìš”ì•½ ê²°ê³¼ë§Œ JSONìœ¼ë¡œ ë°˜í™˜í•˜ë¼.\n\n<<<ì›ë¬¸>>>\n{raw_text}\n<<<ë>>>"
    try:
        resp = client.chat.completions.create(
            model=model, temperature=0.2,
            messages=[{"role":"system","content":sys},{"role":"user","content":user}]
        )
        content = resp.choices[0].message.content.strip()
        # JSONë§Œ ì¶”ì¶œ
        m = re.search(r"\{.*\}", content, re.S)
        if m:
            data = json.loads(m.group(0))
            def _norm(arr):
                return [_clean_text(x)[:300] for x in (arr or []) if _clean_text(x)]
            return {
                "responsibilities": _norm(data.get("responsibilities", []))[:12],
                "qualifications":   _norm(data.get("qualifications", []))[:12],
                "preferences":      _norm(data.get("preferences", []))[:12],
            }
    except Exception:
        pass
    # ì™„ì „ í´ë°±: ë‹¨ìˆœ ë¬¸ì¥ ë¶„ë¦¬
    parts = [x.strip(" -â€¢Â·â–ªï¸â€”") for x in re.split(r"[â€¢\n\r\t]+", raw_text)]
    parts = [p for p in parts if 5 < len(p) < 220][:24]
    n = len(parts); k = n//3 if n>=9 else max(3, n//3 or 1)
    return {"responsibilities":parts[:k], "qualifications":parts[k:2*k], "preferences":parts[2*k:2*k+k]}

# ------------------------------ íšŒì‚¬/ìš”ì•½ ìƒì„± ------------------------------
def fetch_site_snippets(base_url: str | None, company_name_hint: str | None = None) -> dict:
    if not base_url:
        return {"values": [], "recent": [], "site_name": None, "about": None}
    url0 = base_url.strip()
    if not url0.startswith("http"): url0 = "https://" + url0
    try:
        r = requests.get(url0, timeout=8, headers=UA)
        if r.status_code != 200 or "text/html" not in r.headers.get("content-type",""):
            return {"values": [], "recent": [], "site_name": None, "about": None}
        soup = BeautifulSoup(r.text, "html.parser")
        # site name
        site_name = None
        og = soup.find("meta", {"property":"og:site_name"}) or soup.find("meta", {"name":"application-name"})
        if og and og.get("content"): site_name = _clean_text(og["content"])
        elif soup.title and soup.title.string: site_name = _clean_text(soup.title.string.split("|")[0])
        # about í›„ë³´
        ps = " ".join([_clean_text(p.get_text(" ")) for p in soup.find_all("p")[:5]])
        about = _clean_text(ps)[:400] if ps else None
        return {"values": [], "recent": [], "site_name": site_name, "about": about}
    except Exception:
        return {"values": [], "recent": [], "site_name": None, "about": None}

def generate_company_intro_only(client: OpenAI, model: str, company_name: str, about_text: str|None) -> str:
    base = about_text or ""
    sys = "ë„ˆëŠ” ì±„ìš©ë‹´ë‹¹ìë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ **íšŒì‚¬ ì†Œê°œë§Œ** í•œêµ­ì–´ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ë¼. ì¶”ì¸¡/ê³¼ì¥ì€ ê¸ˆì§€."
    user = f"[íšŒì‚¬ëª…] {company_name}\n[í…ìŠ¤íŠ¸]\n{base[:1200]}"
    try:
        resp = client.chat.completions.create(model=model, temperature=0.3,
                                             messages=[{"role":"system","content":sys},{"role":"user","content":user}])
        return resp.choices[0].message.content.strip()
    except Exception:
        return _clean_text(base)[:220] or "íšŒì‚¬ ì†Œê°œ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

# ------------------------------ OpenAI client ------------------------------
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    API_KEY = load_api_key_from_env_or_secrets()
    if not API_KEY:
        st.info("í™˜ê²½ë³€ìˆ˜/Secretsì—ì„œ í‚¤ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•„ë˜ì— ì…ë ¥ í›„ ì—”í„°.")
        API_KEY = st.text_input("OPENAI_API_KEY", type="password")
    MODEL = st.selectbox("ì±— ëª¨ë¸", ["gpt-4o-mini","gpt-4o","gpt-4.1-mini"], index=0)
    EMBED_MODEL = st.selectbox("ì„ë² ë”© ëª¨ë¸", ["text-embedding-3-small","text-embedding-3-large"], index=0)
    with st.expander("ë””ë²„ê·¸: ì‹œí¬ë¦¿/ë²„ì „"):
        try:
            import openai as _openai_pkg; ov = getattr(_openai_pkg, "__version__", None)
        except Exception: ov = None
        st.write({"api_key": bool(API_KEY), "naver_keys": bool(NAVER_ID and NAVER_SECRET), "openai": ov})

if not API_KEY:
    st.error("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."); st.stop()
client = OpenAI(api_key=API_KEY, timeout=30.0)

# ==========================================================
# â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥
# ==========================================================
st.subheader("â‘  íšŒì‚¬/ì§ë¬´ ì…ë ¥")
company_name_input = st.text_input("íšŒì‚¬ ì´ë¦„", placeholder="ì˜ˆ: ë„¤ì´ë²„ / Kakao / ì‚¼ì„±SDS")
role_title         = st.text_input("ì§€ì› ì§ë¬´ëª…", placeholder="ë°ì´í„° ì• ë„ë¦¬ìŠ¤íŠ¸ / ML ì—”ì§€ë‹ˆì–´ ...")
job_url_input      = st.text_input("ì±„ìš© ê³µê³  URL(ì„ íƒ)")
homepage_input     = st.text_input("ê³µì‹ í™ˆí˜ì´ì§€ URL(ì„ íƒ)", placeholder="https://...")

if "company_state" not in st.session_state:
    st.session_state.company_state = {}
if "answer_text" not in st.session_state:
    st.session_state.answer_text = ""

def build_company_obj(name: str, homepage: str|None, role: str|None, job_url: str|None) -> dict:
    site = fetch_site_snippets(homepage or None, name)

    # 1) ê³µê³  URL ê²°ì •
    candidates = [job_url] if job_url else discover_job_posting_urls(name, role or "", homepage, limit=4)
    job_detail = None
    if candidates:
        job_detail = ensure_detail_url(candidates[0], role or "")

    # 2) ê³µê³  í…ìŠ¤íŠ¸ â†’ LLM ìš”ì•½ìœ¼ë¡œ 3ì„¹ì…˜ ë§Œë“¤ê¸°
    resp_list, qual_list, pref_list = [], [], []
    raw_len = 0
    if job_detail:
        raw = fetch_page_text(job_detail, max_chars=16000)
        raw_len = len(raw)
        parsed = llm_summarize_job_sections(client, MODEL, raw)
        resp_list, qual_list, pref_list = parsed["responsibilities"], parsed["qualifications"], parsed["preferences"]

    # 3) ë‰´ìŠ¤
    news_items = naver_search_news(name, display=6, sort="date")

    return {
        "company_name": name.strip() or "(íšŒì‚¬ëª… ë¯¸ì„¤ì •)",
        "homepage": homepage or None,
        "company_intro_site": site.get("about"),
        "role": role or "",
        "role_requirements": resp_list,
        "role_qualifications": qual_list,
        "preferences": pref_list,
        "job_url": job_detail or (candidates[0] if candidates else (job_url or None)),
        "news": news_items,
        "_debug": {"job_url": job_detail, "raw_len": raw_len,
                   "resp_cnt": len(resp_list), "qual_cnt": len(qual_list), "pref_cnt": len(pref_list)}
    }

def build_company_summary_md(c: dict) -> str:
    intro = generate_company_intro_only(client, MODEL, c.get("company_name",""), c.get("company_intro_site"))
    md = f"""**íšŒì‚¬ëª…**  
{c.get('company_name')}

**ê°„ë‹¨í•œ íšŒì‚¬ ì†Œê°œ(ìš”ì•½)**  
{intro}

**ì±„ìš© ê³µê³  ì—´ê¸°**  
{"[ë§í¬](" + c["job_url"] + ")" if c.get("job_url") else "â€”"}
"""
    return md

if st.button("íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°", type="primary"):
    if not company_name_input.strip():
        st.warning("íšŒì‚¬ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("íšŒì‚¬/ì§ë¬´/ê³µê³ /ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘ ì¤‘..."):
            cobj = build_company_obj(company_name_input, homepage_input or None, role_title or None, job_url_input or None)
            summary_md = build_company_summary_md(cobj)
            st.session_state.company_state["company"] = cobj
            st.session_state.company_state["summary_md"] = summary_md
        st.success("íšŒì‚¬ ì •ë³´ ê°±ì‹  ì™„ë£Œ")

company = st.session_state.get("company_state",{}).get("company", {
    "company_name": "(íšŒì‚¬ëª… ë¯¸ì„¤ì •)", "homepage": None, "company_intro_site": None, "role": "",
    "role_requirements": [], "role_qualifications": [], "preferences": [], "job_url": None, "news": [], "_debug": {}
})
summary_md = st.session_state.get("company_state",{}).get("summary_md", None)

# ==========================================================
# â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´ (LLM ìš”ì•½ â€” ì„¸ë¡œ ìŠ¤íƒ)
# ==========================================================
st.subheader("â‘¡ íšŒì‚¬ ìš”ì•½ / ì±„ìš© ìš”ê±´")
if summary_md:
    st.markdown(summary_md)

    if company.get("homepage") or company.get("job_url"):
        c1,c2 = st.columns(2)
        with c1:
            if company.get("homepage"): st.link_button("í™ˆí˜ì´ì§€ ì—´ê¸°", company["homepage"])
        with c2:
            if company.get("job_url"): st.link_button("ì±„ìš© ê³µê³  ì—´ê¸°", company["job_url"])

    st.markdown("---")
    st.markdown("#### ì£¼ìš”ì—…ë¬´(ìš”ì•½)")
    if company["role_requirements"]:
        for it in company["role_requirements"]:
            st.markdown(f"- {it}")
    else:
        st.caption("ìš”ì•½ ê°€ëŠ¥í•œ ì£¼ìš”ì—…ë¬´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("#### ìê²©ìš”ê±´(ìš”ì•½)")
    if company["role_qualifications"]:
        for it in company["role_qualifications"]:
            st.markdown(f"- {it}")
    else:
        st.caption("ìš”ì•½ ê°€ëŠ¥í•œ ìê²©ìš”ê±´ì´ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("#### ìš°ëŒ€ì‚¬í•­(ìš”ì•½)")
    if company["preferences"]:
        for it in company["preferences"]:
            st.markdown(f"- {it}")
    else:
        st.caption("ìš”ì•½ ê°€ëŠ¥í•œ ìš°ëŒ€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

    with st.expander("ë””ë²„ê·¸: ê³µê³  ìš”ì•½ ìƒíƒœ"):
        st.json(company.get("_debug", {}))
else:
    st.info("ìœ„ì˜ ì…ë ¥ì„ ì™„ë£Œí•˜ê³  â€˜íšŒì‚¬/ì§ë¬´ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°â€™ë¥¼ ëˆŒëŸ¬ ìš”ì•½/ìš”ê±´ì„ ìƒì„±í•˜ì„¸ìš”.")

# ==========================================================
# â‘¢ ì§ˆë¬¸ ìƒì„± (ì´ì „ê³¼ ë™ì¼)
# ==========================================================
st.subheader("â‘¢ ì§ˆë¬¸ ìƒì„±")

def embed_texts(client: OpenAI, embed_model: str, texts: list[str]) -> np.ndarray:
    if not texts:
        return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(model=embed_model, input=texts)
    return np.array([d.embedding for d in resp.data], dtype=np.float32)

with st.expander("RAG ì˜µì…˜ (ì„ íƒ)"):
    rag_enabled = st.toggle("íšŒì‚¬ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸/ì½”ì¹­ ì‚¬ìš©", value=True, key="rag_on")
    top_k = st.slider("ê²€ìƒ‰ ìƒìœ„ K", 1, 8, 4, 1, key="topk")
    if "rag_store" not in st.session_state:
        st.session_state.rag_store = {"chunks": [], "embeds": None}
    docs = st.file_uploader("íšŒì‚¬ ë¬¸ì„œ ì—…ë¡œë“œ (TXT/MD/PDF, ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥)", type=["txt","md","pdf"], accept_multiple_files=True)
    chunk_size = st.slider("ì²­í¬ ê¸¸ì´(ë¬¸ì)", 400, 2000, 900, 100)
    chunk_ovlp = st.slider("ì˜¤ë²„ë©(ë¬¸ì)", 0, 400, 150, 10)
    if docs:
        with st.spinner("ë¬¸ì„œ ì¸ë±ì‹± ì¤‘..."):
            chunks=[]
            for up in docs:
                t = read_file_to_text(up)
                if t: chunks += chunk_text(t, chunk_size, chunk_ovlp)
            if chunks:
                embs = embed_texts(client, "text-embedding-3-small", chunks)
                st.session_state.rag_store["chunks"] += chunks
                if st.session_state.rag_store["embeds"] is None or st.session_state.rag_store["embeds"].size==0:
                    st.session_state.rag_store["embeds"] = embs
                else:
                    st.session_state.rag_store["embeds"] = np.vstack([st.session_state.rag_store["embeds"], embs])
                st.success(f"ì¶”ê°€ ì²­í¬ {len(chunks)}ê°œ")

def cosine_topk(matrix: np.ndarray, query: np.ndarray, k: int = 4):
    if matrix.size == 0:
        return np.array([]), np.array([], dtype=int)
    qn = query / (np.linalg.norm(query, axis=1, keepdims=True) + 1e-12)
    mn = matrix / (np.linalg.norm(matrix, axis=1, keepdims=True) + 1e-12)
    sims = mn @ qn.T
    sims = sims.reshape(-1)
    idx = np.argsort(-sims)[:k]
    return sims[idx], idx

def retrieve_supports(qtext: str, k: int):
    store = st.session_state.rag_store
    chs, embs = store.get("chunks", []), store.get("embeds")
    if not st.session_state.get("rag_on") or embs is None or not chs:
        return []
    qv = embed_texts(client, "text-embedding-3-small", [qtext])
    scores, idxs = cosine_topk(embs, qv, k=k)
    return [("íšŒì‚¬ìë£Œ", float(s), chs[int(i)]) for s,i in zip(scores, idxs)]

TYPE_INSTRUCTIONS = {
    "í–‰ë™(STAR)": "ê³¼ê±° ì‹¤ë¬´ ì‚¬ë¡€ë¥¼ ëŒì–´ë‚´ë„ë¡ S(ìƒí™©)-T(ê³¼ì œ)-A(í–‰ë™)-R(ì„±ê³¼)ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸",
    "ê¸°ìˆ  ì‹¬ì¸µ": "í•µì‹¬ ê¸°ìˆ ì  ì˜ì‚¬ê²°ì •Â·íŠ¸ë ˆì´ë“œì˜¤í”„Â·ì„±ëŠ¥/ë¹„ìš©/í’ˆì§ˆ ì§€í‘œë¥¼ íŒŒê³ ë“œëŠ” ì‹¬ì¸µ ì§ˆë¬¸",
    "í•µì‹¬ê°€ì¹˜ ì í•©ì„±": "í•µì‹¬ê°€ì¹˜ì™€ íƒœë„ë¥¼ ê²€ì¦í•˜ëŠ”, ìƒí™©ê¸°ë°˜ í–‰ë™ì„ ìœ ë„í•˜ëŠ” ì§ˆë¬¸",
    "ì—­ì§ˆë¬¸": "ì§€ì›ìê°€ íšŒì‚¬ë¥¼ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ í†µì°°ë ¥ ìˆëŠ” ì—­ì§ˆë¬¸"
}

def build_ctx(c: dict) -> str:
    news = ", ".join([_clean_text(n["title"])[:70] for n in c.get("news", [])[:3]])
    return textwrap.dedent(f"""
    [íšŒì‚¬ëª…] {c.get('company_name','')}
    [ëª¨ì§‘ ë¶„ì•¼] {c.get('role','')}
    [ì£¼ìš” ì—…ë¬´] {", ".join(c.get('role_requirements', [])[:6])}
    [ìê²© ìš”ê±´] {", ".join(c.get('role_qualifications', [])[:6])}
    [ìš°ëŒ€ ì‚¬í•­] {", ".join(c.get('preferences', [])[:6])}
    [ìµœê·¼ ì´ìŠˆ/ë‰´ìŠ¤] {news}
    """).strip()

def build_focuses(c: dict, supports: list[Tuple[str,float,str]], k: int = 4) -> list[str]:
    pool=[]
    if c.get("role"): pool.append(c["role"])
    pool += c.get("role_requirements", [])[:6]
    pool += c.get("role_qualifications", [])[:6]
    pool += c.get("preferences", [])[:6]
    for _,_,txt in (supports or [])[:3]:
        pool += [t.strip() for t in re.split(r"[â€¢\-\n\.]", txt) if 6 < len(t.strip()) < 100][:3]
    pool=[p for p in pool if p]; random.shuffle(pool)
    return pool[:k]

def _similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def pick_diverse(cands: list[str], hist: list[str], gamma: float = 0.35) -> str:
    if not cands: return ""
    if not hist:  return random.choice(cands)
    best=None; best_score=1e9
    for q in cands:
        sims=[_similarity(q,h) for h in hist] or [0.0]
        score=(sum(sims)/len(sims)) + gamma*np.std(sims)
        if score < best_score:
            best_score=score; best=q
    return best

q_type = st.selectbox("ì§ˆë¬¸ ìœ í˜•", list(TYPE_INSTRUCTIONS.keys()))
level  = st.selectbox("ë‚œì´ë„/ì—°ì°¨", ["ì£¼ë‹ˆì–´","ë¯¸ë“¤","ì‹œë‹ˆì–´"])
hint   = st.text_input("ì§ˆë¬¸ ìƒì„± íŒíŠ¸(ì„ íƒ)", placeholder="ì˜ˆ: ì „í™˜ í¼ë„ / ëª¨ë¸ ì„±ëŠ¥-ë¹„ìš© / ë°ì´í„° í’ˆì§ˆ")

if "history" not in st.session_state:
    st.session_state.history = []
if "current_question" not in st.session_state:
    st.session_state.current_question = ""

if st.button("ìƒˆ ì§ˆë¬¸ ë°›ê¸°", use_container_width=True, type="primary"):
    st.session_state.answer_text = ""
    try:
        supports=[]
        if st.session_state.get("rag_on"):
            base_q = hint.strip() or f"{company.get('role','')} {' '.join(company.get('role_requirements', [])[:3])}"
            supports = retrieve_supports(base_q, st.session_state.get("topk",4))
        ctx = build_ctx(company)
        focuses = build_focuses(company, supports, k=4)
        seed = int(time.time()*1000) % 2_147_483_647
        sys = f"""ë„ˆëŠ” '{company.get('company_name','')}'ì˜ '{company.get('role','')}' ë©´ì ‘ê´€ì´ë‹¤.
**{q_type}** ìœ í˜•({TYPE_INSTRUCTIONS[q_type]})ì˜ ì§ˆë¬¸ 6ê°œë¥¼ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ë¼. ì„œë¡œ í˜•íƒœ/ê´€ì /í‚¤ì›Œë“œê°€ ë‹¬ë¼ì•¼ í•œë‹¤. ë‚œì´ë„ {level}.
ì•„ë˜ 'í¬ì»¤ìŠ¤' ì¤‘ ìµœì†Œ 1ê°œë¥¼ ë¬¸ì¥ì— ëª…ì‹œì ìœ¼ë¡œ í¬í•¨í•˜ê³ , ì§€í‘œÂ·ìˆ˜ì¹˜Â·ê¸°ê°„Â·ê·œëª¨Â·ë¦¬ìŠ¤í¬ ìš”ì†Œë¥¼ ì ì ˆíˆ ì„ì–´ë¼.
í¬ë§·: 1) ... 2) ... 3) ..."""
        user = f"""[íšŒì‚¬/ì§ë¬´ ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n[í¬ì»¤ìŠ¤]\n- {chr(10).join(focuses)}\n[ëœë¤ì‹œë“œ] {seed}"""
        resp = client.chat.completions.create(model=MODEL, temperature=0.95,
                                              messages=[{"role":"system","content":sys},{"role":"user","content":user}])
        raw = resp.choices[0].message.content.strip()
        cands = [re.sub(r'^\s*\d+\)\s*','',line).strip() for line in raw.splitlines() if re.match(r'^\s*\d+\)', line)]
        if not cands: cands = [l.strip("- ").strip() for l in raw.splitlines() if len(l.strip())>0][:6]
        hist_qs = [h["question"] for h in st.session_state.get("history", [])][-10:]
        selected = pick_diverse(cands, hist_qs)
        st.session_state.current_question = selected or (cands[0] if cands else "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
        st.session_state.last_supports_q = supports
    except Exception as e:
        st.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")

st.text_area("ì§ˆë¬¸", height=110, value=st.session_state.get("current_question",""))

# ==========================================================
# â‘£ ë‚˜ì˜ ë‹µë³€ / ì½”ì¹­ (100ì ì œ) â€” ê¸°ì¡´ ë¡œì§ ìœ ì§€
# ==========================================================
st.subheader("â‘£ ë‚˜ì˜ ë‹µë³€ / ì½”ì¹­")

def coach_answer(company: dict, question: str, answer: str, supports: list[Tuple[str,float,str]]) -> dict:
    ctx = textwrap.dedent(f"""
    [íšŒì‚¬ëª…] {company.get('company_name','')}
    [ëª¨ì§‘ ë¶„ì•¼] {company.get('role','')}
    [ì£¼ìš” ì—…ë¬´] {", ".join(company.get('role_requirements', [])[:6])}
    [ìê²© ìš”ê±´] {", ".join(company.get('role_qualifications', [])[:6])}
    [ìš°ëŒ€ ì‚¬í•­] {", ".join(company.get('preferences', [])[:6])}
    """).strip()
    sys = """ë„ˆëŠ” í†±í‹°ì–´ ë©´ì ‘ ì½”ì¹˜ë‹¤. í•œêµ­ì–´ë¡œ ì•„ë˜ í˜•ì‹ì— ë§ì¶° ë‹µí•˜ë¼:
1) ì´ì : 0~100 ì •ìˆ˜ 1ê°œ
2) ê°•ì : 2~3ê°œ ë¶ˆë¦¿
3) ë¦¬ìŠ¤í¬: 2~3ê°œ ë¶ˆë¦¿
4) ê°œì„  í¬ì¸íŠ¸: 3ê°œ ë¶ˆë¦¿ (í–‰ë™Â·ì§€í‘œÂ·ì„íŒ©íŠ¸ ì¤‘ì‹¬)
5) ìˆ˜ì •ë³¸ ë‹µë³€: STAR(ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼) êµ¬ì¡°ë¡œ ê°„ê²°í•˜ê²Œ
6) ì—­ëŸ‰ ì ìˆ˜(ê° 0~20 ì •ìˆ˜): [ë¬¸ì œì •ì˜, ë°ì´í„°/ì§€í‘œ, ì‹¤í–‰ë ¥/ì£¼ë„ì„±, í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜, ê³ ê°ê°€ì¹˜] â€” ìˆ«ì 5ê°œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ ë§ˆì§€ë§‰ ì¤„ì— ì¶œë ¥"""
    user = f"""[íšŒì‚¬/ì§ë¬´ ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n[ë©´ì ‘ ì§ˆë¬¸]\n{question}\n\n[í›„ë³´ì ë‹µë³€]\n{answer}"""
    resp = client.chat.completions.create(model=MODEL, temperature=0.35,
                                          messages=[{"role":"system","content":sys},{"role":"user","content":user}])
    content = resp.choices[0].message.content.strip()
    score = None
    m = re.search(r'(\d{1,3})\s*(?:/100|ì |$)', content)
    if m: score = int(m.group(1))
    if score is None:
        m10 = re.search(r'(\d{1,2})\s*/\s*10', content)
        if m10: score = max(0, min(100, int(m10.group(1))*10))
    if score is None:
        m_any = re.search(r'\b(\d{1,3})\b', content)
        if m_any: score = max(0, min(100, int(m_any.group(1))))
    if score is not None: score = max(0, min(100, score))
    line = content.splitlines()[-1]
    nums = re.findall(r'\b(\d{1,2})\b', line) or re.findall(r'\b(\d{1,2})\b', content)
    comps=None
    if len(nums)>=5:
        cand=[int(x) for x in nums[:5]]
        if all(0<=x<=5 for x in cand): cand=[x*4 for x in cand]
        if all(0<=x<=10 for x in cand) and any(x>5 for x in cand): cand=[x*2 for x in cand]
        comps=[max(0,min(20,x)) for x in cand]
    return {"raw": content, "score": score, "competencies": comps}

if "history" not in st.session_state:
    st.session_state.history = []

ans = st.text_area("ì—¬ê¸°ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš” (STAR ê¶Œì¥: ìƒí™©-ê³¼ì œ-í–‰ë™-ì„±ê³¼)", height=180, key="answer_text")

if st.button("ì±„ì  & ì½”ì¹­", type="primary", use_container_width=True):
    if not st.session_state.get("current_question"):
        st.warning("ë¨¼ì € 'ìƒˆ ì§ˆë¬¸ ë°›ê¸°'ë¡œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.")
    elif not st.session_state.answer_text.strip():
        st.warning("ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ì½”ì¹­ ì¤‘..."):
            res = coach_answer(company, st.session_state["current_question"], st.session_state.answer_text, [])
            st.session_state.history.append({
                "ts": pd.Timestamp.now(),
                "question": st.session_state["current_question"],
                "user_answer": st.session_state.answer_text,
                "score": res.get("score"),
                "feedback": res.get("raw"),
                "competencies": res.get("competencies")
            })

# ------------------------------ ê²°ê³¼/ë ˆì´ë”/CSV ------------------------------
st.divider()
st.subheader("í”¼ë“œë°± ê²°ê³¼")
if st.session_state.history:
    last = st.session_state.history[-1]
    c1,c2 = st.columns([1,3])
    with c1: st.metric("ì´ì (/100)", last.get("score","â€”"))
    with c2: st.markdown(last.get("feedback",""))
else:
    st.info("ì•„ì§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("ì—­ëŸ‰ ë ˆì´ë” (ì„¸ì…˜ ëˆ„ì )")
competencies = ["ë¬¸ì œì •ì˜","ë°ì´í„°/ì§€í‘œ","ì‹¤í–‰ë ¥/ì£¼ë„ì„±","í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜","ê³ ê°ê°€ì¹˜"]
def comp_df(hist):
    rows=[h["competencies"] for h in hist if h.get("competencies") and len(h["competencies"])==5]
    return pd.DataFrame(rows, columns=competencies) if rows else None
cdf = comp_df(st.session_state.history)
if cdf is not None:
    avg = cdf.mean().values.tolist()
    if PLOTLY_OK:
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(name="í‰ê· ", r=avg+[avg[0]], theta=competencies+[competencies[0]], fill='toself', opacity=0.6))
        cum = cdf.sum().values.tolist()
        max_possible = 20 * len(cdf)
        if max_possible > 0:
            norm = [(v/max_possible)*20 for v in cum]
            fig.add_trace(go.Scatterpolar(name="ëˆ„ì ë¹„ìœ¨", r=norm+[norm[0]], theta=competencies+[competencies[0]], fill='toself', opacity=0.3))
        fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0,20])), showlegend=True, height=420)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.bar_chart(pd.DataFrame({"avg": avg}, index=competencies))
    cdf2 = cdf.copy()
    cdf2["í•©ê³„"] = cdf2.sum(axis=1)
    st.dataframe(cdf2, use_container_width=True)
else:
    st.caption("ì•„ì§ ì—­ëŸ‰ ì ìˆ˜ê°€ íŒŒì‹±ëœ ì½”ì¹­ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.divider()
st.subheader("ì„¸ì…˜ ë¦¬í¬íŠ¸ (CSV)")
def build_report(hist):
    rows=[]
    for h in hist:
        row={"timestamp":h.get("ts"),"question":h.get("question"),"user_answer":h.get("user_answer"),
             "score":h.get("score"),"feedback_raw":h.get("feedback")}
        comps=h.get("competencies")
        if comps and len(comps)==5:
            for k,v in zip(competencies, comps): row[f"comp_{k}"]=v
        rows.append(row)
    return pd.DataFrame(rows) if rows else pd.DataFrame(columns=["timestamp","question","user_answer","score","feedback_raw"])
rep = build_report(st.session_state.history)
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=rep.to_csv(index=False).encode("utf-8-sig"),
                   file_name="interview_session_report.csv", mime="text/csv")

st.caption("ì§€ê¸ˆì€ â€˜ì›ë¬¸ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ê¸°â€™ ëŒ€ì‹  í˜ì´ì§€ ì „ì²´ë¥¼ ë¶„ì„í•´ ìš”ì•½í•©ë‹ˆë‹¤. ê³µê³  URLì´ ì—†ì–´ë„ ìë™ íƒìƒ‰í•œ URLì—ì„œ ìš”ì•½ì„ ì‹œë„í•©ë‹ˆë‹¤.")
